[{"path":"index.html","id":"acerca-de-este-curso","chapter":"Acerca de este curso","heading":"Acerca de este curso","text":"","code":""},{"path":"index.html","id":"requerimiento-y-competencias","chapter":"Acerca de este curso","heading":"0.1 Requerimiento y competencias","text":"se requieren conocimiento previos de programación para trabajar en este curso.\nEs solamente necesario un repaso de algunos conceptos de estadística y algo de experiencia en gestión de la cadena de suministrosEste curso tendrá más valor agragado para ti si en logar de utilizar los datos que te entregamos utilizas tus propios datos. Esto es lo que llamaremos datasets. En general tus datos pueden venir de hojas de cálculo, pero la tecnología que utilizaremos puede manejar volúnemes de información cientos o miles de veces más extensas que tu hoja de excel más grande que puedas concebir.","code":""},{"path":"index.html","id":"software-utilizado","chapter":"Acerca de este curso","heading":"0.2 Software utilizado","text":"Utilizaremos R-CranQue puedes bajarlo de este sitio:Descargar R-Cran\nHay versiones para Mac y LinuxComplementaremos el trabajo con R-Studio (hoy llamado Posit)Finalmente es conveniente (absolutamente necesario) que sepas trabajar en github.","code":""},{"path":"data_analitics.html","id":"data_analitics","chapter":"Capítulo 1 Data_Analitics","heading":"Capítulo 1 Data_Analitics","text":"lo largo de este curso desarrollaremos una serie de tópicos que son novedosos y omnipresentes en la Ingeniería Industrial 4.0. Muchos de ellos han estado presente en el pasado, tales como los métodos estadísticos. Pero solamente han capturado la atención de gobiernos, academia y empresas cuando las arquitecturas del hardware, más los avances de datos disponibles en la nube han logrado superar las barreras de los estancos que los límites que el papel nos imponía. Gran parte de los cimientos de la inteligencia artificial se remontan los años 50 del siglo XX, autores como GEORGE BOOLE en 1854 sentaron las bases de esta disciplina.\nEn 1950 ALAN TURING Propone en su ensayo Computing Machinery Intelligence el Test de Turing, una prueba de comunicación verbal hombre-máquina que evalúa la capacidad de las segundas de hacerse pasar por humanos.Luego en 1956 JOHN MCCARTHY El informático acuña por primera vez el término “Inteligencia Artificial”, durante la Conferencia de Darmouth, evento considerado el germen de la disciplina, para finalizar con MARVIN MINSKY que en 1969 escribe el libro perceptores, el trabajo fundamental del análisis de las redes neuronales artificiales.En estos días hay dos lenguajes de propósito general que se están imponiendo en el terreno de abordar y aplicar los conceptos de Analítica de Datos.El primero de ellos es Python, que sin dudas veras que ha tenido repercución y éxito, aún cuando ha llegado ser parte del Sylabus de todas las carreras de ingeniería industrial.El otro lengunaje, que mi leal entender es más rápidamente asimilable con los ingenieros industriales es R-Cran.","code":""},{"path":"data_analitics.html","id":"bibliografía-recomendada","chapter":"Capítulo 1 Data_Analitics","heading":"1.1 Bibliografía Recomendada","text":"","code":""},{"path":"data_analitics.html","id":"desambiguación-de-conceptos","chapter":"Capítulo 1 Data_Analitics","heading":"1.2 Desambiguación de conceptos","text":"Se suele utilizar como sinónimos, pero son diferentes conceptosAnalítica de DatosMachine LearningDeep LearningBusiness IntelligenceMinería de DatosInteligencia ArtificialTodos ellos tienen un factor en común, el uso del un dataset, hardware y software, pero si bien tienen similitudes son bastantes distintos.Veamos una aproximación al problema partiendo de los modelos estadísticos.Cualquier estadístico que usemos (T-Studen, Distribución Normal, Criterio de máxima verosimilitud, etc) puede ser obordado con el uso de computadora tomando datos etiquetados (que tienen una columna que muestra de que tipo son), luego con un algoritmo apropiado podemos construir un modelo.En analítica de datos invertimos el proceso.\nTomando datos sin etiquetar los sometemos al análisis de un modelo y predecimos que resultado de obtendría en un mundo físico partir de los datos que son abstracción de ese mundo.Así:Predecir cuando se romperá un ejeEncontrar en que categoría de la flota sería conveniente incorporar un camión nuevoDeterminar antes de cerrar una transacción electrónica virtual la probabilidad de fraudeEtc.Son ejemplos de lo que se puede hacer con analítica de datos.@ehrlinger_treating_2018En inteligencia artificial el proceso es muy semejante. diferencia del caso anterior tenemos un algorítmo que puede crear el modelo en base las etiquetas de los datos. En este caso se opera en forma supervisada. para crear un modelo.\nEl resultado de la inteligencia artificial es un modelo que nos entrega datos y partir de datos que el tomó para entrenarse y nos muestra patrones de semejanza entre los datos de entrenamiento y los de prueba.Es una práctica habitual crear modelos de IA utilizando un dataset y remuestrear (sampling) usando el 25% de los datos para entrenar y el 75% para predecir y evaluar la calidad del modelo.Ejemplos de este tipo de modelos son los árboles de decisión y su entrenamientoPor último tenemos el abordaje de machine learning.1","code":""},{"path":"data_analitics.html","id":"data-analitycs","chapter":"Capítulo 1 Data_Analitics","heading":"1.3 Data Analitycs","text":"La analítica de datos permite las organizaciones analizar todos sus datos (en tiempo real, históricos, estructurados, estructurados, cualitativos) para identificar patrones y generar conocimientos para informar y, en algunos casos, automatizar decisiones, conectando la inteligencia y la acción. Las mejores soluciones actuales respaldan el proceso analítico de un extremo otro, desde el acceso, la preparación y la analítica de datos hasta la operatividad de los análisis y el seguimiento de los resultados.Según TIBCO, la analítica de datos permite las organizaciones transformar digitalmente su empresa y su cultura, volviéndose más innovadoras y con visión de futuro en la toma de decisiones. Más allá del monitoreo y la generación de informes tradicionales de KPI para encontrar patrones ocultos en los datos, las organizaciones potenciadas por algoritmos son los nuevos innovadores y líderes empresariales.Al cambiar el paradigma más allá de los datos para conectar los conocimientos con la acción, las empresas podrán crear experiencias personalizadas para los clientes, crear productos digitales conectados, optimizar las operaciones y aumentar la productividad de los empleados, pero el tratamiento que asegure la calidad de la informació nes y será aprtir de este momento un factor clave de éxito.2Con la analítica colaborativa de datos, las empresas permiten que todos contribuyan al éxito empresarial, desde ingenieros de datos y científicos de datos, hasta desarrolladores y analistas empresariales, e incluso profesionales y líderes empresariales. La analítica colaborativa de datos también incentiva quienes están dentro y fuera de una organización conectarse y colaborar. Por ejemplo, los científicos de datos pueden trabajar en estrecha colaboración con un cliente para ayudarlo resolver sus problemas en tiempo real utilizando la interfaz de usuario altamente colaborativa de la analítica moderna del mundo de hoy.La analítica de datos impulsa las empresas avanzar mediante la introducción de algoritmos en todas partes para optimizar los momentos comerciales críticos, como un cliente que ingresa su tienda, un equipo punto de fallar u otros eventos que podrían significar la diferencia entre ganar o perder negocios. La analítica de datos se aplica todas las industrias, incluidas las de servicios financieros y seguros, fabricación, energía, transporte, viajes y logística, atención médica y otras. La analítica de datos puede ayudar predecir y manejar interrupciones, optimizar rutas, brindar un servicio proactivo al cliente, realizar ofertas inteligentes de venta cruzada, predecir fallas inminentes de equipos, administrar el inventario en tiempo real, optimizar los precios y prevenir el fraude.","code":""},{"path":"data_analitics.html","id":"qué-dimensiones-engloba-el-concepto-de-ad","chapter":"Capítulo 1 Data_Analitics","heading":"1.4 ¿Qué dimensiones engloba el concepto de AD?","text":"Automatización de ReportesInteligencia de NegociosPreparción de datos (Data Wrangling)Visualización de Datos y ConocimientoAnalítica Geoespacial (GIS)Analítica productiva (predictiva y prescriptiva)Machine LearningAnalítica en tiempo real","code":""},{"path":"data_analitics.html","id":"cómo-utilizar-la-analítica-de-datos-el-proceso-analítico","chapter":"Capítulo 1 Data_Analitics","heading":"1.5 ¿Cómo utilizar la analítica de datos: el proceso analítico?","text":"Comprenda el problema empresarial.Recopile/identifique datos relevantes para el problema.Prepare los datos para el análisis.Analice los datos para generar conocimientos.Implemente/ponga en funcionamiento los análisis y los modelos.Supervise y optimice el rendimiento.","code":""},{"path":"data_analitics.html","id":"dataset-o-información-base","chapter":"Capítulo 1 Data_Analitics","heading":"1.5.1 Dataset o Información base","text":"Tomaremos un dataset simple, que nos muestra la cantidad de semanas que un pack de baterias de un autoelevador fue capaz de tomar carga como para resistir las operaciones durante toda una jornada de trabajo sin necesidad de recargarse o ser cambiado. Fuente [apte_data_1997]Trucos:Puedes cargar los datos en un vecto con el comando Edad_75 <- scan() . tipea uno uno los valores de edad y luego dos veces seguidas Enter para finalizar el proceso.En los siguientes capítulos veremos como capturar de fuentes de datos como .xls","code":""},{"path":"data_analitics.html","id":"tratamiento-y-captura-de-la-información","chapter":"Capítulo 1 Data_Analitics","heading":"1.6 Tratamiento y captura de la información","text":"Obtener el la edad de la batería con nro de inventario 3Nos interesa saber si la vida en semanas de nuestras baterias compradas coincide con los valores publicados en el sitio web del fabricantes. Se considera que una batería tiene una vida últil mientras al aplicarle una carga completa ella pueda resistir todo el turno de 8 horas sin necesidad de ser reemplazada.Obtener las edades de la muestraPromedio de vida","code":"\ndatos_75 <- c(1 , 19 , 0,\n2 , 18 , 0,\n3 , 22 , 0,\n4 , 25 , 0,\n5 , 17 , 0,\n6 , 30 , 0,\n7 , 29 , 0,\n8 , 32 , 0,\n9 , 31 , 0,\n10, 33 , 0,\n11, 38 , 0,\n12, 36 , 0,\n13, 40 , 1,\n14, 40 , 0,\n15, 42 , 0,\n16, 45 , 0,\n17, 47 , 0,\n18, 49 , 0,\n19, 55 , 0,\n20, 58 , 1,\n21, 57 , 1,\n22, 63 , 1,\n23, 65 , 1,\n24, 65 , 1,\n25, 66 , 1)\nMuestra_75 <- matrix(datos_75, ncol = 3, byrow = TRUE)\nMuestra_75\n#>       [,1] [,2] [,3]\n#>  [1,]    1   19    0\n#>  [2,]    2   18    0\n#>  [3,]    3   22    0\n#>  [4,]    4   25    0\n#>  [5,]    5   17    0\n#>  [6,]    6   30    0\n#>  [7,]    7   29    0\n#>  [8,]    8   32    0\n#>  [9,]    9   31    0\n#> [10,]   10   33    0\n#> [11,]   11   38    0\n#> [12,]   12   36    0\n#> [13,]   13   40    1\n#> [14,]   14   40    0\n#> [15,]   15   42    0\n#> [16,]   16   45    0\n#> [17,]   17   47    0\n#> [18,]   18   49    0\n#> [19,]   19   55    0\n#> [20,]   20   58    1\n#> [21,]   21   57    1\n#> [22,]   22   63    1\n#> [23,]   23   65    1\n#> [24,]   24   65    1\n#> [25,]   25   66    1\nMuestra_75[3,2]\n#> [1] 22\nplot ((Muestra_75[ ,2]), main= \"Vida en Semanas\",xlab=\"Ficha Taller Batería con falla\", ylab= \"Semanas\")\nvida_media_muestra <- mean(Muestra_75[,2])\nvida_media_muestra\n#> [1] 40.88\nplot (sort(Muestra_75[ ,2]), main= \"Vida en Semanas\",xlab=\"Batería\", ylab= \"Semanas\")\nabline(h=vida_media_muestra)\nabline(h=40,col=\"red\")\nabline(h=50,col=\"green\")"},{"path":"data_analitics.html","id":"muestra-de-historial-de-carga","chapter":"Capítulo 1 Data_Analitics","heading":"1.6.1 Muestra de Historial de Carga","text":"","code":"\nplot(Muestra_75[ , 1:2],main = \"Resumen de Casos\",xlab = \"Ficha Bateria\",ylab=\"Edad\", type=\"b\", col=\"RED\")"},{"path":"data_analitics.html","id":"histogramas-de-edades","chapter":"Capítulo 1 Data_Analitics","heading":"1.7 Histogramas de Edades","text":"","code":"\nhist(Muestra_75[ ,2],breaks = 10, main = \"Histogramas de edades\")"},{"path":"data_analitics.html","id":"gráficos-de-densidad","chapter":"Capítulo 1 Data_Analitics","heading":"1.8 Gráficos de Densidad","text":"También conocido como gráfico de densidad de Kernel y gráfico de densidad de traza.Un gráfico de densidad visualiza la distribución de datos en un intervalo o período de tiempo continuo. Este gráfico es una variación de un Histograma que usa el suavizado de cerner para trazar valores, permitiendo distribuciones más suaves al suavizar el ruido. Los picos de un gráfico de densidad ayudan mostrar dónde los valores se concentran en el intervalo.Una ventaja de los gráficos de densidad sobre los histogramas es que son mejores para determinar la forma de distribución porque se ven afectados por el número de contenedores utilizados (cada barra utilizada en un histograma típico). Un histograma que consta de solo 4 compartimientos producirá una forma de distribución lo suficientemente distinguible como lo haría un histograma de 20 compartimientos. Sin embargo, con los gráficos de densidad esto es un problema.La función de densidad puede calcularse fácilmene en R-Cran con el siguiente comandoDado que es un objeto de R partir del resultado de los cuantiles es posible plotear directamente la función invocada","code":"\ndensity(Muestra_75[ ,2])\n#> \n#> Call:\n#>  density.default(x = Muestra_75[, 2])\n#> \n#> Data: Muestra_75[, 2] (25 obs.); Bandwidth 'bw' = 7.394\n#> \n#>        x                y            \n#>  Min.   :-5.183   Min.   :5.383e-05  \n#>  1st Qu.:18.159   1st Qu.:2.474e-03  \n#>  Median :41.500   Median :1.246e-02  \n#>  Mean   :41.500   Mean   :1.070e-02  \n#>  3rd Qu.:64.841   3rd Qu.:1.698e-02  \n#>  Max.   :88.183   Max.   :2.186e-02\nplot(density(Muestra_75[ ,2]), main = \"Gráfico de Densidad\", ylab=\"Cantidad relativa de muestras\",xlab=\"Edad\")"},{"path":"data_analitics.html","id":"gráfica-conjunta-de-histograma-y-densidad","chapter":"Capítulo 1 Data_Analitics","heading":"1.8.1 Gráfica conjunta de Histograma y Densidad","text":"","code":"\nhist(Muestra_75[ ,2], # histogram\n    breaks = 3,\n     col=\"peachpuff\", # column color\n border=\"black\",\n prob = TRUE, # show densities instead of frequencies\n xlab = \"Edad\",\n main = \"Distribución Edades de la Muestra\")\nlines(density(Muestra_75[ ,2]), # density plot\n lwd = 2, # thickness of line\n col = \"chocolate3\")"},{"path":"data_analitics.html","id":"varianza-de-muestra-y-población","chapter":"Capítulo 1 Data_Analitics","heading":"1.9 Varianza de Muestra y Población","text":"La varianza de una población está expresada por la ecuación\\[\\sigma^2 = \\sum_{=1}^{n} \\frac {(x_i – \\mu)^2} {N}   \\]La varianza (de la muestra puede ser calculada con el comandoDato que la muestra siempre tiene un número considerablemente menor de individuos que la población la expresión que se usa para calcular la varianza de la muestra es distinta.Varianza de la muestra es\\[ s^2 = \\sum_{=1}^{n} \\frac {(x_i – \\bar{x})^2} {n-1}  \\]Puede encontrarse la varianza de la población sustituyendo los valores como se indica continuación:Con esto valores podemos calcular la varianza de la población con la siguiente codificación\\[ \\sigma = \\frac {S} {n} \\]El desvio estandar de la meustra esEn tanto que el desvió estándar de la población esUna forma más elegante de presentar estos resultados parciales puede conseguirse con el uso de la función *sprintf()Un valor que es interesante calcular para comparar con la muestra es el desvío porcentual calculado como \\(dsp_{\\%}= \\frac{dsp}{n}\\)","code":"  var(Muestra_75[ ,2])\nvar_muestra <- var(Muestra_75 [ ,2])\nvar_muestra\n#> [1] 244.61\nn <- length(Muestra_75[ ,2])\nn\n#> [1] 25\nn_over_n_1 <- n/(n-1)\nn_over_n_1\n#> [1] 1.041667\nvar_poblacion <- var(Muestra_75[ ,2]) / n\nvar_poblacion\n#> [1] 9.7844\ndsm <- sd({Muestra_75[ ,2]})\ndsm\n#> [1] 15.64001\ndsp <- sqrt(var_poblacion)\ndsp\n#> [1] 3.128003\nsprintf(\"Varianza muestral =%s, Varianza poblacional = %s\", var_muestra, var_poblacion )\n#> [1] \"Varianza muestral =244.61, Varianza poblacional = 9.7844\"\ndspp_75 <- dsp/n\ndspp_75\n#> [1] 0.1251201"},{"path":"data_analitics.html","id":"repetir-en-análisis-para-la-muestra-siemens-1005","chapter":"Capítulo 1 Data_Analitics","heading":"1.10 Repetir en análisis para la muestra Siemens 1005","text":"Tarea !Siempre comenzaremos nuestro trabajo con un análisis exploratorio básico, tal como hemos señalado.\nEn este caso hemos descartado ningún individuo de la muestra, pero sería conveniente hacerlo.\nEl comando boxplot() te permitirá hacer una inspección rápida.\nAdisionalmente prueba el comando summary() para analizar la distribución de cunatiles y datos faltantes.","code":""},{"path":"data_analitics.html","id":"construcción-de-un-modelo","chapter":"Capítulo 1 Data_Analitics","heading":"1.11 Construcción de un modelo","text":"Nota!En esta parte comenzaremos desarrollar un modelo.En la ficha del fabricante de baterías, se indica que el hecho de sustituir el electrolíto original por otro que sea el oficial y que en general contiene antimonio puede ser en primera instancia considerado como un factor que prolonga la vida útil de la batería y amplía el rango de su ciclo de carga/descarga.Sin embrago el fabricante advierte del peligro potencial de cáncer de próstata que el antimonio presenta para el personal técnico que opera las baterías y para el conductor del vehículo.Vale recordar que las placas de plomo tienen en su aleación antimonio y medida que se van envejeciendo liberan antimonio al electrolito, que aún cuando sea en concentraciones bajas, son capaces de afctar la salud. Por este motivo podríamos inferir que hay una correlación entre la vida útil de las baterías y la aparición de síntomas de cáncer de próstata.Utilizaremos una hipótesis simple. El modelo de regresión lineal podría ayudarnos predecir para esta población que edad deberíamos comenzar el diagnóstico de antígeno prostático en el personal afectado las tareas que se relacionan con las baterías.También realizaremos una comprobación basada en el modelo para saber si ha habido cambios significativos en los síntomas atribuibles al uso del antimonio.Como se puede ver, recurrimos aún la medicina, pero nos planteamos una hipótesis sobre las implicancias del antimonio y la salud y el objetivo es otorgarle al médico una recomendación para que tenga una alerta temprana, sin que esto afirme o desmienta hechos científicos sobre los que tenemos capacidad para diagnostiras enfermedades y nos limitamos prevenirlas.","code":""},{"path":"data_analitics.html","id":"hipótesis-o-pregunta-de-investigación","chapter":"Capítulo 1 Data_Analitics","heading":"1.12 Hipótesis o pregunta de investigación","text":"El proceso de investigación científica repetible parte de este tipo de premisa. Es un proceso iterativo de aproximación y revelación.3 Comenzaremos con una pregunta de investigación. ¿Será posible utilizar un modelo de regresión lineal para entender el comportamiento de estos datos?. Esta pregunta con el tiempo podrá ser formulada como una afirmación. Este proceso debe tener una investigación preliminar. De esteo modo podríamos expresar algo como El modelo de regresión lineal, que es muy utilizado en inteligencia artificial podrá darme respuestas dos preguntas clave de este problema1- El modelo regresión lineal puede predecir la probabilidad de aparición temprana de cáncer.\n2- Es posible utilizar el modelo construido para comparar dos poblaciones.","code":""},{"path":"data_analitics.html","id":"bases-de-un-modelo-ia-de-regresión-lienal","chapter":"Capítulo 1 Data_Analitics","heading":"1.13 Bases de un modelo IA de regresión lienal","text":"Para trabajar en este sentido R-Cran tiene posibilidades de desarrollar modelos de regresión lineal en un sólo comando. pesar de ello construiremos un modelo desde CERO para entender como se procede en Inteligencia Artificial.4Todo modelo de regresión se basa en hallar los parámetros \\(\\) y \\(b\\) de una ecuación de una recta.\\[ y = *x + b \\]Donde: y es un número que varía entre 1 y 01 señala que se trata de un caso probable positivo, en tanto que0 señala que es poco probable la aparición de cancer.Para poder hallar los valores de estas variable o parámetros del modelo deberíamos utilizar los datos. En realidad con solamente dos datos de la muestra podríamos encontrar una recta que pase por estos dos puntos. Lo más aconsejable es utilizar sólo dos, sino todos los puntos.\nEsto implica plantear un problema de dos ecuaciones con dos incógnitas y en rigor el método de los mínimos cuadrados sería lo más indicado.Siguiendo con la idea de la construcción artesanal del modelo podemos decir que encontrando un punto significativo del modelo por el que pase la recta podremos agregar alguna estrategia para hallar la pendiente y con esos datos calcular \\(\\) y \\(b\\).","code":""},{"path":"data_analitics.html","id":"puntos-significativos-de-modelo","chapter":"Capítulo 1 Data_Analitics","heading":"1.14 Puntos significativos de modelo","text":"Un punto significativo podría ser la moda de las edades y la moda de la variable categórica [1,0].\nSe puede demostrar que en el modelo de regresión de los mínimos cuadrados este punto es el promedio de \\(\\overline{x}\\) y el promedio de \\(\\overline{y}\\).\nPero existen otros valores verosímiles (Ver criterios de máxima verosimilitud de Ronald Fisher).","code":""},{"path":"data_analitics.html","id":"cácluo-de-x-e-y-significativos","chapter":"Capítulo 1 Data_Analitics","heading":"1.14.1 Cácluo de x e y significativos","text":"Valor de y verosimilValor de x verosimilEl punto de coordenadas [24.5 , 0.5] es un punto por el que debe pasar la recta que construiremos.","code":"\nindex_ymax <- which.max(Muestra_75[ ,3])\nymax <- Muestra_75[index_ymax,3]\n\nindex_ymin <- which.min(Muestra_75[ ,3])\nymin <- Muestra_75[index_ymin,3]\n\ny_sig <- (ymax-ymin) /2\n\ny_sig \n#> [1] 0.5\nindex_xmax <- which.max(Muestra_75[ ,2])\nxmax <- Muestra_75[index_xmax,2]\nxmax\n#> [1] 66\nindex_xmin <- which.min(Muestra_75[ ,2])\nxmin <- Muestra_75[index_xmin,2]\nxmin\n#> [1] 17\n\nx_sig <- (xmax-xmin)\n\nx_sig \n#> [1] 49"},{"path":"data_analitics.html","id":"cáclulo-de-la-pendiente","chapter":"Capítulo 1 Data_Analitics","heading":"1.14.2 Cáclulo de la pendiente","text":"em valor \\(\\) representa la pendiente de la recta. De modo que con los \\(\\Delta{x}\\) y \\(\\Delta{y}\\) podremos calcularlo facilmente\\(\\) = valor de la pendiente es 0.02040816Pendiente en grados","code":"\na <- ((ymax-ymin)/(xmax-xmin))\na\n#> [1] 0.02040816\natan(a)\n#> [1] 0.02040533"},{"path":"data_analitics.html","id":"cáclulo-de-la-ordenada-al-origen.","chapter":"Capítulo 1 Data_Analitics","heading":"1.14.3 Cáclulo de la ordenada al origen.","text":"Para calcular la ordenada al origen basta con poner \\(x=\\overline{x}\\) del punto significativo que hemos adoptado en la formula de la recta usando el \\(\\) calculado y despejar \\(b\\)\\[ y = *x +b\\]\n\\[ b = y-*x\\]\nCáclulo de b","code":"\ny_sig\n#> [1] 0.5\nx_sig\n#> [1] 49\na\n#> [1] 0.02040816\n\nb <- y_sig-(a*x_sig)\nb\n#> [1] -0.5"},{"path":"data_analitics.html","id":"gráfica","chapter":"Capítulo 1 Data_Analitics","heading":"1.14.4 Gráfica","text":"","code":"\nplot(Muestra_75[ ,2],Muestra_75[ ,3], main = \"Dignóstico Preliminar\", ylab=\"Diagnóstico + -\", xlab=\"Edad\")\nabline(h=y_sig, col=\"red\")\nabline(v=x_sig,col=\"red\")\nabline(b,a)"},{"path":"data_analitics.html","id":"modelo-de-regresión-lineal-generalizado","chapter":"Capítulo 1 Data_Analitics","heading":"1.15 Modelo de regresión lineal generalizado","text":"En realidad este trabajo que hemos hecho puede ser realizado con solamente un comando en R-Cran.Construiremos el datasetConstrucción del modelo linealComo vemos nos dice que la ordenada al origen es -0.610 y la pendiente es 0.021 muy parecidas los valores \\(\\) y \\(b\\) que obtuvimos antes.","code":"\nx_muestra_75 <- Muestra_75[ ,2]\ny_muestra_75 <- Muestra_75[ ,3]\nmrlg <- as.data.frame( cbind(x_muestra_75,y_muestra_75))\nmi_formula <- y_muestra_75 ~ x_muestra_75\nmodelo_2 <-  lm (mi_formula, data=mrlg)\nmodelo_2\n#> \n#> Call:\n#> lm(formula = mi_formula, data = mrlg)\n#> \n#> Coefficients:\n#>  (Intercept)  x_muestra_75  \n#>     -0.61021       0.02178\nmodelo_2$coefficients[2]\n#> x_muestra_75 \n#>   0.02177616\nmodelo_2$coefficients[1]\n#> (Intercept) \n#>  -0.6102094\nplot(mrlg)\nabline(modelo_2,col=\"blue\")"},{"path":"data_analitics.html","id":"modelo-de-regresión-logística","chapter":"Capítulo 1 Data_Analitics","heading":"1.16 Modelo de regresión logística","text":"Una de las cosas destacables del uso de IA es que puedo recurrir muchos modelos para interpretar o predecir el comportamiento de los datos. Puedo generar (entrenar) el modelo o modelos y utilizar algún mecanismo de medición de la calidad de cada modelo y en base ello utilizar IA para seleccionar el que más se adapta mis datos.Hay una distribución de probabilidades llamada Distribución Logística que puede utilizarse (del mismo modo que la distribución lineal se usa para el modelo de regresión lineal generalizado) para armar una regresión lineal logística.La forma típica de la distribución logística normalizada tiene la siguiente ecuación paramétrica y luce como se ve en el ejemplo.\\[ \\sigma(x)= \\frac{e^x}{(e^x+1)} = \\frac{1}{1+e^{-x}}\\]Como pude verse el modelo de regresión logística es mas sensible los datos y proporcional resultados más verosímiles, pero aún mantienen error admisible. De todos modos esta análisis visual nos indica que ya tenemos una alternativa que es mejor que nuestro modelo lineal inicial.La función de distribución de probabilidad acumulada de la regresión logística es:\\[g(F(x))=  ln \\left[ \\frac{F(x)}{1-F(x)} \\right] = \\beta^0 + \\beta_1*x \\]\nAl igual que en la regresión lineal es posible (aplicando logaritmos) encontrar los parámetros \\(\\beta_0\\) y \\(\\beta_1\\) con solamente cuatro datos en la muestra. El error será inadmisible, pero es algo parecido armar una regresión lineal con sólo dos muestras.","code":"\ncurve((1/(1+exp(-x))),-10,10,col =\"red\")\nabline(0.5,0.05,col=\"green\")\ntext(-8,0.2,\"Rergresion Lineal\",col=\"green\")\ntext(6,0.8,\"Rergresion Logística\",col=\"red\")"},{"path":"patrones-de-asociación.html","id":"patrones-de-asociación","chapter":"Capítulo 2 Patrones de asociación","heading":"Capítulo 2 Patrones de asociación","text":"","code":""},{"path":"patrones-de-asociación.html","id":"introducción","chapter":"Capítulo 2 Patrones de asociación","heading":"2.1 Introducción","text":"Los algoritmos de reglas de asociación tienen como objetivo encontrar relaciones dentro un conjunto de transacciones, en concreto, items o atributos que tienden ocurrir de forma conjunta. En este contexto, el término transacción hace referencia cada grupo de eventos que están asociados de alguna forma, por ejemplo:El pedido de la compra en un supermercado.El pedido de la compra en un supermercado.Los libros que compra un cliente en una librería.Los libros que compra un cliente en una librería.Las páginas web visitadas por un usuario.Las páginas web visitadas por un usuario.Las características que aparecen de forma conjunta.Las características que aparecen de forma conjunta.Cómo ordenar los repuestos en el almacén de un taller de mantenimientoCómo ordenar los repuestos en el almacén de un taller de mantenimientoDisposición de insumos para la ayuda humanitariaDisposición de insumos para la ayuda humanitariaEl protfolio de iniciativas de innovación (Metodo TRIZ)El protfolio de iniciativas de innovación (Metodo TRIZ)","code":""},{"path":"patrones-de-asociación.html","id":"formatos-del-dataset","chapter":"Capítulo 2 Patrones de asociación","heading":"2.2 Formatos del dataset","text":"Los datos que se necesitan pueden venir especificados en dos formatos denominados lista corta o lista larga (matricial)","code":""},{"path":"patrones-de-asociación.html","id":"lista-corta","chapter":"Capítulo 2 Patrones de asociación","heading":"2.2.1 Lista corta","text":"Este el el formato utilizado habitualmente por las bases de datos de salida de almacenes, especialmente utilizado en los archivos csv de SAP.","code":"\ndatos_crudo <- c(1,1,1,2,2,3,3,\"5W40\",\"Filtro Aceite\",\"Filtro Aire\",\"Correa Distr\",\"Bomba de Agua\",\"Filtro Nafta\",\"Limpia Inyectores\")\nlista_corta <- matrix(datos_crudo,ncol=2, byrow=FALSE)\ndimnames(lista_corta) <- list(salida = c(1,2,3,4,5,6,7) , producto = c(\"ticket\",\"producto\") )\nlista_corta\n#>       producto\n#> salida ticket producto           \n#>      1 \"1\"    \"5W40\"             \n#>      2 \"1\"    \"Filtro Aceite\"    \n#>      3 \"1\"    \"Filtro Aire\"      \n#>      4 \"2\"    \"Correa Distr\"     \n#>      5 \"2\"    \"Bomba de Agua\"    \n#>      6 \"3\"    \"Filtro Nafta\"     \n#>      7 \"3\"    \"Limpia Inyectores\""},{"path":"patrones-de-asociación.html","id":"lista-corta-1","chapter":"Capítulo 2 Patrones de asociación","heading":"2.2.2 Lista corta","text":"cada uno de los eventos o elementos que forman parte de una transacción (ticket) se le conoce como item y un conjunto de ellos itemset. Una transacción puede estar formada por uno o varios items, en el caso de ser varios, cada posible subconjunto de ellos es un itemset distinto. Por ejemplo, la transacción \\(T = {,B,C}\\) está formada por 3 items (, B y C) y sus posibles itemsets son: {,B,C}, {,B}, {B,C}, {,C}, {}, {B} y {C}.Una regla de asociación se define como una implicación del tipo “si \\(X entonces Y” (X⇒Y )\\), donde \\(X e Y\\) son itemsets o items individuales. El lado izquierdo de la regla recibe el nombre de antecedente o lenft-hand-side (LHS) y el lado derecho el nombre de consecuente o right-hand-side (RHS). Por ejemplo, la regla {,B} => {C} significa que, cuando ocurren y B, también ocurre C.Existen varios algoritmos diseñados para identificar itemsets frecuentes y reglas de asociación. continuación, se describen algunos de los más utilizados.","code":"\ndatos_m <- c(1,1,1,0,0,0,0, 0,0,0,1,1,0,0, 0,0,0,0,0,1,1)\nlista_larga <- matrix(datos_m,nrow=3,byrow=TRUE)\nlista_larga\n#>      [,1] [,2] [,3] [,4] [,5] [,6] [,7]\n#> [1,]    1    1    1    0    0    0    0\n#> [2,]    0    0    0    1    1    0    0\n#> [3,]    0    0    0    0    0    1    1\ndimnames(lista_larga) <- list(ticket = c(1,2,3) , producto = c(\"5W40\",\"Filtro Aceite\",\"Filtro Aire\",\"Correa Distr\",\"Bomba de Agua\",\"Filtro Nafta\",\"Limpia Inyectores\"))\n\nlista_larga\n#>       producto\n#> ticket 5W40 Filtro Aceite Filtro Aire Correa Distr\n#>      1    1             1           1            0\n#>      2    0             0           0            1\n#>      3    0             0           0            0\n#>       producto\n#> ticket Bomba de Agua Filtro Nafta Limpia Inyectores\n#>      1             0            0                 0\n#>      2             1            0                 0\n#>      3             0            1                 1"},{"path":"patrones-de-asociación.html","id":"apriori","chapter":"Capítulo 2 Patrones de asociación","heading":"2.3 Apriori","text":"Apriori fue uno de los primeros algoritmos desarrollados para la búsqueda de reglas de asociación y sigue siendo uno de los más empleados, tiene dos etapas:Identificar todos los itemsets que ocurren con una frecuencia por encima de un determinado límite (itemsets frecuentes).Convertir esos itemsets frecuentes en reglas de asociación.Con la finalidad de ilustrar el funcionamiento del algoritmo, se emplea un ejemplo sencillo. Supóngase la siguiente base de datos de un centro comercial en la que cada fila es una transacción. En este caso, el término transacción hace referencia todos los productos comprados bajo un mismo ticket (misma cesta de la compra). Las letras , B, C y D hacen referencia 4 productos (items) distintos.","code":""},{"path":"patrones-de-asociación.html","id":"transacción","chapter":"Capítulo 2 Patrones de asociación","heading":"2.4 Transacción","text":"{, B, C, D}{, B, D}{, B}{B, C, D}{B, C}{C, D}{B, D}Antes de entrar en los detalles del algoritmo, conviene definir una serie de conceptos:Soporte: El soporte del item o itemset X es el número de transacciones que contienen X dividido entre el total de transacciones.\\[ Sop_x = \\frac{itemset_x}{itemset_{total}}\\]Confianza: La confianza de una regla “Si X entonces Y” se define acorde la ecuación\\[confianza(X=>Y)=\\frac{soporte(unión(X,Y))}{soporte(X)},\\]\ndonde unión(XY) es el itemset que contienen todos los items de X y de Y. La confianza se interpreta como la probabilidad \\(P(Y|X)\\), es decir, la probabilidad de que una transacción que contiene los items de \\(X\\), también contenga los items de \\(Y\\).Volviendo al ejemplo del centro comercial, puede observarse que, el artículo , aparece en 3 de las 7 transacciones, el artículo B en 6 y ambos artículos juntos en 3. El soporte del item {} es por lo tanto del 43%, el del item {B} del 86% y del itemset {, B} del 43%. De las 3 transacciones que incluyen , las 3 incluyen B, por lo tanto, la regla “clientes que compran el artículo también compran B”, se cumple, acorde los datos, un 100% de las veces. Esto significa que la confianza de la regla {=> B} es del 100%.Encontrar itemsets frecuentes (itemsets con una frecuencia mayor o igual un determinado soporte mínimo) es un proceso trivial debido la explosión combinatoria de posibilidades, sin embargo, una vez identificados, es relativamente directo generar reglas de asociación que presenten una confianza mínima. El algoritmo Apriori hace una búsqueda exhaustiva por niveles de complejidad (de menor mayor tamaño de itemsets). Para reducir el espacio de búsqueda aplica la norma de “si un itemset es frecuente, ninguno de sus supersets (itemsets de mayor tamaño que contengan al primero) puede ser frecuente”. Visto de otra forma, si un conjunto es infrecuente, entonces, todos los conjuntos donde este último se encuentre, también son infrecuentes. Por ejemplo, si el itemset {,B} es infrecuente, entonces, {,B,C} y {,B,E} también son infrecuentes ya que todos ellos contienen {,B}.El funcionamiento del algoritmo es sencillo, se inicia identificando los items individuales que aparecen en el total de transacciones con una frecuencia por encima de un mínimo establecido por el usuario. continuación, se sigue una estrategia bottom-en la que se extienden los candidatos añadiendo un nuevo item y se eliminan aquellos que contienen un subconjunto infrecuente o que alcanzan el soporte mínimo. Este proceso se repite hasta que el algoritmo encuentra más ampliaciones exitosas de los itemsets previos o cuando se alcanza un tamaño máximo.Se procede identificar los itemsets frecuentes y, partir de ellos, crear reglas de asociación.Transacción{, B, C, D}{, B, D}{, B}{B, C, D}{B, C}{C, D}{B, D}Para este problema se considera que un item o itemset es frecuente si aparece en un mínimo de 3 transacciones, es decir, su soporte debe de ser igual o superior 3/7 = 0.43. Se inicia el algoritmo identificando todos los items individuales (itemsets de un único item) y calculando su soporte.Itemset (k=1) Ocurrencias SoporteTodos los itemsets de tamaño k = 1 tienen un soporte igual o superior al mínimo establecido, por lo que todos superan la fase de filtrado (poda).continuación, se generan todos los posibles itemsets de tamaño k = 2 que se pueden crear con los itemsets que han superado el paso anterior y se calcula su soporte.Itemset (k=2) Ocurrencias SoporteLos itemsets {, B}, {B, C}, {B, D} y {C, D} superan el límite de soporte, por lo que son frecuentes. Los itemsets {, C} y {, D} superan el soporte mínimo por lo que se descartan. Además, cualquier futuro itemset que los contenga también será descartado ya que puede ser frecuente por el hecho de que contiene un subconjunto infrecuente.Itemset (k=2) Ocurrencias SoporteSe repite el proceso, esta vez creando itemsets de tamaño k = 3.Itemset (k=3)Los itemsets {, B, C}, {, B, D} y {C, D, } contienen subconjuntos infrecuentes, por lo que son descartados. Para los restantes se calcula su soporte.Itemset (k=3) Ocurrencias SoporteEl items {B, C, D} supera el soporte mínimo por lo que se considera infrecuente. Al haber ningún nuevo itemset frecuente, se detiene el algoritmo.Como resultado de la búsqueda se han identificado los siguientes itemsets frecuentes:Itemset frecuentes{, B}{B, C}{B, D}{C, D}El siguiente paso es crear las reglas de asociación partir de cada uno de los itemsets frecuentes. De nuevo, se produce una explosión combinatoria de posibles reglas ya que, de cada itemset frecuente, se generan tantas reglas como posibles particiones binarias. En concreto, el proceso seguido es el siguiente:Por cada itemset frecuente , obtener todos los posibles subsets de .1.1 Para cada subset s de , crear la regla \\(“s => (-s)”\\)Descartar todas las reglas que superen un mínimo de confianza.Supóngase que se desean únicamente reglas con una confianza igual o superior 0.7, es decir, que la regla se cumpla un 70% de las veces. Tal y como se describió anteriormente, la confianza de una regla se calcula como el soporte del itemset formado por todos los items que participan en la regla, dividido por el soporte del itemset formado por los items del antecedente.","code":""},{"path":"patrones-de-asociación.html","id":"reglas-confianza-confianza","chapter":"Capítulo 2 Patrones de asociación","heading":"2.5 Reglas Confianza Confianza","text":"De todas las posibles reglas, únicamente {C} => {D} y {C} => {B} superan el límite de confianza.La principal desventaja de algoritmo Apriori es el número de veces que se tienen que escanear los datos en busca de los itemsets frecuentes, en concreto, el algoritmo escanea todas las transacciones un total de kmax\n+1, donde:kmax es el tamaño máximo de itemset permitido. Esto hace que el algoritmo Apriori pueda aplicarse en situaciones con millones de registros, sin embargo, se han desarrollado adaptaciones (FP-growth, Eclat, Hash-Based, partitioning, etc) que solucionan esta limitación.","code":""},{"path":"patrones-de-asociación.html","id":"fp-growth","chapter":"Capítulo 2 Patrones de asociación","heading":"2.6 FP-Growth","text":"Los investigadores Han et al. propusieron en el 2000 un nuevo algoritmo llamado FP-Growth que permite extraer reglas de asociación partir de itemsets frecuentes pero, diferencia del algoritmo Apriori, estos se identifican sin necesidad de generar candidatos para cada tamaño.En términos generales, el algoritmo emplea una estructura de árbol (Frequent Pattern Tree) donde almacena toda la información de las transacciones. Esta estructura permite comprimir la información de una base de datos de transacciones hasta 200 veces, haciendo posible que pueda ser cargada en memoria RAM. Una vez que la base de datos ha sido comprimida en una estructura FP-Tree, se divide en varias bases de datos condicionales, cada una asociada con un patrón frecuente. Finalmente, cada partición se analiza de forma separada y se concatenan los resultados obtenidos. En la mayoría de casos, FP-Growth es más rápido que Apriori.","code":""},{"path":"patrones-de-asociación.html","id":"eclat","chapter":"Capítulo 2 Patrones de asociación","heading":"2.7 Eclat","text":"En el 2000, Zaki propuso un nuevo algoritmo para encontrar patrones frecuentes (itemsets frecuentes) llamado Equivalence Class Transformation (Eclat). La principal diferencia entre este algoritmo y Apriori es la forma en que se escanean y analizan los datos. El algoritmo Apriori emplea transacciones almacenadas de forma horizontal, es decir, todos los elementos que forman una misma transacción están en la misma línea. El algoritmo Eclat, sin embargo, analiza las transacciones en formato vertical, donde cada línea contiene un item y las transacciones en las que aparece ese item.Para ilustrar el funcionamiento del algoritmo Eclat, se muestra un ejemplo simplificado. La siguiente tabla contiene la información de las transacciones en formato horizontal. El soporte mínimo para considerar un itemset frecuente es del 20%.En primer lugar, el algoritmo identifica los items que aparecen en el conjunto de todas las transacciones y los emplea para crear la primera columna de una nueva tabla. continuación, se añade cada item el identificador de las transacciones en las que aparece y se calcula su soporte. La siguiente tabla muestra el resultado de reestructurar los datos de formato horizontal vertical.Itemset (k = 1) Transacciones SoporteCalculando todas las posibles intersecciones de la columna Transacciones de la tabla k=1 se obtienen los itemsets de longitud k+1.Itemset (k = 2) Transacciones SoporteDe nuevo, con las intersecciones de las transacciones de la tabla Itemset (k = 2) se obtienen los itemsets k = 3. Gracias al principio downward closure, es necesario realizar la intersección de {i1,i5} e {i4,i5} ya que {i1,i4} es frecuente y por lo tanto tampoco lo es {i1,i4,i5}.Itemset (k = 3) Transacciones SoporteEl algoritmo finaliza cuando hay más itemsets frecuentes.Cabe destacar que, el algoritmo Eclat, permite la identificación de itemsets frecuentes pero genera reglas de asociación. pesar de ello, una vez identificados los itemsets frecuentes, se puede aplicar la segunda parte del algoritmo Apriori para obtenerlas.","code":""},{"path":"patrones-de-asociación.html","id":"arules-package-de-r-cran","chapter":"Capítulo 2 Patrones de asociación","heading":"2.8 Arules package de R-Cran","text":"El paquete de R arules implementa el algoritmo Apriori para la identificación de itemsets frecuentes y la creación de reglas de asociación través de la función apriori(). También implementa el algoritmo Eclat con la función eclat().Tanto \\(apriori()\\) como \\(eclat()\\) reciben como argumento un objeto transaction con los datos de las transacciones, un argumento parameter que determina las características de los itemsets o reglas generadas (por ejemplo, el soporte mínimo) y un argumento control que determina el comportamiento del algoritmo (por ejemplo, ordenación de los resultados). La función apriori() también se incluye el argumento aparence que impone restricciones sobre las reglas generadas, por ejemplo, crear solo reglas que contengan un determinado item. El resultado de ambas funciones es un objeto de tipo association que puede ser manipulado con toda una serie de funciones que ofrece arules. Entre las principales destacan:summary(): muestra un resumen de los resultados.inspect(): muestra los resultados.length(): número de elementos (reglas o itemsets) almacenados.items(): extrae los items que forman un itemset o una regla. En el caso de reglas, combina los items de antecedente (LHS) y del consecuente (RHS). .sort(): ordena los resultados.subset: filtrado de los resultados.","code":""},{"path":"patrones-de-asociación.html","id":"ejemplo-ticket-de-supermercado","chapter":"Capítulo 2 Patrones de asociación","heading":"2.9 Ejemplo ticket de supermercado","text":"Supóngase que se dispone del registro de todas las compras que se han realizado en un supermercado. El objetivo del análisis es identificar productos que tiendan comprarse de forma conjunta para así poder situarlos en posiciones cercanas dentro de la tienda y maximizar la probabilidad de que los clientes compren.Para este ejemplo se emplea el set de datos Groceries del paquete arules, que contiene un registro de todas las ventas realizadas por un supermercado durante 30 días. En total se dispone de 9835 transacciones formadas por combinaciones de 169 productos. El objeto Groceries almacena la información en un formato propio de este paquete (descrito más adelante). Para representar mejor lo que suele ocurrir en la práctica, se ha reestructurado la información en formato de tabla. Los datos pueden encontrarse en Github.Cada línea del archivo contiene la información de un item y el identificador de la transacción (compra) la que pertenece. Esta es la estructura en la que comúnmente se almacenan los datos dentro de una base de datos y que, en el ámbito de las transacciones, recibe el nombre de tabla larga o single. Otra forma en la que se pueden encontrar los datos de transacciones es en formato matriz o tabla ancha, en el que cada fila contiene todos los items que forman parte de una misma transacción, este formato recibe el nombre de cesta o basket.Tal y como se ha definido previamente, el concepto de transacción hace referencia al conjunto de items o eventos que ocurren de forma conjunta. Para este caso de estudio, compras de supermercado, cada transacción está formada por todos los productos que se compran la vez, es decir, el vínculo de unión es el cliente sino cada una de las “cestas de la compra”. Por ejemplo, la transacción con id_compra == 14 está formada por 3 items.","code":"\nlibrary(tidyverse)\n#> ── Attaching packages ─────────────────── tidyverse 1.3.1 ──\n#> ✔ ggplot2 3.4.2     ✔ purrr   1.0.1\n#> ✔ tibble  3.2.1     ✔ dplyr   1.1.1\n#> ✔ tidyr   1.3.0     ✔ stringr 1.5.0\n#> ✔ readr   2.1.2     ✔ forcats 0.5.1\n#> ── Conflicts ────────────────────── tidyverse_conflicts() ──\n#> ✖ dplyr::filter() masks stats::filter()\n#> ✖ dplyr::lag()    masks stats::lag()\nlibrary(readr)\ndatos <- read_csv(\"https://themys.sid.uncu.edu.ar/rpalma/R-cran/Analitica_Industrial/datos_groceries.csv\")\n#> Rows: 43367 Columns: 2\n#> ── Column specification ────────────────────────────────────\n#> Delimiter: \",\"\n#> chr (1): item\n#> dbl (1): id_compra\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# datos <- read_csv(file = \"datos_groceries.csv\", col_names = TRUE)\nhead(datos)\n#> # A tibble: 6 × 2\n#>   id_compra item               \n#>       <dbl> <chr>              \n#> 1         1 citrus fruit       \n#> 2         1 semi-finished bread\n#> 3         1 margarine          \n#> 4         1 ready soups        \n#> 5         2 tropical fruit     \n#> 6         2 yogurt\ndatos %>% filter(id_compra == 14) %>% pull(item)\n#> [1] \"frankfurter\" \"rolls/buns\"  \"soda\""},{"path":"patrones-de-asociación.html","id":"lectura-de-datos","chapter":"Capítulo 2 Patrones de asociación","heading":"2.9.1 Lectura de datos","text":"Con la función read.transactions() se pueden leer directamente los datos de archivos tipo texto y almacenarlos en un objeto de tipo transactions, que es la estructura de almacenamiento que emplea arules. Esta función tiene los siguientes argumentos:file: nombre del archivo que se quiere leer.format: estructura en la que se encuentran almacenados los datos, “basket” si cada línea del archivo es una transacción completa, o “single” si cada línea representa un item.sep: tipo de separación de los campos.cols: si el formato es de tipo “basket”, un entero que indica la columna que contiene el identificador de las transacciones. Para el formato “single”, es un vector con los nombres (o posiciones) de las dos columnas que identifican las transacciones y los items, respectivamente.rm.duplicates: valor lógico indicando si se eliminan los items duplicados en una misma transacción. Por lo general, es conveniente eliminarlos, el interés suele estar en qué items ocurren de forma conjunta, en qué cantidad.quote: carácter que se utiliza como comillas.skip: número de líneas que hay que saltar desde el comienzo del fichero.Es importante recordar que los objetos de tipo transactions solo trabajan con información booleana, es decir, con la presencia o de cada uno de los items en la transacción. Por lo tanto, si el set de datos contiene variables numéricas, estas deben de ser discretizadas en intervalos o niveles. Para discretizar una variable numérica, se puede emplear la función discretize() o bien otras alternativas como la función case_when() del paquete dplyr.Los objetos transactions, se almacenan internamente como un tipo de matriz binaria. Se trata de una matriz de valores 0/1, con una fila por cada transacción, en este caso cada compra, y una columna por cada posible item, en este caso productos. La posición de la matriz (,j) tiene el valor 1 si la transacción contiene el item j.También es posible convertir un objeto dataframe en uno de tipo transactions con la función (dataframe, “transactions”). Para lograrlo, primero hay que convertir el dataframe en una lista en la que cada elemento contiene los items de una transacción. Este proceso puede ser muy lento si el dataframe tiene muchos registros, por lo que suele ser mejor crear un archivo de texto con los datos e importarlo mediante read.transactions().Otra alternativa es convertir el dataframe en una matriz 0/1 en la que cada fila es una transacción y cada columna uno de los posibles items.Exploración de itemsUno de los primeros análisis que conviene realizar cuando se trabaja con transacciones es explorar su contenido y tamaño, es decir, el número de items que las forman y cuáles son. Mediante la función inspect() se muestran los items que forman cada transacción.También es posible mostrar los resultados en formato de dataframe con la función DATAFRAME() o con (transacciones, “dataframe”).Para extraer el tamaño de cada transacción se emplea la función size().CuantilesTamañosLa gran mayoría de clientes compra entre 3 y 4 productos y el 90% de ellos compra como máximo 9.El siguiente análisis básico consiste en identificar cuáles son los items más frecuentes (los que tienen mayor soporte) dentro del conjunto de todas las transacciones. Con la función itemFrequency() se puede extraer esta información de un objeto tipo transactions. El nombre de esta función puede causar confusión. Por “frecuencia” se hace referencia al soporte de cada item, que es la fracción de transacciones que contienen dicho item respecto al total de todas las transacciones. Esto es distinto la frecuencia de un item respecto al total de items, de ahí que la suma de todos los soportes sea 1.Si se indica el argumento type = “absolute”, la función itemFrequency() devuelve el número de transacciones en las que aparece cada item.El listado anterior muestra que los 5 productos que más se venden son: whole milk, vegetables, rolls/buns y soda.Es muy importante estudiar cómo se distribuye el soporte de los items individuales en un conjunto de transacciones antes identificar itemsets frecuentes o crear reglas de asociación, ya que, dependiendo del caso, tendrá sentido emplear un límite de soporte u otro. Por lo general, cuando el número de posibles items es muy grande (varios miles) prácticamente todos los artículos son raros, por lo que los soportes son muy bajos.","code":"\n# IMPORTACIÓN DIRECTA DE LOS DATOS A UN OBJETO TIPO TRANSACTION\n# ==============================================================================\nlibrary(arules)\n#> Loading required package: Matrix\n#> \n#> Attaching package: 'Matrix'\n#> The following objects are masked from 'package:tidyr':\n#> \n#>     expand, pack, unpack\n#> \n#> Attaching package: 'arules'\n#> The following object is masked from 'package:dplyr':\n#> \n#>     recode\n#> The following objects are masked from 'package:base':\n#> \n#>     abbreviate, write\ntransacciones <- read.transactions(file = \"datos_groceries.csv\",\n                                   format = \"single\",\n                                   sep = \",\",\n                                   header = TRUE,\n                                   cols = c(\"id_compra\", \"item\"),\n                                   rm.duplicates = TRUE)\ntransacciones\n#> transactions in sparse format with\n#>  9835 transactions (rows) and\n#>  169 items (columns)\n\ncolnames(transacciones)[1:5]\n#> [1] \"abrasive cleaner\" \"artif. sweetener\" \"baby cosmetics\"  \n#> [4] \"baby food\"        \"bags\"\n\nrownames(transacciones)[1:5]\n#> [1] \"1\"    \"10\"   \"100\"  \"1000\" \"1001\"\n# CONVERSIÓN DE UN DATAFRAME A UN OBJETO TIPO TRANSACTION\n# ==============================================================================\n# Se convierte el dataframe a una lista en la que cada elemento  contiene los\n# items de una transacción\ndatos_split <- split(x = datos$item, f = datos$id_compra)\ntransacciones <- as(datos_split, Class = \"transactions\")\ntransacciones\n#> transactions in sparse format with\n#>  9835 transactions (rows) and\n#>  169 items (columns)\n\n\n# CONVERSIÓN DE UNA MATRIZ A UN OBJETO TIPO TRANSACTION\n# ==============================================================================\ndatos_matriz <- datos %>%\n                as.data.frame() %>%\n                mutate(valor = 1) %>%\n                spread(key = item, value = valor, fill = 0) %>%\n                column_to_rownames(var = \"id_compra\") %>%\n                as.matrix()\ntransacciones <- as(datos_matriz, Class = \"transactions\")\ntransacciones\n#> transactions in sparse format with\n#>  9835 transactions (rows) and\n#>  169 items (columns)\ninspect(transacciones[1:5])\n#>     items                       transactionID\n#> [1] {citrus fruit,                           \n#>      margarine,                              \n#>      ready soups,                            \n#>      semi-finished bread}                   1\n#> [2] {coffee,                                 \n#>      tropical fruit,                         \n#>      yogurt}                                2\n#> [3] {whole milk}                            3\n#> [4] {cream cheese,                           \n#>      meat spreads,                           \n#>      pip fruit,                              \n#>      yogurt}                                4\n#> [5] {condensed milk,                         \n#>      long life bakery product,               \n#>      other vegetables,                       \n#>      whole milk}                            5\ndf_transacciones <- as(transacciones, Class = \"data.frame\")\n# Para que el tamaño de la tabla se ajuste mejor, se convierte el dataframe a tibble\nas.tibble(df_transacciones) %>% head()\n#> Warning: `as.tibble()` was deprecated in tibble 2.0.0.\n#> ℹ Please use `as_tibble()` instead.\n#> ℹ The signature and semantics have changed, see\n#>   `?as_tibble`.\n#> This warning is displayed once every 8 hours.\n#> Call `lifecycle::last_lifecycle_warnings()` to see where\n#> this warning was generated.\n#> # A tibble: 6 × 2\n#>   items                                        transactionID\n#>   <chr>                                        <chr>        \n#> 1 {citrus fruit,margarine,ready soups,semi-fi… 1            \n#> 2 {coffee,tropical fruit,yogurt}               2            \n#> 3 {whole milk}                                 3            \n#> 4 {cream cheese,meat spreads,pip fruit,yogurt} 4            \n#> 5 {condensed milk,long life bakery product,ot… 5            \n#> 6 {abrasive cleaner,butter,rice,whole milk,yo… 6\ntamanyos <- size(transacciones)\nsummary(tamanyos)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>   1.000   2.000   3.000   4.409   6.000  32.000\nquantile(tamanyos, probs = seq(0,1,0.1))\n#>   0%  10%  20%  30%  40%  50%  60%  70%  80%  90% 100% \n#>    1    1    1    2    3    3    4    5    7    9   32\ndata.frame(tamanyos) %>%\n  ggplot(aes(x = tamanyos)) +\n  geom_histogram() +\n  labs(title = \"Distribución del tamaño de las transacciones\",\n       x = \"Tamaño\") +\n  theme_bw()\n#> `stat_bin()` using `bins = 30`. Pick better value with\n#> `binwidth`.\nfrecuencia_items <- itemFrequency(x = transacciones, type = \"relative\")\nfrecuencia_items %>% sort(decreasing = TRUE) %>% head(5)\n#>       whole milk other vegetables       rolls/buns \n#>        0.2555160        0.1934926        0.1839349 \n#>             soda           yogurt \n#>        0.1743772        0.1395018\nfrecuencia_items <- itemFrequency(x = transacciones, type = \"absolute\")\nfrecuencia_items %>% sort(decreasing = TRUE) %>% head(5)\n#>       whole milk other vegetables       rolls/buns \n#>             2513             1903             1809 \n#>             soda           yogurt \n#>             1715             1372"},{"path":"patrones-de-asociación.html","id":"itemsets","chapter":"Capítulo 2 Patrones de asociación","heading":"2.9.2 Itemsets","text":"Con la función apriori() se puede aplicar el algoritmo Apriori un objeto de tipo transactions y extraer tanto itemsets frecuentes como reglas de asociación que superen un determinado soporte y confianza. Los argumentos de esta función son:data: un objeto del tipo transactions o un objeto que pueda ser convertido tipo transactions, por ejemplo un dataframe o una matriz binaria.parameter: lista en la que se indican los parámetros del algoritmo.support: soporte mínimo que debe tener un itemset para ser considerado frecuente. Por defecto es 0.1.minlen: número mínimo de items que debe tener un itemset para ser incluido en los resultados. Por defecto 1.maxlen: número máximo de items que puede tener un itemset para ser incluido en los resultados. Por defecto 10.target: tipo de resultados que debe de generar el algoritmo, pueden ser “frequent itemsets”, “maximally frequent itemsets”, “closed frequent itemsets”, “rules” o “hyperedgesets”.confidence: confianza mínima que debe de tener una regla para ser incluida en los resultados. Por defecto 0.8.maxtime: tiempo máximo que puede estar el algoritmo buscando subsets. Por defecto 5 segundos.appearance: lista que permite definir patrones para restringir el espacio de búsqueda, por ejemplo, especificando qué items pueden o pueden aparecer.control: lista que permite modificar aspectos internos de algoritmo como la ordenación de los itemsets, si se construye un árbol con las transacciones, aspectos relacionados con el uso de memoria, etc.Se procede extraer aquellos itemsets, incluidos los formados por un único item, que hayan sido comprados al menos 30 veces. En un caso real, este valor sería excesivamente bajo si se tiene en cuenta la cantidad total de transacciones, sin embargo, se emplea 30 para que en los resultados aparezcan un número suficiente de itemsets y reglas de asociación que permitan mostrar las posibilidades de análisis que ofrece el paquete arules.Se han encontrado un total de 2226 itemsets frecuentes que superan el soporte mínimo de 0.003908286, la mayoría de ellos (1140) formados por dos items. En el siguiente listado se muestran los 20 itemsets con mayor soporte que, como cabe esperar, son los formados por items individuales (los itemsets de menor tamaño).Se muestran los top 20 itemsets de mayor menor soporteSi se quieren excluir del análisis los itemsets formados únicamente por un solo item, se puede, o bien aplicar de nuevo la función apriori() especificando minlen = 2, o filtrar los resultados con la función size().\nSe muestran los 20 itemsets más frecuentes formados por más de un item.","code":"\nsoporte <- 30 / dim(transacciones)[1]\nitemsets <- apriori(data = transacciones,\n                    parameter = list(support = soporte,\n                                     minlen = 1,\n                                     maxlen = 20,\n                                     target = \"frequent itemset\"))\n#> Apriori\n#> \n#> Parameter specification:\n#>  confidence minval smax arem  aval originalSupport maxtime\n#>          NA    0.1    1 none FALSE            TRUE       5\n#>     support minlen maxlen            target  ext\n#>  0.00305033      1     20 frequent itemsets TRUE\n#> \n#> Algorithmic control:\n#>  filter tree heap memopt load sort verbose\n#>     0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n#> \n#> Absolute minimum support count: 30 \n#> \n#> set item appearances ...[0 item(s)] done [0.00s].\n#> set transactions ...[169 item(s), 9835 transaction(s)] done [0.01s].\n#> sorting and recoding items ... [136 item(s)] done [0.00s].\n#> creating transaction tree ... done [0.01s].\n#> checking subsets of size 1 2 3 4 5 done [0.01s].\n#> sorting transactions ... done [0.00s].\n#> writing ... [2226 set(s)] done [0.00s].\n#> creating S4 object  ... done [0.00s].\nsummary(itemsets)\n#> set of 2226 itemsets\n#> \n#> most frequent items:\n#>       whole milk other vegetables           yogurt \n#>              556              468              316 \n#>  root vegetables       rolls/buns          (Other) \n#>              251              241             3536 \n#> \n#> element (itemset/transaction) length distribution:sizes\n#>    1    2    3    4    5 \n#>  136 1140  850   98    2 \n#> \n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>   1.000   2.000   2.000   2.412   3.000   5.000 \n#> \n#> summary of quality measures:\n#>     support             count        \n#>  Min.   :0.003050   Min.   :  30.00  \n#>  1st Qu.:0.003660   1st Qu.:  36.00  \n#>  Median :0.004779   Median :  47.00  \n#>  Mean   :0.007879   Mean   :  77.49  \n#>  3rd Qu.:0.007219   3rd Qu.:  71.00  \n#>  Max.   :0.255516   Max.   :2513.00  \n#> \n#> includes transaction ID lists: FALSE \n#> \n#> mining info:\n#>           data ntransactions    support confidence\n#>  transacciones          9835 0.00305033          1\n#>                                                                                                                      call\n#>  apriori(data = transacciones, parameter = list(support = soporte, minlen = 1, maxlen = 20, target = \"frequent itemset\"))\ntop_20_itemsets <- sort(itemsets, by = \"support\", decreasing = TRUE)[1:20]\ninspect(top_20_itemsets)\n#>      items                          support    count\n#> [1]  {whole milk}                   0.25551601 2513 \n#> [2]  {other vegetables}             0.19349263 1903 \n#> [3]  {rolls/buns}                   0.18393493 1809 \n#> [4]  {soda}                         0.17437722 1715 \n#> [5]  {yogurt}                       0.13950178 1372 \n#> [6]  {bottled water}                0.11052364 1087 \n#> [7]  {root vegetables}              0.10899847 1072 \n#> [8]  {tropical fruit}               0.10493137 1032 \n#> [9]  {shopping bags}                0.09852567  969 \n#> [10] {sausage}                      0.09395018  924 \n#> [11] {pastry}                       0.08896797  875 \n#> [12] {citrus fruit}                 0.08276563  814 \n#> [13] {bottled beer}                 0.08052872  792 \n#> [14] {newspapers}                   0.07981698  785 \n#> [15] {canned beer}                  0.07768175  764 \n#> [16] {pip fruit}                    0.07564820  744 \n#> [17] {other vegetables, whole milk} 0.07483477  736 \n#> [18] {fruit/vegetable juice}        0.07229283  711 \n#> [19] {whipped/sour cream}           0.07168277  705 \n#> [20] {brown bread}                  0.06487036  638\nas(top_20_itemsets, Class = \"data.frame\") %>%\n  ggplot(aes(x = reorder(items, support), y = support)) +\n  geom_col() +\n  coord_flip() +\n  labs(title = \"Itemsets más frecuentes\", x = \"itemsets\") +\n  theme_bw()\ninspect(sort(itemsets[size(itemsets) > 1], decreasing = TRUE)[1:20])\n#>      items                               support    count\n#> [1]  {other vegetables, whole milk}      0.07483477 736  \n#> [2]  {rolls/buns, whole milk}            0.05663447 557  \n#> [3]  {whole milk, yogurt}                0.05602440 551  \n#> [4]  {root vegetables, whole milk}       0.04890696 481  \n#> [5]  {other vegetables, root vegetables} 0.04738180 466  \n#> [6]  {other vegetables, yogurt}          0.04341637 427  \n#> [7]  {other vegetables, rolls/buns}      0.04260295 419  \n#> [8]  {tropical fruit, whole milk}        0.04229792 416  \n#> [9]  {soda, whole milk}                  0.04006101 394  \n#> [10] {rolls/buns, soda}                  0.03833249 377  \n#> [11] {other vegetables, tropical fruit}  0.03589222 353  \n#> [12] {bottled water, whole milk}         0.03436706 338  \n#> [13] {rolls/buns, yogurt}                0.03436706 338  \n#> [14] {pastry, whole milk}                0.03324860 327  \n#> [15] {other vegetables, soda}            0.03274021 322  \n#> [16] {whipped/sour cream, whole milk}    0.03223183 317  \n#> [17] {rolls/buns, sausage}               0.03060498 301  \n#> [18] {citrus fruit, whole milk}          0.03050330 300  \n#> [19] {pip fruit, whole milk}             0.03009659 296  \n#> [20] {domestic eggs, whole milk}         0.02999492 295"},{"path":"patrones-de-asociación.html","id":"filtrado-de-itemsets","chapter":"Capítulo 2 Patrones de asociación","heading":"2.10 Filtrado de itemsets","text":"Una vez que los itemsets frecuentes han sido identificados mediante el algoritmo Apripori, pueden ser filtrados con la función subset(). Esta función recibe dos argumentos: un objeto itemset o rules y una condición lógica que tienen que cumplir las reglas/itemsets para ser seleccionados. La siguiente tabla muestra los operadores permitidos:Como esta función tiene el mismo nombre que una función del paquete básico de R, para evitar errores, es conveniente especificar el paquete donde se encuentra.Se procede identificar aquellos itemsets frecuentes que contienen el item newspapers.Se repite el proceso pero, esta vez, con aquellos itemsets que contienen newspapers y whole milk.Puede observarse que muchos itemsets están su vez contenidos en itemsets de orden superior, es decir, existen itemsets que son subsets de otros. Para identificar cuáles son, o cuales lo son, se puede emplear la función .subset(). Encontrar los itemsets que son subsets de otros itemsets implica comparar todos los pares de itemsets y determinar si uno está contenido en el otro. La función .subset() realiza comparaciones entre dos conjuntos de itemsets y devuelve una matriz lógica que determina si el itemset de la fila está contenido en cada itemset de las columnas.Para encontrar los subsets dentro de un conjunto de itemsets, se compara el\nconjunto de itemsets con sigo mismo.","code":"\nitemsets_filtrado <- arules::subset(itemsets,\n                                    subset = items %in% \"newspapers\")\nitemsets_filtrado\n#> set of 80 itemsets\ninspect(itemsets_filtrado[1:10])\n#>      items                                  support    \n#> [1]  {newspapers}                           0.079816980\n#> [2]  {meat, newspapers}                     0.003050330\n#> [3]  {newspapers, sliced cheese}            0.003152008\n#> [4]  {newspapers, UHT-milk}                 0.004270463\n#> [5]  {newspapers, oil}                      0.003152008\n#> [6]  {newspapers, onions}                   0.003152008\n#> [7]  {hygiene articles, newspapers}         0.003050330\n#> [8]  {newspapers, sugar}                    0.003152008\n#> [9]  {newspapers, waffles}                  0.004168785\n#> [10] {long life bakery product, newspapers} 0.003457041\n#>      count\n#> [1]  785  \n#> [2]   30  \n#> [3]   31  \n#> [4]   42  \n#> [5]   31  \n#> [6]   31  \n#> [7]   30  \n#> [8]   31  \n#> [9]   41  \n#> [10]  34\nitemsets_filtrado <- arules::subset(itemsets,\n                                    subset = items %ain% c(\"newspapers\", \"whole milk\"))\nitemsets_filtrado\n#> set of 16 itemsets\n\ninspect(itemsets_filtrado[1:10])\n#>      items                                    support    \n#> [1]  {newspapers, whole milk}                 0.027351296\n#> [2]  {chocolate, newspapers, whole milk}      0.003152008\n#> [3]  {brown bread, newspapers, whole milk}    0.004067107\n#> [4]  {margarine, newspapers, whole milk}      0.003152008\n#> [5]  {butter, newspapers, whole milk}         0.003152008\n#> [6]  {newspapers, pastry, whole milk}         0.003863752\n#> [7]  {citrus fruit, newspapers, whole milk}   0.003355363\n#> [8]  {newspapers, sausage, whole milk}        0.003050330\n#> [9]  {bottled water, newspapers, whole milk}  0.004067107\n#> [10] {newspapers, tropical fruit, whole milk} 0.005083884\n#>      count\n#> [1]  269  \n#> [2]   31  \n#> [3]   40  \n#> [4]   31  \n#> [5]   31  \n#> [6]   38  \n#> [7]   33  \n#> [8]   30  \n#> [9]   40  \n#> [10]  50\nsubsets <- is.subset(x = itemsets, y = itemsets, sparse = FALSE)\n\n# Para conocer el total de itemsets que son subsets de otros itemsets se cuenta el número total de TRUE en la matriz resultante.\n\n# La suma de una matriz lógica devuelve el número de TRUEs\nsum(subsets)\n#> [1] 11038"},{"path":"patrones-de-asociación.html","id":"reglas-de-asociación","chapter":"Capítulo 2 Patrones de asociación","heading":"2.11 Reglas de asociación","text":"Para crear las reglas de asociación se sigue el mismo proceso que para obtener itemsets frecuentes pero, además de especificar un soporte mínimo, se tiene que establecer una confianza mínima para que una regla se incluya en los resultados. En este caso, se emplea una confianza mínima del 70%.Se han identificado un total de 19 reglas, la mayoría de ellas formadas por 4 items en el antecedente (parte izquierda de la regla).","code":"\nsoporte <- 30 / dim(transacciones)[1]\nreglas <- apriori(data = transacciones,\n                  parameter = list(support = soporte,\n                                   confidence = 0.70,\n                                   # Se especifica que se creen reglas\n                                   target = \"rules\"))\n#> Apriori\n#> \n#> Parameter specification:\n#>  confidence minval smax arem  aval originalSupport maxtime\n#>         0.7    0.1    1 none FALSE            TRUE       5\n#>     support minlen maxlen target  ext\n#>  0.00305033      1     10  rules TRUE\n#> \n#> Algorithmic control:\n#>  filter tree heap memopt load sort verbose\n#>     0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n#> \n#> Absolute minimum support count: 30 \n#> \n#> set item appearances ...[0 item(s)] done [0.00s].\n#> set transactions ...[169 item(s), 9835 transaction(s)] done [0.01s].\n#> sorting and recoding items ... [136 item(s)] done [0.00s].\n#> creating transaction tree ... done [0.01s].\n#> checking subsets of size 1 2 3 4 5 done [0.01s].\n#> writing ... [19 rule(s)] done [0.00s].\n#> creating S4 object  ... done [0.00s].\n\nreglas\n#> set of 19 rules\nsummary(reglas)\n#> set of 19 rules\n#> \n#> rule length distribution (lhs + rhs):sizes\n#> 3 4 5 \n#> 7 9 3 \n#> \n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>   3.000   3.000   4.000   3.789   4.000   5.000 \n#> \n#> summary of quality measures:\n#>     support           confidence        coverage       \n#>  Min.   :0.003050   Min.   :0.7000   Min.   :0.003559  \n#>  1st Qu.:0.003203   1st Qu.:0.7047   1st Qu.:0.004525  \n#>  Median :0.003559   Median :0.7164   Median :0.004982  \n#>  Mean   :0.003767   Mean   :0.7373   Mean   :0.005143  \n#>  3rd Qu.:0.004169   3rd Qu.:0.7500   3rd Qu.:0.005592  \n#>  Max.   :0.005694   Max.   :0.8857   Max.   :0.008134  \n#>       lift           count      \n#>  Min.   :2.740   Min.   :30.00  \n#>  1st Qu.:2.758   1st Qu.:31.50  \n#>  Median :2.804   Median :35.00  \n#>  Mean   :3.044   Mean   :37.05  \n#>  3rd Qu.:2.984   3rd Qu.:41.00  \n#>  Max.   :4.578   Max.   :56.00  \n#> \n#> mining info:\n#>           data ntransactions    support confidence\n#>  transacciones          9835 0.00305033        0.7\n#>                                                                                                    call\n#>  apriori(data = transacciones, parameter = list(support = soporte, confidence = 0.7, target = \"rules\"))\ninspect(sort(x = reglas, decreasing = TRUE, by = \"confidence\"))\n#>      lhs                      rhs                    support confidence    coverage     lift count\n#> [1]  {citrus fruit,                                                                               \n#>       root vegetables,                                                                            \n#>       tropical fruit,                                                                             \n#>       whole milk}          => {other vegetables} 0.003152008  0.8857143 0.003558719 4.577509    31\n#> [2]  {butter,                                                                                     \n#>       root vegetables,                                                                            \n#>       yogurt}              => {whole milk}       0.003050330  0.7894737 0.003863752 3.089723    30\n#> [3]  {citrus fruit,                                                                               \n#>       root vegetables,                                                                            \n#>       tropical fruit}      => {other vegetables} 0.004473818  0.7857143 0.005693950 4.060694    44\n#> [4]  {brown bread,                                                                                \n#>       other vegetables,                                                                           \n#>       root vegetables}     => {whole milk}       0.003152008  0.7750000 0.004067107 3.033078    31\n#> [5]  {butter,                                                                                     \n#>       onions}              => {whole milk}       0.003050330  0.7500000 0.004067107 2.935237    30\n#> [6]  {curd,                                                                                       \n#>       tropical fruit,                                                                             \n#>       yogurt}              => {whole milk}       0.003965430  0.7500000 0.005287239 2.935237    39\n#> [7]  {curd,                                                                                       \n#>       domestic eggs}       => {whole milk}       0.004778851  0.7343750 0.006507372 2.874086    47\n#> [8]  {butter,                                                                                     \n#>       tropical fruit,                                                                             \n#>       yogurt}              => {whole milk}       0.003355363  0.7333333 0.004575496 2.870009    33\n#> [9]  {root vegetables,                                                                            \n#>       tropical fruit,                                                                             \n#>       whipped/sour cream}  => {other vegetables} 0.003355363  0.7333333 0.004575496 3.789981    33\n#> [10] {butter,                                                                                     \n#>       curd}                => {whole milk}       0.004880529  0.7164179 0.006812405 2.803808    48\n#> [11] {domestic eggs,                                                                              \n#>       sugar}               => {whole milk}       0.003558719  0.7142857 0.004982206 2.795464    35\n#> [12] {other vegetables,                                                                           \n#>       root vegetables,                                                                            \n#>       tropical fruit,                                                                             \n#>       yogurt}              => {whole milk}       0.003558719  0.7142857 0.004982206 2.795464    35\n#> [13] {baking powder,                                                                              \n#>       yogurt}              => {whole milk}       0.003253686  0.7111111 0.004575496 2.783039    32\n#> [14] {tropical fruit,                                                                             \n#>       whipped/sour cream,                                                                         \n#>       yogurt}              => {whole milk}       0.004372140  0.7049180 0.006202339 2.758802    43\n#> [15] {citrus fruit,                                                                               \n#>       other vegetables,                                                                           \n#>       root vegetables,                                                                            \n#>       tropical fruit}      => {whole milk}       0.003152008  0.7045455 0.004473818 2.757344    31\n#> [16] {butter,                                                                                     \n#>       pork}                => {whole milk}       0.003863752  0.7037037 0.005490595 2.754049    38\n#> [17] {butter,                                                                                     \n#>       coffee}              => {whole milk}       0.003355363  0.7021277 0.004778851 2.747881    33\n#> [18] {domestic eggs,                                                                              \n#>       other vegetables,                                                                           \n#>       whipped/sour cream}  => {whole milk}       0.003558719  0.7000000 0.005083884 2.739554    35\n#> [19] {root vegetables,                                                                            \n#>       tropical fruit,                                                                             \n#>       yogurt}              => {whole milk}       0.005693950  0.7000000 0.008134215 2.739554    56"},{"path":"patrones-de-asociación.html","id":"evaluación-de-las-reglas","chapter":"Capítulo 2 Patrones de asociación","heading":"2.12 Evaluación de las reglas","text":"Además de la confianza y el soporte, existen otras métricas que permiten cuantificar la calidad de las reglas y la probabilidad de que reflejen relaciones reales. Algunas de las más empleadas son:Lift: el estadístico lift compara la frecuencia observada de una regla con la frecuencia esperada simplemente por azar (si la regla existe realmente). El valor lift de una regla “si X, entonces Y” se obtiene acorde la siguiente ecuación:\nsoporte(union(X,Y))soporte(X) * soporte(Y)\nCuanto más se aleje el valor de lift de 1, más evidencias de que la regla se debe un artefacto aleatorio, es decir, mayor la evidencia de que la regla representa un patrón real.Coverage: es el soporte de la parte izquierda de la regla (antecedente). Se interpreta como la frecuencia con la que el antecedente aparece en el conjunto de transacciones.Fisher exact test: devuelve el p-value asociado la probabilidad de observar la regla solo por azar.Con la función interestMeasure() se pueden calcular más de 20 métricas distintas para un conjunto de reglas creadas con la función apriori().Estas nuevas métricas pueden añadirse al objeto que contiene las reglas.","code":"\nmetricas <- interestMeasure(reglas, measure = c(\"coverage\", \"fishersExactTest\"),\n                            transactions = transacciones)\nmetricas\n#>       coverage fishersExactTest\n#> 1  0.004575496     1.775138e-10\n#> 2  0.004067107     7.502990e-11\n#> 3  0.004982206     1.990017e-11\n#> 4  0.004778851     1.582670e-10\n#> 5  0.006812405     2.857678e-15\n#> 6  0.006507372     1.142131e-15\n#> 7  0.005490595     5.673757e-12\n#> 8  0.005287239     1.003422e-13\n#> 9  0.004067107     8.092894e-12\n#> 10 0.004575496     2.336708e-11\n#> 11 0.003863752     7.584014e-12\n#> 12 0.005083884     5.006888e-11\n#> 13 0.004575496     5.680562e-15\n#> 14 0.006202339     2.037822e-13\n#> 15 0.005693950     1.301061e-21\n#> 16 0.008134215     7.433712e-17\n#> 17 0.004473818     5.003096e-10\n#> 18 0.003558719     1.459542e-18\n#> 19 0.004982206     1.990017e-11\ncoverage\n#> standardGeneric for \"coverage\" defined from package \"arules\"\n#> \n#> function (x, transactions = NULL, reuse = TRUE) \n#> standardGeneric(\"coverage\")\n#> <bytecode: 0x561e295cf9a8>\n#> <environment: 0x561e295bd580>\n#> Methods may be defined for arguments: x, transactions, reuse\n#> Use  showMethods(coverage)  for currently available ones.\nquality(reglas) <- cbind(quality(reglas), metricas)\n# inspect(sort(x = reglas, decreasing = TRUE, by = \"confidence\"))\ndf_reglas <- as(reglas, Class = \"data.frame\") \ndf_reglas %>% as.tibble() %>% arrange(desc(confidence)) %>% head()\n#> # A tibble: 6 × 8\n#>   rules   support confidence coverage  lift count coverage.1\n#>   <chr>     <dbl>      <dbl>    <dbl> <dbl> <int>      <dbl>\n#> 1 {citru… 0.00315      0.886  0.00356  4.58    31    0.00356\n#> 2 {butte… 0.00305      0.789  0.00386  3.09    30    0.00386\n#> 3 {citru… 0.00447      0.786  0.00569  4.06    44    0.00569\n#> 4 {brown… 0.00315      0.775  0.00407  3.03    31    0.00407\n#> 5 {butte… 0.00305      0.75   0.00407  2.94    30    0.00407\n#> 6 {curd,… 0.00397      0.75   0.00529  2.94    39    0.00529\n#> # ℹ 1 more variable: fishersExactTest <dbl>\nrules\n#> function (rhs, lhs, itemLabels = NULL, quality = data.frame()) \n#> {\n#>     if (!is(lhs, \"itemMatrix\")) \n#>         lhs <- encode(lhs, itemLabels = itemLabels)\n#>     if (!is(rhs, \"itemMatrix\")) \n#>         rhs <- encode(rhs, itemLabels = itemLabels)\n#>     new(\"rules\", lhs = lhs, rhs = rhs, quality = quality)\n#> }\n#> <bytecode: 0x561e1fd7e578>\n#> <environment: namespace:arules>"},{"path":"patrones-de-asociación.html","id":"filtrado-de-reglas","chapter":"Capítulo 2 Patrones de asociación","heading":"2.13 Filtrado de reglas","text":"Cuando se crean reglas de asociación, pueden ser interesantes únicamente aquellas que contienen un determinado conjunto de items en el antecedente o en el consecuente. Con arules existen varias formas de seleccionar solo determinadas reglas.","code":""},{"path":"patrones-de-asociación.html","id":"restringir-las-reglas-que-se-crean","chapter":"Capítulo 2 Patrones de asociación","heading":"2.13.1 Restringir las reglas que se crean","text":"Es posible restringir los items que aparecen en el lado izquierdo y/o derecho de la reglas la hora de crearlas, por ejemplo, supóngase que solo son de interés reglas que muestren productos que se vendan junto con vegetables. Esto significa que el item vegetables, debe aparecer en el lado derecho (rhs).Esto mismo puede hacerse con el lado izquierdo (lhs) o en ambos ().","code":"\nsoporte <- 30 / dim(transacciones)[1]\nreglas_vegetables <- apriori(data = transacciones,\n                             parameter = list(support = soporte,\n                                              confidence = 0.70,\n                                              # Se especifica que se creen reglas\n                                              target = \"rules\"),\n                             appearance = list(rhs = \"other vegetables\"))\n#> Apriori\n#> \n#> Parameter specification:\n#>  confidence minval smax arem  aval originalSupport maxtime\n#>         0.7    0.1    1 none FALSE            TRUE       5\n#>     support minlen maxlen target  ext\n#>  0.00305033      1     10  rules TRUE\n#> \n#> Algorithmic control:\n#>  filter tree heap memopt load sort verbose\n#>     0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n#> \n#> Absolute minimum support count: 30 \n#> \n#> set item appearances ...[1 item(s)] done [0.00s].\n#> set transactions ...[169 item(s), 9835 transaction(s)] done [0.01s].\n#> sorting and recoding items ... [136 item(s)] done [0.00s].\n#> creating transaction tree ... done [0.01s].\n#> checking subsets of size 1 2 3 4 5 done [0.01s].\n#> writing ... [3 rule(s)] done [0.00s].\n#> creating S4 object  ... done [0.00s].\nsummary(reglas_vegetables)\n#> set of 3 rules\n#> \n#> rule length distribution (lhs + rhs):sizes\n#> 4 5 \n#> 2 1 \n#> \n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>   4.000   4.000   4.000   4.333   4.500   5.000 \n#> \n#> summary of quality measures:\n#>     support           confidence        coverage       \n#>  Min.   :0.003152   Min.   :0.7333   Min.   :0.003559  \n#>  1st Qu.:0.003254   1st Qu.:0.7595   1st Qu.:0.004067  \n#>  Median :0.003355   Median :0.7857   Median :0.004575  \n#>  Mean   :0.003660   Mean   :0.8016   Mean   :0.004609  \n#>  3rd Qu.:0.003915   3rd Qu.:0.8357   3rd Qu.:0.005135  \n#>  Max.   :0.004474   Max.   :0.8857   Max.   :0.005694  \n#>       lift           count     \n#>  Min.   :3.790   Min.   :31.0  \n#>  1st Qu.:3.925   1st Qu.:32.0  \n#>  Median :4.061   Median :33.0  \n#>  Mean   :4.143   Mean   :36.0  \n#>  3rd Qu.:4.319   3rd Qu.:38.5  \n#>  Max.   :4.578   Max.   :44.0  \n#> \n#> mining info:\n#>           data ntransactions    support confidence\n#>  transacciones          9835 0.00305033        0.7\n#>                                                                                                                                                 call\n#>  apriori(data = transacciones, parameter = list(support = soporte, confidence = 0.7, target = \"rules\"), appearance = list(rhs = \"other vegetables\"))\ninspect(reglas_vegetables)\n#>     lhs                     rhs                    support confidence    coverage     lift count\n#> [1] {root vegetables,                                                                           \n#>      tropical fruit,                                                                            \n#>      whipped/sour cream} => {other vegetables} 0.003355363  0.7333333 0.004575496 3.789981    33\n#> [2] {citrus fruit,                                                                              \n#>      root vegetables,                                                                           \n#>      tropical fruit}     => {other vegetables} 0.004473818  0.7857143 0.005693950 4.060694    44\n#> [3] {citrus fruit,                                                                              \n#>      root vegetables,                                                                           \n#>      tropical fruit,                                                                            \n#>      whole milk}         => {other vegetables} 0.003152008  0.8857143 0.003558719 4.577509    31"},{"path":"patrones-de-asociación.html","id":"filtrar-reglas-creadas","chapter":"Capítulo 2 Patrones de asociación","heading":"2.14 Filtrar reglas creadas","text":"También es posible filtrar las reglas una vez que han sido creadas. Por ejemplo, se procede filtrar aquellas reglas que contienen vegetables y citrus fruit en el antecedente.","code":"\nfiltrado_reglas <- subset(x = reglas,\n                          subset = lhs %ain% c(\"other vegetables\",\"citrus fruit\"))\ninspect(filtrado_reglas)\n#>     lhs                    rhs              support confidence    coverage     lift count    coverage fishersExactTest\n#> [1] {citrus fruit,                                                                                                    \n#>      other vegetables,                                                                                                \n#>      root vegetables,                                                                                                 \n#>      tropical fruit}    => {whole milk} 0.003152008  0.7045455 0.004473818 2.757344    31 0.004473818     5.003096e-10"},{"path":"patrones-de-asociación.html","id":"reglas-maximales","chapter":"Capítulo 2 Patrones de asociación","heading":"2.15 Reglas maximales","text":"Un itemset es maximal si existe otro itemset que sea su superset. Una regla de asociación se define como regla maximal si está generada con un itemset maximal. Con la función .maximal() se pueden identificar las reglas maximales.","code":"\nreglas_maximales <- reglas[is.maximal(reglas)]\nreglas_maximales\n#> set of 17 rules\n\ninspect(reglas_maximales[1:10])\n#>      lhs                    rhs              support confidence    coverage     lift count    coverage fishersExactTest\n#> [1]  {baking powder,                                                                                                   \n#>       yogurt}            => {whole milk} 0.003253686  0.7111111 0.004575496 2.783039    32 0.004575496     1.775138e-10\n#> [2]  {butter,                                                                                                          \n#>       onions}            => {whole milk} 0.003050330  0.7500000 0.004067107 2.935237    30 0.004067107     7.502990e-11\n#> [3]  {domestic eggs,                                                                                                   \n#>       sugar}             => {whole milk} 0.003558719  0.7142857 0.004982206 2.795464    35 0.004982206     1.990017e-11\n#> [4]  {butter,                                                                                                          \n#>       coffee}            => {whole milk} 0.003355363  0.7021277 0.004778851 2.747881    33 0.004778851     1.582670e-10\n#> [5]  {butter,                                                                                                          \n#>       curd}              => {whole milk} 0.004880529  0.7164179 0.006812405 2.803808    48 0.006812405     2.857678e-15\n#> [6]  {curd,                                                                                                            \n#>       domestic eggs}     => {whole milk} 0.004778851  0.7343750 0.006507372 2.874086    47 0.006507372     1.142131e-15\n#> [7]  {butter,                                                                                                          \n#>       pork}              => {whole milk} 0.003863752  0.7037037 0.005490595 2.754049    38 0.005490595     5.673757e-12\n#> [8]  {curd,                                                                                                            \n#>       tropical fruit,                                                                                                  \n#>       yogurt}            => {whole milk} 0.003965430  0.7500000 0.005287239 2.935237    39 0.005287239     1.003422e-13\n#> [9]  {brown bread,                                                                                                     \n#>       other vegetables,                                                                                                \n#>       root vegetables}   => {whole milk} 0.003152008  0.7750000 0.004067107 3.033078    31 0.004067107     8.092894e-12\n#> [10] {butter,                                                                                                          \n#>       tropical fruit,                                                                                                  \n#>       yogurt}            => {whole milk} 0.003355363  0.7333333 0.004575496 2.870009    33 0.004575496     2.336708e-11"},{"path":"patrones-de-asociación.html","id":"reglas-redundantes","chapter":"Capítulo 2 Patrones de asociación","heading":"2.16 Reglas redundantes","text":"Dos reglas son idénticas si tienen el mismo antecedente (parte izquierda) y consecuente (parte derecha). Supóngase ahora que una de estas reglas tiene en su antecedente los mismos items que forman el antecedente de la otra, junto con algunos items más. La regla más genérica se considera redundante, ya que aporta información adicional. En concreto, se considera que una regla X => Y es redundante si existe un subset X’ tal que existe una regla X’ => Y cuyo soporte es mayor.\\(X => Y\\) es redundante si existe un subset X’ tal que: conf(X’ -> Y) >= conf(X -> Y)Para este ejemplo se detectan reglas redundantes.","code":"\nreglas_redundantes <- reglas[is.redundant(x = reglas, measure = \"confidence\")]\nreglas_redundantes\n#> set of 0 rules"},{"path":"patrones-de-asociación.html","id":"transacciones-que-verifican-una-determinada-regla","chapter":"Capítulo 2 Patrones de asociación","heading":"2.17 Transacciones que verifican una determinada regla","text":"Una vez identificada una determinada regla, puede ser interesante recuperar todas aquellas transacciones en las que se cumple. continuación, se recuperan aquellas transacciones para las que se cumple la regla con mayor confianza de entre todas las encontradas.Las transacciones que cumplen esta regla son todas aquellas que contienen los items: citrus fruit, root vegetables, tropical fruit, whole milk y vegetables.","code":"\n# Se identifica la regla con mayor confianza\nas(reglas, \"data.frame\") %>%\n  arrange(desc(confidence)) %>%\n  head(1) %>%\n  pull(rules)\n#> [1] \"{citrus fruit,root vegetables,tropical fruit,whole milk} => {other vegetables}\"\nfiltrado_transacciones <- subset(x = transacciones,\n                                 subset = items %ain% c(\"citrus fruit\",\n                                                        \"root vegetables\",\n                                                        \"tropical fruit\",\n                                                        \"whole milk\",\n                                                        \"other vegetables\"))\nfiltrado_transacciones\n#> transactions in sparse format with\n#>  31 transactions (rows) and\n#>  169 items (columns)\n\ninspect(filtrado_transacciones[1:3])\n#>     items                    transactionID\n#> [1] {berries,                             \n#>      bottled water,                       \n#>      butter,                              \n#>      citrus fruit,                        \n#>      hygiene articles,                    \n#>      napkins,                             \n#>      other vegetables,                    \n#>      root vegetables,                     \n#>      rubbing alcohol,                     \n#>      tropical fruit,                      \n#>      whole milk}                      596 \n#> [2] {bottled water,                       \n#>      citrus fruit,                        \n#>      curd,                                \n#>      dessert,                             \n#>      frozen meals,                        \n#>      frozen vegetables,                   \n#>      fruit/vegetable juice,               \n#>      grapes,                              \n#>      napkins,                             \n#>      other vegetables,                    \n#>      pip fruit,                           \n#>      root vegetables,                     \n#>      specialty chocolate,                 \n#>      tropical fruit,                      \n#>      UHT-milk,                            \n#>      whipped/sour cream,                  \n#>      whole milk}                      1122\n#> [3] {beef,                                \n#>      beverages,                           \n#>      butter,                              \n#>      candles,                             \n#>      chicken,                             \n#>      citrus fruit,                        \n#>      cream cheese,                        \n#>      curd,                                \n#>      domestic eggs,                       \n#>      flour,                               \n#>      frankfurter,                         \n#>      ham,                                 \n#>      hard cheese,                         \n#>      hygiene articles,                    \n#>      liver loaf,                          \n#>      margarine,                           \n#>      mayonnaise,                          \n#>      other vegetables,                    \n#>      pasta,                               \n#>      roll products,                       \n#>      rolls/buns,                          \n#>      root vegetables,                     \n#>      sausage,                             \n#>      skin care,                           \n#>      soft cheese,                         \n#>      soups,                               \n#>      specialty fat,                       \n#>      sugar,                               \n#>      tropical fruit,                      \n#>      whipped/sour cream,                  \n#>      whole milk,                          \n#>      yogurt}                          1217"},{"path":"patrones-de-asociación.html","id":"información-sesión","chapter":"Capítulo 2 Patrones de asociación","heading":"2.18 Información sesión","text":"","code":"\nsesion_info <- devtools::session_info()\ndplyr::select(\n  tibble::as_tibble(sesion_info$packages),\n  c(package, loadedversion, source)\n)\n#> # A tibble: 95 × 3\n#>    package    loadedversion source        \n#>    <chr>      <chr>         <chr>         \n#>  1 arules     1.7-6         CRAN (R 4.1.2)\n#>  2 assertthat 0.2.1         CRAN (R 4.0.1)\n#>  3 backports  1.4.1         CRAN (R 4.1.2)\n#>  4 bit        4.0.4         CRAN (R 4.0.2)\n#>  5 bit64      4.0.5         CRAN (R 4.0.2)\n#>  6 bookdown   0.33          CRAN (R 4.1.2)\n#>  7 broom      0.7.12        CRAN (R 4.1.2)\n#>  8 bslib      0.4.2         CRAN (R 4.1.2)\n#>  9 cachem     1.0.7         CRAN (R 4.1.2)\n#> 10 callr      3.7.3         CRAN (R 4.1.2)\n#> # ℹ 85 more rows"},{"path":"patrones-de-asociación.html","id":"visualizacion-de-las-reglas","chapter":"Capítulo 2 Patrones de asociación","heading":"2.19 Visualizacion de las reglas","text":"para visualizar los graficos interactivos utiliza la funcionBasado en el texto de:Reglas de asociación y algoritmo Apriori con R Joaquín Amat Rodrigo, available Attribution 4.0 International (CC 4.0) https://www.cienciadedatos.net/documentos/43_reglas_de_asociacion","code":"library(arulesViz)\n\nplot(reglas)plot(reglas, method = \"graph\")"},{"path":"clasificación-con-knn.html","id":"clasificación-con-knn","chapter":"Capítulo 3 Clasificación con KNN","heading":"Capítulo 3 Clasificación con KNN","text":"","code":""},{"path":"clasificación-con-knn.html","id":"carga-de-dataset-y-exploración","chapter":"Capítulo 3 Clasificación con KNN","heading":"3.1 Carga de dataset y exploración","text":"Como es habitual el internet de las cosas industriales amenudo puede generar datos que van cambiando minuto minuto. Por ello se hace necesario tener un mecanismo para tomarlos desde la fuente que lo genera en forma inmediata, cada vez que querramos realizar un reporte automático o análisis exploratorio del mismo.Carga de Bibliotecas Utilizadas","code":"\nlibrary(readr)\nperformance_scm <- read_delim(\"https://themys.sid.uncu.edu.ar/rpalma/R-cran/scm_perform.csv\",     delim = \";\", escape_double = FALSE, col_types = cols(Performance = col_factor(levels = c(\"good\", \"fair\", \"poor\"))), locale = locale(decimal_mark = \",\",         grouping_mark = \".\"), trim_ws = TRUE)\nlibrary(e1071) # svm navie bayes\nlibrary(class) # knn\nlibrary(caret) # Construcción de Modelos\n#> Loading required package: ggplot2\n#> Loading required package: lattice\nlibrary(neuralnet) # Redes Neuronales\nlibrary(randomForest) # Random Forest\n#> randomForest 4.7-1.1\n#> Type rfNews() to see new features/changes/bug fixes.\n#> \n#> Attaching package: 'randomForest'\n#> The following object is masked from 'package:ggplot2':\n#> \n#>     margin\nlibrary(ggplot2) # Representacion del conocimiento\nlibrary(gridExtra) # Ploteo\n#> \n#> Attaching package: 'gridExtra'\n#> The following object is masked from 'package:randomForest':\n#> \n#>     combine"},{"path":"clasificación-con-knn.html","id":"entrenamiento-y-prueba","chapter":"Capítulo 3 Clasificación con KNN","heading":"3.2 Entrenamiento y prueba","text":"Separaremos los datos en dos conjuntos aleatoriosDataset de EntrenamientoDataset de PruebaLos datos puede ser elegidos aleatoriamente con el comando sample.\nEn nuestro caso emplearemos la proporción \\(70/30\\)\n70% Para entrenar 30% para test (preuba). Esto debería dejaronos 105 observaciones en un conjunto y 45 en el otro.Para que podamos repetir el experimento con los mismos datos fijaremos la semilla y así obtener resultado parecidos.5Ahora partiremos el dataset utilizando como índice inTrain.CPodemos ver la tabla de resultados contenidos con este comandoDel mismo modo podemos ver el dataset de pruebas","code":"\nset.seed(831)\ninTrain.C <- createDataPartition(performance_scm$Performance,p=.7,list = FALSE)\ntraining.C <- performance_scm[inTrain.C, ]\ntesting.C <- performance_scm[-inTrain.C, ]\ntable(training.C$Performance)\n#> \n#> good fair poor \n#>   35   35   35\ntable(testing.C$Performance)\n#> \n#> good fair poor \n#>   15   15   15"},{"path":"clasificación-con-knn.html","id":"ajuste-del-modelo","chapter":"Capítulo 3 Clasificación con KNN","heading":"3.3 Ajuste del modelo","text":"Realizaremos una tabla de contingencia o matriz de confusión","code":""},{"path":"clasificación-con-knn.html","id":"mediciones-model-level","chapter":"Capítulo 3 Clasificación con KNN","heading":"3.4 Mediciones Model-level","text":"","code":""},{"path":"clasificación-con-knn.html","id":"accuracy","chapter":"Capítulo 3 Clasificación con KNN","heading":"3.4.1 Accuracy","text":"Mide la proporción de predicciones correctas y es extraño encontrarlo como porcentaje. Su rango va de 0 100% . valor más alto mejor preformance del modelo.6\\[Accuracy = \\frac {TP + TN}{TP+TN+FP+FN} \\]","code":""},{"path":"clasificación-con-knn.html","id":"error-rate","chapter":"Capítulo 3 Clasificación con KNN","heading":"3.4.2 Error-Rate","text":"Mide la proporción de instancias mal clasificadas sobre el total de instancias. También es frecuente su uso como porcentaje en el rango 0% 100%. El valor más bajo es el más conveniente, al contrario que el de Acurracy\\[ErorRate = \\frac{FP + FN}{TP+TN+FP+FN} = 1- Accuracy\\]","code":""},{"path":"clasificación-con-knn.html","id":"class-level-mesures","chapter":"Capítulo 3 Clasificación con KNN","heading":"3.5 Class-Level Mesures","text":"","code":""},{"path":"clasificación-con-knn.html","id":"precisión","chapter":"Capítulo 3 Clasificación con KNN","heading":"3.5.1 Precisión","text":"Mide cuantas predicciones hechas fueron correctas respecto del total de predicciones emitidas.\\[Precision = \\frac{CorrectPredic}{TotalPredic}\\]","code":""},{"path":"clasificación-con-knn.html","id":"recall-o-rememoración","chapter":"Capítulo 3 Clasificación con KNN","heading":"3.5.2 Recall o rememoración","text":"Estima cuantas predicciones para una clase dada son correctas contra el total de casos predichos en la clase (entrenamiento y test)\\[Recall = \\frac{CorrectPredic}{TotalActual}\\]\n### Métrica-F\nEstima la bondad de clasificación para una clase dada. Se compara el total de casos correctamente clasificados contra el balance en la precisión y rememoración (recall). El valor máximo de F-Messure es igual 1\\[F1Messure = 2*\\frac{Precision * Recall}{Precision * Recall} \\]","code":""},{"path":"clasificación-con-knn.html","id":"subreentrenado-sobreentrenado","chapter":"Capítulo 3 Clasificación con KNN","heading":"3.6 Subreentrenado Sobreentrenado","text":"","code":""},{"path":"clasificación-con-knn.html","id":"k-nearest-neighbor-knn","chapter":"Capítulo 3 Clasificación con KNN","heading":"3.7 k Nearest Neighbor kNN","text":"En kNN la similaridd o distancia entre observaciones es utilizada para clasificar los elementos de una muestra o población.\nTipicamente usamos la distancia Euclídea en el espacio n dimensional. El modo más común de clasificación de un grupo es con el k-ésimo vecino(s) más próximo(s) en base datos etiquetados.7Se dice que kNN es una instanciación antes que un método basado en modelo, significando esto que en realidad se crea un modelo para clasificar.Eligiendo un k grande, se reduce el impacto de la varianza causada por el “ruido” aleatorio de los datos, pero esto puede sesgar el aprendizaje de ignorar patrones pequeños, pero significativos.Se suele utilizar una regla de facto para elegir k, que es la raíz cuadrada del número de observaciones del set de entrenamiento redondeado hacia arriba\\[k =\\sqrt{n_{training}}\\]Para el dataset de performance logística se tieneUtilizar un número impar para \\(k\\) es una buena práctica, porque evita caer en empate al seleccionar la etiqueta dominante.Utilizaremos la función \\(knn()\\) del paquete \\(class\\) para construir lo que representaría el modelo.","code":"\nk.choice <- ceiling(sqrt(nrow(training.C)))\nk.choice\n#> [1] 11\nlibrary(class)\nknn.pred.C <- knn(train=training.C[ ,1:4] , test=testing.C[ , 1:4], cl=training.C$Performance, k=k.choice)"},{"path":"clasificación-con-knn.html","id":"testeo-de-performace-del-algoritmo-entrenado","chapter":"Capítulo 3 Clasificación con KNN","heading":"3.7.1 Testeo de Performace del Algoritmo entrenado","text":"Dado que kNN precide un modelo podemos hacer una ponderación o medida de la bondad del ajuste en los datos de entrenamiento, por lo que debemos necesariamente aplicar el modelo los datos de prueba.8","code":"\nknn.test.acc <- confusionMatrix(knn.pred.C,testing.C$Performance, mode=\"prec_recall\")\nknn.test.acc\n#> Confusion Matrix and Statistics\n#> \n#>           Reference\n#> Prediction good fair poor\n#>       good   15    0    0\n#>       fair    0   14    0\n#>       poor    0    1   15\n#> \n#> Overall Statistics\n#>                                           \n#>                Accuracy : 0.9778          \n#>                  95% CI : (0.8823, 0.9994)\n#>     No Information Rate : 0.3333          \n#>     P-Value [Acc > NIR] : < 2.2e-16       \n#>                                           \n#>                   Kappa : 0.9667          \n#>                                           \n#>  Mcnemar's Test P-Value : NA              \n#> \n#> Statistics by Class:\n#> \n#>                      Class: good Class: fair Class: poor\n#> Precision                 1.0000      1.0000      0.9375\n#> Recall                    1.0000      0.9333      1.0000\n#> F1                        1.0000      0.9655      0.9677\n#> Prevalence                0.3333      0.3333      0.3333\n#> Detection Rate            0.3333      0.3111      0.3333\n#> Detection Prevalence      0.3333      0.3111      0.3556\n#> Balanced Accuracy         1.0000      0.9667      0.9833"},{"path":"clasificación-con-knn.html","id":"buenas-prácticas-en-knn","chapter":"Capítulo 3 Clasificación con KNN","heading":"3.8 Buenas prácticas en kNN","text":"Normalizar los valoresDado que kNN se basa en las distancias puede haber un error de influencia si una columna tiene magnitudes muy diferentes otra.9Basados en el gráfico boxplot deberíamos transfomar el recorrido de las variables al rango \\([0,1]\\).\\[ \\frac{x-min(x)}{max(x)-min(x)}\\]\nNoramlizaremos los datosAhora podemos hacer el gráfico con los valores escaladosAlternativamente de puede usar z-scores, que están estandarizados con la media entorno 0 y el desvío estandard.\\[Z= \\frac{x-u}{\\sigma} \\]Vista del gráfico normalizado","code":"\nboxplot(performance_scm[ ,1:4])\nperf_escalado <-apply(performance_scm[ ,1:4], 2, function (x) (x-min(x))/(max(x)-min(x)) )\nperf_escalado <- data.frame(perf_escalado, performance_scm$Performance)\nboxplot(perf_escalado[ ,1:4], main=\"Gráfico de cajas Predictores Escalados\", las=2)\nperf_z <- apply(performance_scm[ ,1:4],2, function(x) (x-mean(x)) / (sd(x))  )\nperf_z <- data.frame(perf_z,performance_scm$Performance)\nboxplot(perf_z[ ,1:4])"},{"path":"clasificación-con-knn.html","id":"evaitar-sobre-entrenamiento","chapter":"Capítulo 3 Clasificación con KNN","heading":"3.9 Evaitar sobre entrenamiento","text":"","code":""},{"path":"clasificación-con-knn.html","id":"variando-k","chapter":"Capítulo 3 Clasificación con KNN","heading":"3.9.1 Variando k","text":"Dado que kNN es un algoritmo de aprendizaje peresozo, pero eficiente la hora de hacer predicciones (pues en realidad hay aprendizaje formal), resulta interesante probar el efecto de variar los valores de k y ver cómo impacta en los resultados obtenidos y la performance de entrenamiento.","code":"\nsave <- list()\nfor (i in 1:25){\n  knn.pred.C.l <- knn(train = training.C[ ,1:4], test=testing.C[ ,1:4], cl=training.C$Performance, k=i)\n  save[[i]] <-confusionMatrix(knn.pred.C.l, testing.C$Performance, mode=\"prec_recall\")$overall[1] \n  \n}\nsave <-do.call(rbind,save)\nplot(1:25,save, xlab=\"k\", ylab=\"Accuracy\", type=\"o\")"},{"path":"algoritmo-del-vecino-más-próximo.html","id":"algoritmo-del-vecino-más-próximo","chapter":"Capítulo 4 Algoritmo del vecino más próximo","heading":"Capítulo 4 Algoritmo del vecino más próximo","text":"kNN (k-nearest neighbors). El algoritmo kNN es un método de aprendizaje automático supervisado utilizado para la clasificación y la regresión. Se basa en la idea de que los puntos de datos similares tienden agruparse en grupos o regiones en el espacio de características.10En el caso de la clasificación, kNN asigna una etiqueta un punto de datos desconocido basándose en las etiquetas de sus vecinos más cercanos. La “k” en kNN se refiere al número de vecinos más cercanos que se toman en cuenta para tomar una decisión de clasificación. Por ejemplo, si k = 3, entonces el algoritmo considerará las etiquetas de los 3 puntos más cercanos y asignará al punto desconocido la etiqueta que sea más común entre esos 3 vecinos.El proceso básico del algoritmo kNN es el siguiente:Calcula la distancia entre el punto de datos desconocido y todos los demás puntos de datos en el conjunto de entrenamiento. La distancia más comúnmente utilizada es la distancia euclidiana, pero también se pueden utilizar otras medidas de distancia.Selecciona los k puntos más cercanos al punto de datos desconocido en función de la distancia calculada en el paso anterior.En el caso de clasificación, cuenta las etiquetas de los k vecinos más cercanos y asigna al punto desconocido la etiqueta más común entre ellos. En el caso de regresión, se puede tomar un promedio ponderado de los valores de los k vecinos más cercanos para obtener una estimación del valor desconocido.11Es importante tener en cuenta que el valor de k en kNN es un parámetro ajustable y debe ser seleccionado cuidadosamente. Un valor pequeño de k puede llevar decisiones inestables y más susceptibles ruido, mientras que un valor grande de k puede suavizar las fronteras entre las clases y puede perder detalles más finos en los datos.El algoritmo kNN es relativamente simple y fácil de implementar, pero puede ser computacionalmente costoso cuando se tienen conjuntos de datos grandes, ya que implica calcular distancias para cada punto en el conjunto de entrenamiento. Además, kNN captura patrones globales en los datos, ya que toma decisiones basadas únicamente en los puntos de datos más cercanos.","code":""},{"path":"algoritmo-del-vecino-más-próximo.html","id":"que-relación-tiene-con-k-means","chapter":"Capítulo 4 Algoritmo del vecino más próximo","heading":"4.1 ¿Que relación tiene con k-means","text":"El algoritmo k-means es otro método utilizado en analítica de datos y big-data.Luego veremos en detalle que es k-means. Lo importante ahora es confundir estos conseptos que tienen nombres parecidos.El algoritmo k-means es otro método utilizado en aprendizaje automático supervisado para agrupar datos en conjuntos llamados “clusters”. Aunque k-means y k-nearest neighbors (kNN) tienen un nombre similar debido al uso de la letra “k”, son algoritmos diferentes con enfoques distintos.Mientras que kNN es un algoritmo de aprendizaje supervisado utilizado para clasificación y regresión, k-means es un algoritmo de aprendizaje supervisado utilizado para agrupamiento. Su objetivo principal es encontrar grupos o clusters en los datos sin tener información previa de las etiquetas o clases las que pertenecen los datos.La idea básica detrás de k-means es asignar cada punto de datos uno de los k clusters existentes, donde k es un número predeterminado definido por el usuario. El algoritmo itera para encontrar los centroides de los clusters de manera que se minimice la distancia total entre los puntos de datos y sus centroides correspondientes. En cada iteración, se recalculan los centroides y se reasignan los puntos de datos los clusters más cercanos.Una diferencia clave entre kNN y k-means es que kNN utiliza la información de etiquetas para tomar decisiones de clasificación o regresión, mientras que k-means agrupa los datos en base la proximidad espacial sin utilizar información de etiquetas. En k-means, los puntos de datos se agrupan en función de la similitud de sus características.En resumen, kNN es un algoritmo de aprendizaje supervisado utilizado para clasificación y regresión, mientras que k-means es un algoritmo de aprendizaje supervisado utilizado para agrupamiento. Ambos algoritmos tienen en común el uso del parámetro “k”, pero se aplican de manera diferente y tienen objetivos distintos en el análisis de datos.12","code":""},{"path":"algoritmo-del-vecino-más-próximo.html","id":"bibliografía-recomendada-1","chapter":"Capítulo 4 Algoritmo del vecino más próximo","heading":"4.2 Bibliografía recomendada","text":"","code":""},{"path":"algoritmo-del-vecino-más-próximo.html","id":"carga-de-dataset-y-exploración-1","chapter":"Capítulo 4 Algoritmo del vecino más próximo","heading":"4.3 Carga de dataset y exploración","text":"Carga de Bibliotecas Utilizadas","code":"\nlibrary(readr)\nperformance_scm <- read_delim(\"https://themys.sid.uncu.edu.ar/rpalma/R-cran/scm_perform.csv\",     delim = \";\", escape_double = FALSE, col_types = cols(Performance = col_factor(levels = c(\"good\", \"fair\", \"poor\"))), locale = locale(decimal_mark = \",\",         grouping_mark = \".\"), trim_ws = TRUE)\nlibrary(e1071) # svm navie bayes\nlibrary(class) # knn\nlibrary(caret) # Construcción de Modelos\n#> Loading required package: ggplot2\n#> Loading required package: lattice\nlibrary(neuralnet) # Redes Neuronales\nlibrary(randomForest) # Random Forest\n#> randomForest 4.7-1.1\n#> Type rfNews() to see new features/changes/bug fixes.\n#> \n#> Attaching package: 'randomForest'\n#> The following object is masked from 'package:ggplot2':\n#> \n#>     margin\nlibrary(ggplot2) # Representacion del conocimiento\nlibrary(gridExtra) # Ploteo\n#> \n#> Attaching package: 'gridExtra'\n#> The following object is masked from 'package:randomForest':\n#> \n#>     combine"},{"path":"algoritmo-del-vecino-más-próximo.html","id":"entrenamiento-y-prueba-1","chapter":"Capítulo 4 Algoritmo del vecino más próximo","heading":"4.4 Entrenamiento y prueba","text":"Separaremos los datos en dos conjuntos aleatoriosDataset de EntrenamientoDataset de PruebaLos datos puede ser elegidos aleatoriamente con el comando sample.\nEn nuestro caso emplearemos la proporción \\(70/30\\)\n70% Para entrenar 30% para test (preuba). Esto debería dejaronos 105 observaciones en un conjunto y 45 en el otro.Para que podamos repetir el experimento con los mismos datos fijaremos la semilla y así obtener resultado parecidosAhora partiremos el dataset utilizando como índice inTrain.CPodemos ver la tabla de resultados contenidos con este comandoDel mismo modo podemos ver el dataset de pruebas","code":"\nset.seed(831)\ninTrain.C <- createDataPartition(performance_scm$Performance,p=.7,list = FALSE)\ntraining.C <- performance_scm[inTrain.C, ]\ntesting.C <- performance_scm[-inTrain.C, ]\ntable(training.C$Performance)\n#> \n#> good fair poor \n#>   35   35   35\ntable(testing.C$Performance)\n#> \n#> good fair poor \n#>   15   15   15"},{"path":"algoritmo-del-vecino-más-próximo.html","id":"ajuste-del-modelo-1","chapter":"Capítulo 4 Algoritmo del vecino más próximo","heading":"4.5 Ajuste del modelo","text":"Realizaremos una tabla de contingencia o matriz de confusión","code":""},{"path":"algoritmo-del-vecino-más-próximo.html","id":"mediciones-model-level-1","chapter":"Capítulo 4 Algoritmo del vecino más próximo","heading":"4.6 Mediciones Model-level","text":"","code":""},{"path":"algoritmo-del-vecino-más-próximo.html","id":"accuracy-1","chapter":"Capítulo 4 Algoritmo del vecino más próximo","heading":"4.6.1 Accuracy","text":"Mide la proporción de predicciones correctas y es extraño encontrarlo como porcentaje. Su rango va de 0 100% . valor más alto mejor preformance del modelo\\[Accuracy = \\frac {TP + TN}{TP+TN+FP+FN} \\]","code":""},{"path":"algoritmo-del-vecino-más-próximo.html","id":"error-rate-1","chapter":"Capítulo 4 Algoritmo del vecino más próximo","heading":"4.6.2 Error-Rate","text":"Mide la proporción de instancias mal clasificadas sobre el total de instancias. También es frecuente su uso como porcentaje en el rango 0% 100%. El valor más bajo es el más conveniente, al contrario que el de Acurracy\\[ErrorRate = \\frac{FP + FN}{TP+TN+FP+FN} = 1- Accuracy\\]","code":""},{"path":"algoritmo-del-vecino-más-próximo.html","id":"class-level-mesures-1","chapter":"Capítulo 4 Algoritmo del vecino más próximo","heading":"4.7 Class-Level Mesures","text":"","code":""},{"path":"algoritmo-del-vecino-más-próximo.html","id":"precisión-1","chapter":"Capítulo 4 Algoritmo del vecino más próximo","heading":"4.7.1 Precisión","text":"Mide cuantas predicciones hechas fueron correctas respecto del total de predicciones emitidas.\\[Precision = \\frac{CorrectPredic}{TotalPredic}\\]","code":""},{"path":"algoritmo-del-vecino-más-próximo.html","id":"recall-o-rememoración-1","chapter":"Capítulo 4 Algoritmo del vecino más próximo","heading":"4.7.2 Recall o rememoración","text":"Estima cuantas predicciones para una clase dada son correctas contra el total de casos predichos en la clase (entrenamiento y test)\\[Recall = \\frac{CorrectPredic}{TotalActual}\\]\n### Métrica-F\nEstima la bondad de clasificación para una clase dada. Se compara el total de casos correctamente clasificados contra el balance en la precisión y rememoración (recall). El valor máximo de F-Messure es igual 1\\[F1Messure = 2*\\frac{Precision - Recall}{Precision * Recall} \\]","code":""},{"path":"algoritmo-del-vecino-más-próximo.html","id":"subreentrenado-sobreentrenado-1","chapter":"Capítulo 4 Algoritmo del vecino más próximo","heading":"4.8 Subreentrenado Sobreentrenado","text":"","code":""},{"path":"algoritmo-del-vecino-más-próximo.html","id":"k-nearest-neighbor-knn-1","chapter":"Capítulo 4 Algoritmo del vecino más próximo","heading":"4.9 k Nearest Neighbor kNN","text":"En kNN la similaridd o distancia entre observaciones es utilizada para clasificar los elementos de una muestra o población.\nTipicamente usamos la distancia Euclídea en el espacio n dimensional. El modo más común de clasificación de un grupo es con el k-ésimo vecino(s) más próximo(s) en base datos etiquetados.Se dice que kNN es una instanciación antes que un método basado en modelo, significando esto que en realidad se crea un modelo para clasificar.Eligiendo un k grande, se reduce el impacto de la varianza causada por el “ruido” aleatorio de los datos, pero esto puede sesgar el aprendizaje de ignorar patrones pequeños, pero significativos.Se suele utilizar una regla de facto para elegir k, que es la raíz cuadrada del número de observaciones del set de entrenamiento redondeado hacia arriba\\[k =\\sqrt{n_{training}}\\]Para el dataset de performance logística se tieneUtilizar un número impar para \\(k\\) es una buena práctica, porque evita caer en empate al seleccionar la etiqueta dominante.Utilizaremos la función \\(knn()\\) del paquete \\(class\\) para construir lo que representaría el modelo.","code":"\nk.choice <- ceiling(sqrt(nrow(training.C)))\nk.choice\n#> [1] 11\nlibrary(class)\nknn.pred.C <- knn(train=training.C[ ,1:4] , test=testing.C[ , 1:4], cl=training.C$Performance, k=k.choice)"},{"path":"algoritmo-del-vecino-más-próximo.html","id":"testeo-de-performace-del-algoritmo-entrenado-1","chapter":"Capítulo 4 Algoritmo del vecino más próximo","heading":"4.9.1 Testeo de Performace del Algoritmo entrenado","text":"Dado que kNN precide un modelo podemos hacer una ponderación o medida de la bondad del ajuste en los datos de entrenamiento, por lo que debemos necesariamente aplicar el modelo los datos de prueba.","code":"\nknn.test.acc <- confusionMatrix(knn.pred.C,testing.C$Performance, mode=\"prec_recall\")\nknn.test.acc\n#> Confusion Matrix and Statistics\n#> \n#>           Reference\n#> Prediction good fair poor\n#>       good   15    0    0\n#>       fair    0   14    0\n#>       poor    0    1   15\n#> \n#> Overall Statistics\n#>                                           \n#>                Accuracy : 0.9778          \n#>                  95% CI : (0.8823, 0.9994)\n#>     No Information Rate : 0.3333          \n#>     P-Value [Acc > NIR] : < 2.2e-16       \n#>                                           \n#>                   Kappa : 0.9667          \n#>                                           \n#>  Mcnemar's Test P-Value : NA              \n#> \n#> Statistics by Class:\n#> \n#>                      Class: good Class: fair Class: poor\n#> Precision                 1.0000      1.0000      0.9375\n#> Recall                    1.0000      0.9333      1.0000\n#> F1                        1.0000      0.9655      0.9677\n#> Prevalence                0.3333      0.3333      0.3333\n#> Detection Rate            0.3333      0.3111      0.3333\n#> Detection Prevalence      0.3333      0.3111      0.3556\n#> Balanced Accuracy         1.0000      0.9667      0.9833"},{"path":"algoritmo-del-vecino-más-próximo.html","id":"buenas-prácticas-en-knn-1","chapter":"Capítulo 4 Algoritmo del vecino más próximo","heading":"4.10 Buenas prácticas en kNN","text":"Nromalizar los caloresDado que kNN se basa en las distancias puede haber un error de influencia si una columna tiene magnitudes muy diferentes otra.Basados en el gráfico boxplot deberíamos transfomar el recorrido de las variables al rango \\([0,1]\\).\\[ \\frac{x-min(x)}{max(x)-min(x)}\\]\nNoramlizaremos los datosAhora podemos hacer el gráfico con los valores escaladosAlternativamente de puede usar z-scores, que están estandarizados con la media entorno 0 y el desvío estandard.\\[Z= \\frac{x-u}{\\sigma} \\]Vista del gráfico normalizado","code":"\nboxplot(performance_scm[ ,1:4])\nperf_escalado <-apply(performance_scm[ ,1:4], 2, function (x) (x-min(x))/(max(x)-min(x)) )\nperf_escalado <- data.frame(perf_escalado, performance_scm$Performance)\nboxplot(perf_escalado[ ,1:4], main=\"Gráfico de cajas Predictores Escalados\", las=2)\nperf_z <- apply(performance_scm[ ,1:4],2, function(x) (x-mean(x)) / (sd(x))  )\nperf_z <- data.frame(perf_z,performance_scm$Performance)\nboxplot(perf_z[ ,1:4])"},{"path":"algoritmo-del-vecino-más-próximo.html","id":"variando-k-1","chapter":"Capítulo 4 Algoritmo del vecino más próximo","heading":"4.10.1 Variando k","text":"Dado que kNN es un algoritmo de aprendizaje peresozo, pero eficiente la hora de hacer predicciones (pues en realidad hay aprendizaje formal), resulta interesante probar el efecto de variar los valores de k y ver cómo impacta en los resultados obtenidos y la performance de entrenamiento.","code":"\nsave <- list()\nfor (i in 1:25){\n  knn.pred.C.l <- knn(train = training.C[ ,1:4], test=testing.C[ ,1:4], cl=training.C$Performance, k=i)\n  save[[i]] <-confusionMatrix(knn.pred.C.l, testing.C$Performance, mode=\"prec_recall\")$overall[1] \n  \n}\nsave <-do.call(rbind,save)\nplot(1:25,save, xlab=\"k\", ylab=\"Accuracy\", type=\"o\")"},{"path":"agrupación-k-means.html","id":"agrupación-k-means","chapter":"Capítulo 5 Agrupación K-Means","heading":"Capítulo 5 Agrupación K-Means","text":"En el enfoque de partición de R, las observaciones se dividen en grupos K y se reorganizan para formar los grupos más cohesivos posibles de acuerdo con un criterio dado. Hay dos métodos: K-medias y partición alrededor de mediodes PAM. En este artículo, basado en el capítulo 16 de R Action, Second Edition, el autor Rob Kabacoff analiza la agrupación de K-means.El algoritmo k-means podría utilizarse en el contexto del mantenimiento industrial.Supongamos que tienes un conjunto de datos históricos sobre el rendimiento y la vida útil de diferentes componentes de maquinaria en una planta industrial. Estos datos pueden incluir variables como la temperatura de funcionamiento, la vibración, el consumo de energía, la carga de trabajo, entre otros.13El objetivo sería utilizar el algoritmo k-means para identificar grupos o clusters de componentes que tengan un comportamiento similar en términos de su rendimiento y vida útil. Esto puede ayudar los ingenieros y operarios de mantenimiento entender mejor el comportamiento de los componentes y tomar decisiones más informadas sobre el mantenimiento preventivo o la sustitución de los mismos.El proceso podría seguir los siguientes pasos:Preparación de datos: Reunir y preparar los datos históricos relevantes de los componentes de maquinaria. Esto puede incluir la recopilación de variables, como la temperatura, la vibración, el consumo de energía, etc., para cada componente en diferentes momentos.Preparación de datos: Reunir y preparar los datos históricos relevantes de los componentes de maquinaria. Esto puede incluir la recopilación de variables, como la temperatura, la vibración, el consumo de energía, etc., para cada componente en diferentes momentos.Selección de características: Identificar las características relevantes para el análisis. Puedes realizar un análisis de correlación o utilizar el conocimiento experto para seleccionar las variables más significativas que pueden influir en el rendimiento y la vida útil de los componentes.Selección de características: Identificar las características relevantes para el análisis. Puedes realizar un análisis de correlación o utilizar el conocimiento experto para seleccionar las variables más significativas que pueden influir en el rendimiento y la vida útil de los componentes.Normalización de datos: Normalizar los datos para asegurarse de que todas las características tengan la misma escala y rango. Esto es importante para que todas las variables tengan una influencia equitativa en el algoritmo k-means.Normalización de datos: Normalizar los datos para asegurarse de que todas las características tengan la misma escala y rango. Esto es importante para que todas las variables tengan una influencia equitativa en el algoritmo k-means.Elección del número de clusters (k): Determinar el número de clusters que deseas encontrar. Puedes utilizar métodos como el método del codo (elbow method) o la silueta (silhouette) para evaluar diferentes valores de k y seleccionar el más adecuado.Elección del número de clusters (k): Determinar el número de clusters que deseas encontrar. Puedes utilizar métodos como el método del codo (elbow method) o la silueta (silhouette) para evaluar diferentes valores de k y seleccionar el más adecuado.Aplicación del algoritmo k-means: Ejecutar el algoritmo k-means en los datos normalizados, agrupando los componentes en los k clusters determinados. El algoritmo asignará cada componente al cluster más cercano en función de las características seleccionadas.Aplicación del algoritmo k-means: Ejecutar el algoritmo k-means en los datos normalizados, agrupando los componentes en los k clusters determinados. El algoritmo asignará cada componente al cluster más cercano en función de las características seleccionadas.Análisis e interpretación de los resultados: Analizar los resultados del clustering para identificar patrones y comportamientos similares entre los componentes. Puedes examinar las características de cada cluster para entender qué factores pueden estar relacionados con un mayor rendimiento o una vida útil más larga. Esto puede ayudar tomar decisiones de mantenimiento más precisas y eficientes.Análisis e interpretación de los resultados: Analizar los resultados del clustering para identificar patrones y comportamientos similares entre los componentes. Puedes examinar las características de cada cluster para entender qué factores pueden estar relacionados con un mayor rendimiento o una vida útil más larga. Esto puede ayudar tomar decisiones de mantenimiento más precisas y eficientes.Atención que este es solo un ejemplo de cómo se podría aplicar el algoritmo k-means en el mantenimiento industrial. La selección de variables, la normalización de datos y la interpretación de los resultados pueden variar según el contexto y los objetivos específicos del estudio de mantenimiento. es lo mismo utilizar esta técnica para mantenimiento en la industria alimenticia que en la industria nuclear. Cada campo de aplicación debería respetar los procedimiento Hazop sugerido por las normas y buenas prácticas recomendadas.","code":""},{"path":"agrupación-k-means.html","id":"bibliografía-recomendada-2","chapter":"Capítulo 5 Agrupación K-Means","heading":"5.1 Bibliografía recomendada","text":"PAM es por lejos el más comunmente utilizado, pero existen otros como CLARA, MELISSA, y otros tantos que se adaptan diferentes problemas. En el caso de la localización de las balizas de satélites como el ARSAT e incluso de los nano satélites como el FossaSat-1 (cubesat) este método es muy utilizado. Ver Satelites para internet de las cosas Industriales.\nTodas estas tecnologías de satélites de bajo costo están permitiendo mejorar la competitividad y eficiencia de las cadenas de suministros en todo el planeta. En especial esta tipo de tecnología sirve para ver el deterioro de las propiedades alimenticias de cargas en línea y en tiempo real. pesar de ello en gran parte de latinoamérica y caribe estas tecnología aún están por descubrirse.\nSe han utilizado varios métodos del skimr() (un sustituto de summary() ), pero el paquete que utilizaremos para esta tarea específica será cluster() paquete exitosamente para problemas de congestion de puertos y misiones de vacunación post covid de el paquete : ClusteR","code":""},{"path":"agrupación-k-means.html","id":"agrupación-k-means-1","chapter":"Capítulo 5 Agrupación K-Means","heading":"5.2 Agrupación K-means","text":"El método de partición más común es el análisis de clúster de K-means. Conceptualmente, el algoritmo K-means:Selecciona K centroides (K filas elegidas al azar)Asigna cada punto de datos su centroide más cercanoRecalcula los centroides como el promedio de todos los puntos de datos en un clúster (es decir, los centroides son vectores medios de p-longitud, donde p es el número de variables)Asigna puntos de datos sus centroides más cercanosContinúa los pasos 3 y 4 hasta que se reasignen las observaciones o se alcance el número máximo de iteraciones (R usa 10 como valor predeterminado).14Los detalles de implementación de este enfoque pueden variar. R utiliza un algoritmo eficiente de Hartigan y Wong (1979) que divide las observaciones en grupos k de tal manera que la suma de cuadrados de las observaciones sus centros de clúster asignados es un mínimo. Esto significa que en los pasos 2 y 4, cada observación se asigna al clúster con el valor más pequeño de:\\[ SS_{(k)}= \\sum_{=1}^n  \\sum_{j=1}^p( x_{ij}- \\bar{x_{kj}}) ²\\]\nDonde \\(k\\) es un cluster, \\(x_{ij}\\) es el valor de la variable \\(j ésima\\) para la observación \\(iésima\\), y \\(\\bar{x}_{kj}\\) es la media de la variable j_{p} para el cluster k_i .La agrupación en clústeres K-means puede manejar conjuntos de datos más grandes que los enfoques de clúster jerárquicos. Además, las observaciones se comprometen permanentemente un grupo. Se mueven cuando al hacerlo mejora la solución general. Sin embargo, el uso de medioides implica que todas las variables deben ser continuas y el enfoque puede verse gravemente afectado por valores atípicos. Por ejemplo k-means es aplicable para localización de pallets en sistemas de posición flotante si dentro del edificio hay muros o barreras que impiden la circulación entre racks.15También se desempeñan mal en presencia de grupos convexos (por ejemplo, en forma de U o celdas de manufactura flexible). El formato de la función K-means en R es:\\[kmeans(x, centros)\\]\ndonde \\(x\\) es un conjunto de datos numéricos (matriz o data.frame) y \\(centros\\) es el número de clústeres extraer. Es un parámetro arbitrario que elijo y con el que pruebo o simulo alternativas. Por su bajo costo computacional converge razonablemente bien una solución.La función devuelve la pertenencia de casos al clúster, los centroides, las sumas de cuadrados (dentro, entre y total) y los tamaños de cada clúster.Dado que el análisis de clústeres de K-medias comienza con k centroides elegidos al azar, se puede obtener una solución diferente cada vez que se invoca la función.\nUtilice la función set.seed() para garantizar que los resultados sean reproducibles, en especial si publicas en papers.\nAdemás, este enfoque de agrupación puede ser sensible la selección inicial de centroides. La función \\(kmeans()\\) tiene una opción o parámetro \\(nstart\\) que intenta varias configuraciones iniciales e informa sobre la mejor. Por ejemplo, agregando \\(nstart=25\\) generará 25 configuraciones iniciales. Este enfoque menudo muy recomendable. diferencia de la agrupación jerárquica, la agrupación en clústeres de K-means requiere que el número de clústeres que se extraer se especifique de antemano. Una vez más, el paquete NbClust() se puede utilizar como guía para estimar un número inicial de clusters acpetable. Además, puede ser útil un gráfico de las sumas totales de cuadrados dentro de los grupos contra el número de clústeres en una solución de K-means. Una curva en el gráfico puede sugerirnos el número apropiado de clústeres.16El gráfico se puede producir mediante la siguiente función.efectos de sostener un estilo único en los gráficos amenudo conviene crear una función de ploteo en la que repito los parámetros.El parámetro de datos es el conjunto de datos numéricos que se analizará, \\(nc\\) es el número máximo de grupos considerar y la semilla es una semilla de número aleatorio. Aquí, se analiza un conjunto de datos que contiene 13 mediciones químicas en 178 muestras de vino italiano. Los datos provienen originalmente del Repositorio de Aprendizaje Automático de la UCI (http://www.ics.uci.edu/~mlearn/MLRepository.html), pero también se pueden acceder ellos través del paquete de rattle.\nEn el listado se proporciona un análisis de conglomerado de K-means para los datos.17Dimensiones:Generaremos un reporte exploratorio automático con la biblioteca skimTable 5.1: Data summaryVariable type: numericProcederemos escalar el dataset, que es un poco menos eficiente que normalizar, pero es más rápido y nos muestra algunas de las herramientas de la biblioteca skimr.18Cargaremos la biblioteca NbClust() para usar el método kmeasn que ella proporciona.Dado que las variables varían en rango, se estandarizan antes de la agrupación (función \\(scale()\\)) es muy importante utiulizarla ya que esta biblioteca es muy sensible al error si se aplica normalizado.continuación, el número de clústeres se determina mediante las funciones wwsplot() y \\(NbClust()\\) . La Figura indica que hay una clara caída en la suma de cuadrados dentro de los grupos cuando se pasa de 1 3 grupos. Después de tres clústeres, esta disminución disminuye, lo que sugiere que una solución de 3 clústeres puede ser una buena opción para los datos.En la siguiente figura, 14 de los 24 criterios proporcionados por el paquete \\(NbClust\\) sugieren una solución de 3 clústeres.Tenga en cuenta que se pueden calcular los 30 criterios para cada conjunto de datos. Se obtiene una solución final de clúster con la función kmeans() y se imprimen los centroides de clúster.Dado que los centroides proporcionados por la función se basan en datos estandarizados, la función \\(aggregate()\\) se utiliza junto con las pertenencias al clúster para determinar las medias variables para cada clúster en la métrica original (o desescalando).¿Cuantas muestras hay en cada grupo?Dónde estan los centroides?Mira con detenimiento los valores entregados por \\[fit$(...)\\]Matriz de confusiónComo podemos ver se nos ha indicado que es muy probable que existan 3 categorías o grupos en los que podemos clasificar o etiquetar este dataset.\nLa matriz de confusión es muy buena.","code":"\nwssplot <- function(data, nc=15, seed=1234){\n               wss <- (nrow(data)-1)*sum(apply(data,2,var))\n               for (i in 2:nc){\n                    set.seed(seed)\n                    wss[i] <- sum(kmeans(data, centers=i)$withinss)}\n                plot(1:nc, wss, type=\"b\", xlab=\"Número de Clusters\",\n                     ylab=\"Suma cuadrática dentro de cada grupo\")}\nlibrary(readr)\nwine <- read_csv(\"http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\")\n#> Rows: 177 Columns: 14\n#> ── Column specification ────────────────────────────────────\n#> Delimiter: \",\"\n#> dbl (14): 1, 14.23, 1.71, 2.43, 15.6, 127, 2.8, 3.06, .2...\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nnombres <- list( \"Type\", \n  \"Alcohol\",\n\"Ácido Malico\",\n \"Cenizas\",\n\"Alcalinidad de las cenizas\"  ,\n \"Magnecio\",\n\"Fenoles Totales\", \n \"Flavonoides\" ,\n\"Fenoles no flavonoides\", \n \"Proantocianinas\" ,\n    \"Intensidad de Color\", \n    \"Tinte\",\n    \"OD280/OD315 de vinos diluídos\",\n\"Prolina\"                )\n\n\ncolnames(wine) <- nombres\nlibrary(skimr)\nskim(wine)\ndf <- scale(wine[-1]) \nwssplot(df)     \nlibrary(NbClust)\nset.seed(1234)\nnc <- NbClust(df, min.nc=2, max.nc=15, method=\"kmeans\")#> *** : The Hubert index is a graphical method of determining the number of clusters.\n#>                 In the plot of Hubert index, we seek a significant knee that corresponds to a \n#>                 significant increase of the value of the measure i.e the significant peak in Hubert\n#>                 index second differences plot. \n#> #> *** : The D index is a graphical method of determining the number of clusters. \n#>                 In the plot of D index, we seek a significant knee (the significant peak in Dindex\n#>                 second differences plot) that corresponds to a significant increase of the value of\n#>                 the measure. \n#>  \n#> ******************************************************************* \n#> * Among all indices:                                                \n#> * 1 proposed 2 as the best number of clusters \n#> * 19 proposed 3 as the best number of clusters \n#> * 1 proposed 9 as the best number of clusters \n#> * 2 proposed 15 as the best number of clusters \n#> \n#>                    ***** Conclusion *****                            \n#>  \n#> * According to the majority rule, the best number of clusters is  3 \n#>  \n#>  \n#> *******************************************************************\ntable(nc$Best.n[1,])\n#> \n#>  0  1  2  3  9 15 \n#>  2  1  1 19  1  2\nbarplot(table(nc$Best.n[1,]), \n          xlab=\"Numer of Clusters\", ylab=\"Number of Criteria\",\n          main=\"Number of Clusters Chosen by 26 Criteria\")\nset.seed(1234)\nfit.km <- kmeans(df, 3, nstart=25)                           #3\nfit.km$size\n#> [1] 61 65 51\nfit.km$centers  \n#>      Alcohol Ácido Malico    Cenizas\n#> 1  0.8333649   -0.3013131  0.3661731\n#> 2 -0.9183253   -0.3953334 -0.4905017\n#> 3  0.1736447    0.8642504  0.1871775\n#>   Alcalinidad de las cenizas    Magnecio Fenoles Totales\n#> 1                 -0.6065538  0.56922228      0.88768039\n#> 2                  0.1637039 -0.48321576     -0.07114136\n#> 3                  0.5168437 -0.06497127     -0.97106500\n#>   Flavonoides Fenoles no flavonoides Proantocianinas\n#> 1  0.98016451            -0.56173008      0.57583669\n#> 2  0.02658937            -0.03709561      0.06509498\n#> 3 -1.20624204             0.71915195     -0.77171004\n#>   Intensidad de Color      Tinte\n#> 1           0.1702296  0.4753467\n#> 2          -0.8955790  0.4614076\n#> 3           0.9378162 -1.1566204\n#>   OD280/OD315 de vinos diluídos    Prolina\n#> 1                     0.7753334  1.1296451\n#> 2                     0.2823571 -0.7460740\n#> 3                    -1.2872265 -0.4002655\nplot(fit.km$centers)\nct.km <- table(wine$Type, fit.km$cluster)\nct.km  \n#>    \n#>      1  2  3\n#>   1 58  0  0\n#>   2  3 65  3\n#>   3  0  0 48"},{"path":"clusterización.html","id":"clusterización","chapter":"Capítulo 6 Clusterización","heading":"Capítulo 6 Clusterización","text":"Una de las cosas interesantes sobre la posibilidad de hacer Inteligencia de Negocios (BI por sus siglas en ingles – Business Intelligence) es que nos permite descubrir patrones visibles en primera instancia, pero que ayudan preguntarnos ¿por qué sucede esto?.\nEn esta serie de ejercicios teóricos, además de familiarizarnos con el lenguaje R-Cran y su uso en Clusters, trataremos de ver como podemos valernos de las bondades del entorno de trabajo para responder esas preguntas.Hasta ahora tenemos claro temas referidos al concepto estadísticos clásicos (monovariados), y hemos insinuado algo sobre las estructuras de los entornos multivariados.","code":""},{"path":"clusterización.html","id":"bibliografía-recomendada-3","chapter":"Capítulo 6 Clusterización","heading":"6.1 Bibliografía recomendada","text":"Teniendo en cuenta lo sensible que este mérodo es el tratamiento previo de los datos (sobre todo la normalización) es recomendable revisar esta publicación.Tanto sea para empresas industriales o de servicios en las que sus sistemas de información abarquen los campos de medición y administración de la estrategia o el aspecto operativo; es de crucial importancia determinar los niveles en los que las variables que representan las perspectivas tengan claramente establecidas sus franjas de flotación. En otras palabras debemos sentar las bases para construir un Cuadro de mando o tablero de control, que es sólo hacer que luces rojas amarillas y verdes se enciendan en un semáforo, sino tener relevancia para la toma de decisión sobre lo que los datos nos aportan.Utilizaremos una base de datos ficticia (cualquier semejanza con la realidad es pura coincidencia).https://themys.sid.uncu.edu.ar/rpalma/R-cran/UCB/BSC_proveedores.csvEn ella se ha realizado un estudio sobre varias empresas que son contratistas o proveedores de servicios de alto valor agregado de grandes compañías constructora. Se ha indagado el desempeño de unos 150 contratistas según cuatro dimensiones o indicadores del BSC.Estas dimensiones están relacionadas con:TecnologíaNormasCapitalEquipamientoLos datos han sido normalizados tode vez que deberemos comparar unidades de medidas muy disimiles. Por ejemplo la calidad de los recursos humanos tiene dimensión y el capital estaba expresado en dolares.","code":""},{"path":"clusterización.html","id":"carga-de-datos-desde-archivos-externos","chapter":"Capítulo 6 Clusterización","heading":"6.1.1 Carga de datos desde archivos externos","text":"Si bien sería posible cargar datos mano en R-CRAN, sería práctico hacer lo así.\nTodos los datos que se utilizan en este entorno de trabajo se manejan internamente como matrices. Estas matrices de datos (semejantes bases de datos) se llaman data-frames.Un data frame podría ser una matriz de una fila por una columna así por ejemploIntente carga dos dataframes y b con valores numéricos y realizamos la suma de ambos como se muestra en el código.Esto nos permitiría usar R-CRAN como si se tratase de una calculadora.","code":"  > X <-200 ;\n  > Y= 20 ;\n\n  > Y= X+Z"},{"path":"clusterización.html","id":"captura-de-datos","chapter":"Capítulo 6 Clusterización","heading":"6.2 Captura de datos","text":"Cargando datos desde la línea de comandoPara finalizar la carga tipee dos veces .","code":"Ejecute los comandos \"scan()\" y luego tipee los números separados por <Enter>."},{"path":"clusterización.html","id":"desde-dónde-podemos-cargar-datos","chapter":"Capítulo 6 Clusterización","heading":"6.3 ¿Desde dónde podemos cargar datos?","text":"Uno de los requisitos para poder cargar datos es que sepamos en que carpeta estamos trabajando.\nEn este ejercicio veremos como hacer para saber donde estamos parados.Puedes cambiar tu carpeta (directorio) de trabajo con el comando setwd()Debes poner entre comillas lo que va dentro del paréntesisPrepare un archivo en excel y guardelo con formato csv\n“comma separated values” para capturarlo desde R-CRAN.","code":"\ngetwd()\n#> [1] \"/media/rpalma/Datos/2023/Posgrado/R-Cran/Analitica_Datos/UCC\""},{"path":"clusterización.html","id":"revisión-de-datos-ejemplo","chapter":"Capítulo 6 Clusterización","heading":"6.3.1 Revisión de Datos Ejemplo","text":"Con los siguientes comando importaremos una planilla generada con una hoja de cálculo que contiene las 150 respuestas de las encuestas que se realizaronPodemos ver el contenido del dataframe BSC_Proveedores que se llama igual que el archivo de texto separado por comas. Para ello ejecutamos el siguiente comando.","code":"\nlibrary(readr)\n#BSC_proveedores <- read_csv(\"BSC_proveedores.csv\")\nBSC_proveedores <- read.csv(\"https://themys.sid.uncu.edu.ar/rpalma/R-cran/UCB/BSC_proveedores.csv\")\nBSC_proveedores [c(1:5 ,70:73, 126:129) ,]\n#>       X Tecnologia Normas Capital Equipo       Empresa\n#> 1     1        5.1    3.5     1.4    0.2   CNB-Cerveza\n#> 2     2        4.9    3.0     1.4    0.2   CNB-Cerveza\n#> 3     3        4.7    3.2     1.3    0.2   CNB-Cerveza\n#> 4     4        4.6    3.1     1.5    0.2   CNB-Cerveza\n#> 5     5        5.0    3.6     1.4    0.2   CNB-Cerveza\n#> 70   70        5.6    2.5     3.9    1.1     FARMACORP\n#> 71   71        5.9    3.2     4.8    1.8     FARMACORP\n#> 72   72        6.1    2.8     4.0    1.3     FARMACORP\n#> 73   73        6.3    2.5     4.9    1.5     FARMACORP\n#> 126 126        7.2    3.2     6.0    1.8 SOFIA-Avicola\n#> 127 127        6.2    2.8     4.8    1.8 SOFIA-Avicola\n#> 128 128        6.1    3.0     4.9    1.8 SOFIA-Avicola\n#> 129 129        6.4    2.8     5.6    2.1 SOFIA-Avicola"},{"path":"clusterización.html","id":"análisis-exploratorio","chapter":"Capítulo 6 Clusterización","heading":"6.4 Análisis Exploratorio","text":"Si quisiésemos ver el desempeño respecto la variable TECNOLOGÍA tendríamos que interponer entre el nombre del dataset el signo pesos y luego el nombre de la columnaRepita todo el prceso con el resto de la dimensiones NORMA, CAPITAL, EQUIPO, EMPRESA.","code":"\nBSC_proveedores [c(1:5 ,70:73, 126:129) ,]\n#>       X Tecnologia Normas Capital Equipo       Empresa\n#> 1     1        5.1    3.5     1.4    0.2   CNB-Cerveza\n#> 2     2        4.9    3.0     1.4    0.2   CNB-Cerveza\n#> 3     3        4.7    3.2     1.3    0.2   CNB-Cerveza\n#> 4     4        4.6    3.1     1.5    0.2   CNB-Cerveza\n#> 5     5        5.0    3.6     1.4    0.2   CNB-Cerveza\n#> 70   70        5.6    2.5     3.9    1.1     FARMACORP\n#> 71   71        5.9    3.2     4.8    1.8     FARMACORP\n#> 72   72        6.1    2.8     4.0    1.3     FARMACORP\n#> 73   73        6.3    2.5     4.9    1.5     FARMACORP\n#> 126 126        7.2    3.2     6.0    1.8 SOFIA-Avicola\n#> 127 127        6.2    2.8     4.8    1.8 SOFIA-Avicola\n#> 128 128        6.1    3.0     4.9    1.8 SOFIA-Avicola\n#> 129 129        6.4    2.8     5.6    2.1 SOFIA-Avicola"},{"path":"clusterización.html","id":"historgramas-análisis-de-histograma","chapter":"Capítulo 6 Clusterización","heading":"6.5 Historgramas Análisis de Histograma","text":"Veremos como se comportan las muestras (contratistar) utilizando el histogramaEl gráfico nos muestra que hay dos grupos de contratistas (casi 29 ocurrencias en cada uno) con un desempeño de 6 en la variable TECNOLOGÍA . La escala original era de 1 10.Utilizaremos el comando par() que permite dividir el área de ploteo en una matriz especificada por el comando Numero de Columna (nmfrow)Haga las interpretaciones de estos gráficos.** Particionado del área de impresión","code":"\nhist(BSC_proveedores$Tecnologia)\npar(mfrow=c(2,2))\nhist(BSC_proveedores$Tecnologia)\nhist(BSC_proveedores$Tecnologia)\nhist(BSC_proveedores$Capital)\nhist(BSC_proveedores$Equipo)"},{"path":"clusterización.html","id":"gráficos-de-densidad-1","chapter":"Capítulo 6 Clusterización","heading":"6.6 Gráficos de Densidad","text":"Algunas personas prefieren utilizar la envolvente del histograma que es el gráfico de densidad.19<<Density,fig=TRUE>>=Algunas de estas gráficas ya nos muestran que existen ciertas diferencias entre las contratistas, es como si hubiese diferentes campanas de Gauss que agrupan las diferentes muestras.","code":"\n\npar(mfrow=c(1,1))\nplot(density(BSC_proveedores$Equipo))\npar(mfrow=c(2,2))\nplot(density(BSC_proveedores$Tecnologia))\nplot(density(BSC_proveedores$Normas))\nplot(density(BSC_proveedores$Capital))\nplot(density(BSC_proveedores$Equipo))"},{"path":"clusterización.html","id":"gráficas-ralas-y-análisis-multivariado","chapter":"Capítulo 6 Clusterización","heading":"6.7 Gráficas Ralas y Análisis Multivariado","text":"Este tipo de análisis multivariado nos permite construir una matriz de gráficas que en la diagonal principal nos muestra los ya conocidos gráficos de densidad.\nLuego cada una de las otras intersecciones nos señala si existe algún tipo de correlación montónica (creciente o decreciente) entre las variables analizadas.\nEsto es importante, pues priori sabemos si las dimensiones que estamos usando tienen o relación entre ellas. En otras palabras si las dimensiones o metas tienen correlación quiere decir que podríamos prescindir de una de ellas.\nAsí capital y equipo parecen priori tener alta linealidad en su correlación.20Notar también las líneas de puntos que nos marcan el intervalo de confianza que podríamos tener sobre esa variable. Es justamente ese margen el que nos perite establecer la franja de flotación que motiva el paso de verde amarillo. Si se desplazase tres veces la varianza estaríamos en rojo.","code":"\nlibrary(car)\n#> Loading required package: carData\nBSC_Rawdata <- BSC_proveedores[ ,c(2,3,4,5)]\nBSC_Rawdata\n#>     Tecnologia Normas Capital Equipo\n#> 1          5.1    3.5     1.4    0.2\n#> 2          4.9    3.0     1.4    0.2\n#> 3          4.7    3.2     1.3    0.2\n#> 4          4.6    3.1     1.5    0.2\n#> 5          5.0    3.6     1.4    0.2\n#> 6          5.4    3.9     1.7    0.4\n#> 7          4.6    3.4     1.4    0.3\n#> 8          5.0    3.4     1.5    0.2\n#> 9          4.4    2.9     1.4    0.2\n#> 10         4.9    3.1     1.5    0.1\n#> 11         5.4    3.7     1.5    0.2\n#> 12         4.8    3.4     1.6    0.2\n#> 13         4.8    3.0     1.4    0.1\n#> 14         4.3    3.0     1.1    0.1\n#> 15         5.8    4.0     1.2    0.2\n#> 16         5.7    4.4     1.5    0.4\n#> 17         5.4    3.9     1.3    0.4\n#> 18         5.1    3.5     1.4    0.3\n#> 19         5.7    3.8     1.7    0.3\n#> 20         5.1    3.8     1.5    0.3\n#> 21         5.4    3.4     1.7    0.2\n#> 22         5.1    3.7     1.5    0.4\n#> 23         4.6    3.6     1.0    0.2\n#> 24         5.1    3.3     1.7    0.5\n#> 25         4.8    3.4     1.9    0.2\n#> 26         5.0    3.0     1.6    0.2\n#> 27         5.0    3.4     1.6    0.4\n#> 28         5.2    3.5     1.5    0.2\n#> 29         5.2    3.4     1.4    0.2\n#> 30         4.7    3.2     1.6    0.2\n#> 31         4.8    3.1     1.6    0.2\n#> 32         5.4    3.4     1.5    0.4\n#> 33         5.2    4.1     1.5    0.1\n#> 34         5.5    4.2     1.4    0.2\n#> 35         4.9    3.1     1.5    0.2\n#> 36         5.0    3.2     1.2    0.2\n#> 37         5.5    3.5     1.3    0.2\n#> 38         4.9    3.6     1.4    0.1\n#> 39         4.4    3.0     1.3    0.2\n#> 40         5.1    3.4     1.5    0.2\n#> 41         5.0    3.5     1.3    0.3\n#> 42         4.5    2.3     1.3    0.3\n#> 43         4.4    3.2     1.3    0.2\n#> 44         5.0    3.5     1.6    0.6\n#> 45         5.1    3.8     1.9    0.4\n#> 46         4.8    3.0     1.4    0.3\n#> 47         5.1    3.8     1.6    0.2\n#> 48         4.6    3.2     1.4    0.2\n#> 49         5.3    3.7     1.5    0.2\n#> 50         5.0    3.3     1.4    0.2\n#> 51         7.0    3.2     4.7    1.4\n#> 52         6.4    3.2     4.5    1.5\n#> 53         6.9    3.1     4.9    1.5\n#> 54         5.5    2.3     4.0    1.3\n#> 55         6.5    2.8     4.6    1.5\n#> 56         5.7    2.8     4.5    1.3\n#> 57         6.3    3.3     4.7    1.6\n#> 58         4.9    2.4     3.3    1.0\n#> 59         6.6    2.9     4.6    1.3\n#> 60         5.2    2.7     3.9    1.4\n#> 61         5.0    2.0     3.5    1.0\n#> 62         5.9    3.0     4.2    1.5\n#> 63         6.0    2.2     4.0    1.0\n#> 64         6.1    2.9     4.7    1.4\n#> 65         5.6    2.9     3.6    1.3\n#> 66         6.7    3.1     4.4    1.4\n#> 67         5.6    3.0     4.5    1.5\n#> 68         5.8    2.7     4.1    1.0\n#> 69         6.2    2.2     4.5    1.5\n#> 70         5.6    2.5     3.9    1.1\n#> 71         5.9    3.2     4.8    1.8\n#> 72         6.1    2.8     4.0    1.3\n#> 73         6.3    2.5     4.9    1.5\n#> 74         6.1    2.8     4.7    1.2\n#> 75         6.4    2.9     4.3    1.3\n#> 76         6.6    3.0     4.4    1.4\n#> 77         6.8    2.8     4.8    1.4\n#> 78         6.7    3.0     5.0    1.7\n#> 79         6.0    2.9     4.5    1.5\n#> 80         5.7    2.6     3.5    1.0\n#> 81         5.5    2.4     3.8    1.1\n#> 82         5.5    2.4     3.7    1.0\n#> 83         5.8    2.7     3.9    1.2\n#> 84         6.0    2.7     5.1    1.6\n#> 85         5.4    3.0     4.5    1.5\n#> 86         6.0    3.4     4.5    1.6\n#> 87         6.7    3.1     4.7    1.5\n#> 88         6.3    2.3     4.4    1.3\n#> 89         5.6    3.0     4.1    1.3\n#> 90         5.5    2.5     4.0    1.3\n#> 91         5.5    2.6     4.4    1.2\n#> 92         6.1    3.0     4.6    1.4\n#> 93         5.8    2.6     4.0    1.2\n#> 94         5.0    2.3     3.3    1.0\n#> 95         5.6    2.7     4.2    1.3\n#> 96         5.7    3.0     4.2    1.2\n#> 97         5.7    2.9     4.2    1.3\n#> 98         6.2    2.9     4.3    1.3\n#> 99         5.1    2.5     3.0    1.1\n#> 100        5.7    2.8     4.1    1.3\n#> 101        6.3    3.3     6.0    2.5\n#> 102        5.8    2.7     5.1    1.9\n#> 103        7.1    3.0     5.9    2.1\n#> 104        6.3    2.9     5.6    1.8\n#> 105        6.5    3.0     5.8    2.2\n#> 106        7.6    3.0     6.6    2.1\n#> 107        4.9    2.5     4.5    1.7\n#> 108        7.3    2.9     6.3    1.8\n#> 109        6.7    2.5     5.8    1.8\n#> 110        7.2    3.6     6.1    2.5\n#> 111        6.5    3.2     5.1    2.0\n#> 112        6.4    2.7     5.3    1.9\n#> 113        6.8    3.0     5.5    2.1\n#> 114        5.7    2.5     5.0    2.0\n#> 115        5.8    2.8     5.1    2.4\n#> 116        6.4    3.2     5.3    2.3\n#> 117        6.5    3.0     5.5    1.8\n#> 118        7.7    3.8     6.7    2.2\n#> 119        7.7    2.6     6.9    2.3\n#> 120        6.0    2.2     5.0    1.5\n#> 121        6.9    3.2     5.7    2.3\n#> 122        5.6    2.8     4.9    2.0\n#> 123        7.7    2.8     6.7    2.0\n#> 124        6.3    2.7     4.9    1.8\n#> 125        6.7    3.3     5.7    2.1\n#> 126        7.2    3.2     6.0    1.8\n#> 127        6.2    2.8     4.8    1.8\n#> 128        6.1    3.0     4.9    1.8\n#> 129        6.4    2.8     5.6    2.1\n#> 130        7.2    3.0     5.8    1.6\n#> 131        7.4    2.8     6.1    1.9\n#> 132        7.9    3.8     6.4    2.0\n#> 133        6.4    2.8     5.6    2.2\n#> 134        6.3    2.8     5.1    1.5\n#> 135        6.1    2.6     5.6    1.4\n#> 136        7.7    3.0     6.1    2.3\n#> 137        6.3    3.4     5.6    2.4\n#> 138        6.4    3.1     5.5    1.8\n#> 139        6.0    3.0     4.8    1.8\n#> 140        6.9    3.1     5.4    2.1\n#> 141        6.7    3.1     5.6    2.4\n#> 142        6.9    3.1     5.1    2.3\n#> 143        5.8    2.7     5.1    1.9\n#> 144        6.8    3.2     5.9    2.3\n#> 145        6.7    3.3     5.7    2.5\n#> 146        6.7    3.0     5.2    2.3\n#> 147        6.3    2.5     5.0    1.9\n#> 148        6.5    3.0     5.2    2.0\n#> 149        6.2    3.4     5.4    2.3\n#> 150        5.9    3.0     5.1    1.8\nscatterplotMatrix(BSC_Rawdata)"},{"path":"clusterización.html","id":"mínimo-numero-de-dimensiones","chapter":"Capítulo 6 Clusterización","heading":"6.8 Mínimo numero de dimensiones","text":"Cuándo nos enfrentamos situaciones como esta, suele ocurrir que al definir los indicadores nos encontramos con el dilema del gran volumen de datos. Esto es un problema que provenga tan solo del número de casos que estudiamos con el objeto de conocer el recorrido de una variable, sino más bien por la gran cantidad de variables o calificadores con los que los definimos o estudiamos.\nYa vimos en el caso anterior como dimensiones o variables que tienen distinto nombre son en realidad más que la misma cosa.\nEn el ejemplo anterior la pregunta era si podríamos prescindir de una variable. En este ejercicio trataremos de ver cuantas podemos eliminar. La consigna es Mientras menos variables mejor, y la restricción que impondremos será la de perder variables siempre que podamos seguir describiendo con alto nivel de confianza el comportamiento de todos los casos. Otra mirada sobre el problema podría enunciarse así. “Como puedo saber que valores o recorrido le impondría la mínima cantidad de variables para calificar como candidato interesante en la nómina de contratistas de las grandes empresas constructoras”.21Para auxilio en este problema utilizaremos el Método de Análisis de Componentes Principales. En este caso y al igual que en el caso anterior usaré un subconjunto de datos (sólo los numéricos) y en especial la matriz de correlación. Esta matriz está armada con las pendientes de las aproximaciones lineales de las rectas del gráfico de densidades.Las técnicas que usaremos pretenden desde sus diferentes enfoques abrodar el problema de simplificar la interpretación del comportamiento individual y colectivo de los casos (empresas constructoras y contratista) y como podemos valernos del proceso de ingeniería inversa para mover los controles de nuestra “nave” en el tablero de comando con el que fijaremos la altura de la vara del tablero de control.","code":""},{"path":"clusterización.html","id":"análisis-de-componentes-principales","chapter":"Capítulo 6 Clusterización","heading":"6.9 Análisis de Componentes Principales","text":"Crearemos un objeto nuevo que se llamará PC1 (por Principal Component 1) y la instrucción con el que crearemos la matriz de correlaciones es princomp.22En el ploteo podemos ver que uno de los componentes principales aporta casi el 4 veces más de la información referida al comportamiento de la varianza de todos los casos. Este componente es el que más incluye en la clasificación o posible identificación del comportamiento de cada individuo de la muestra.<<sumario_pc1,echo=TRUE>>=Si observamos bien el reporte que nos entrega el comando summary nos podemos dar cuenta que con los dos primeros componentes podríamos explicar 97.768521% del comportamiento de las muestras de la población. En nuestro caso del total de empresas contratistas analizadas.¿Qué pasaría si representamos las empresas en un gŕafico en el que las variables de los ejes sean los dos componentes principales? , pues tendríamos un primer indicio de la bondad de las dimensiones o variables para agrupar las muestras\nEsto lo podemos realizar con el comando bilot.23Los ńumeros que aparecen el el diagrama son el caso de estudio (renglón en que se encuentra la empresa contratista).\nsimple vista observamos que hay como tres tipos distintos empresas (tres nubes claramente diferenciadas). Aquí nos queda claro que el principal componente que ordena o divide estas colonias es indistintamente el CAPITAL o el EQUIPAMIENTO con que cuentas.También podemos ver que hay empresas como la 15, 16, 132, 118, 61, 107 sobre las que el gráfico nos recomienda estudiarlas más pues es capaz de clasificarlas bien (son casos extremos o anómalos). Tal vez con poco capital o sin equipo pueden llegar ser competitivas o interesantes para las grandes constructoras.24Por último la dimensión referida la cetificación de NORMAS es la dimensión que menos valor aporta. Esto implica que certificar sea poco importante, sino que probablemente sea una pregunta irrelevante si todos contestaron que SI certificaron ISO 9000.","code":"\nPC1 <- princomp(BSC_Rawdata)\nPC1\n#> Call:\n#> princomp(x = BSC_Rawdata)\n#> \n#> Standard deviations:\n#>    Comp.1    Comp.2    Comp.3    Comp.4 \n#> 2.0494032 0.4909714 0.2787259 0.1538707 \n#> \n#>  4  variables and  150 observations.\nplot(PC1)\nsummary(PC1)\n#> Importance of components:\n#>                           Comp.1     Comp.2     Comp.3\n#> Standard deviation     2.0494032 0.49097143 0.27872586\n#> Proportion of Variance 0.9246187 0.05306648 0.01710261\n#> Cumulative Proportion  0.9246187 0.97768521 0.99478782\n#>                             Comp.4\n#> Standard deviation     0.153870700\n#> Proportion of Variance 0.005212184\n#> Cumulative Proportion  1.000000000\nbiplot(PC1)"},{"path":"clusterización.html","id":"scores","chapter":"Capítulo 6 Clusterización","heading":"6.10 Scores","text":"Si el comportamiento del componente va hacia el lado positivo, se debe interpretar como que mayor desempeño mejor resultado o calificación. Si algún componente apunta para el lado negativo tendremos que pensar que mayor calificación en esa dimensión pero sería el desempeño.\nLa variable PC1 que usamos tiene mucha información valiosa.\nRevise todo el contenido, voy mostrar una dimensión que es el score que indica como se comportarían todos los individuos si sólo los analizásemos con los componentes 1 y 2.Voy realizar el mismo score pero ahora solo con los componentes 1 y 2Aquí ya podemos ver más claramente la división que se produce entre distintos clusters. Para poder diferencias aún más recurriremos un nuevo tipo de análisis diferenciado que se llama análisis de clusters","code":"\nacp1 <- PC1$scores\nacp1 [1:10 , ]\n#>          Comp.1      Comp.2      Comp.3       Comp.4\n#>  [1,] -2.684126  0.31939725  0.02791483  0.002262437\n#>  [2,] -2.714142 -0.17700123  0.21046427  0.099026550\n#>  [3,] -2.888991 -0.14494943 -0.01790026  0.019968390\n#>  [4,] -2.745343 -0.31829898 -0.03155937 -0.075575817\n#>  [5,] -2.728717  0.32675451 -0.09007924 -0.061258593\n#>  [6,] -2.280860  0.74133045 -0.16867766 -0.024200858\n#>  [7,] -2.820538 -0.08946138 -0.25789216 -0.048143106\n#>  [8,] -2.626145  0.16338496  0.02187932 -0.045297871\n#>  [9,] -2.886383 -0.57831175 -0.02075957 -0.026744736\n#> [10,] -2.672756 -0.11377425  0.19763272 -0.056295401\nacp2 <-PC1$scores[ ,1:2]\nplot(acp2)"},{"path":"clusterización.html","id":"análisis-de-clusters-o-conglomerados","chapter":"Capítulo 6 Clusterización","heading":"6.11 Análisis de Clusters o Conglomerados","text":"Para realizar este análisis recurriremos cargar la biblioteca clustersEn el análisis de conglomerados existen dos formas clásicas de estudio. Ambas recurren las distancias euclídeas entre las muestras. Tenemos aproximaciones Jerárquicas y Jerárquicas\nAGNES, CLARA, DIANA, MORA, PAM son nombres de las técnicas que la biblioteca Clusters usa. Todas las técnicas se caracterizan por ser un acrónimo de la combinación de aproximaciones que usan (Single Linkage, Complete Linkage, Average Linkage) .Todas tienen nombre de mujer, pero esto quiere necesariamente decir que se trate de una técnica con complicaciones inesperadas, sino más bien que si quieres lo mejor de una de ellas es mejor que la entiendas e indagues en la página del manual.Con la clasificacion terminada procederemos ver gráficamente el resultado.Pasa asignar las muestras grupos usaré el comendo cuttree que permite valerme de las franjas blancas de corte del los gráficos para armar los clusters","code":"\nlibrary(cluster)\nagp1 = agnes(acp2,method=\"single\")\nagp2 = agnes(acp2,method=\"complete\")\nagp3 = agnes(acp2,method=\"average\")\npar(mfrow=c(2,2))\nplot(acp2)\nplot(agp1)\nplot(agp2)\nplot(agp3)\nagpcut <- cutree(agp3,3)\npar(mfrow=c(1,1))\nplot(acp2,col=agpcut)"},{"path":"clusterización.html","id":"otros-gráficos-de-agrupamiento","chapter":"Capítulo 6 Clusterización","heading":"6.11.1 Otros gráficos de agrupamiento","text":"Método Clara","code":"\nplot(clara(BSC_Rawdata,5))"},{"path":"clusterización.html","id":"método-melisa","chapter":"Capítulo 6 Clusterización","heading":"6.12 Método Melisa","text":"","code":"\nplot(diana(BSC_Rawdata),ask = FALSE)"},{"path":"redes-neuronales-artificiales.html","id":"redes-neuronales-artificiales","chapter":"Capítulo 7 Redes Neuronales artificiales","heading":"Capítulo 7 Redes Neuronales artificiales","text":"Las redes neuronales artificiales son modelos computacionales inspirados en el funcionamiento del cerebro humano. Están compuestas por una serie de unidades llamadas neuronas artificiales, que están conectadas entre sí través de conexiones ponderadas. Estas conexiones permiten las redes neuronales procesar información y realizar tareas de aprendizaje y predicción.En ingeniería industrial, las redes neuronales artificiales se utilizan en una amplia variedad de aplicaciones, como:Pronóstico de demanda: Las redes neuronales pueden utilizarse para predecir la demanda futura de productos o servicios en función de datos históricos, ayudando así la planificación de la producción y la gestión de inventarios.Pronóstico de demanda: Las redes neuronales pueden utilizarse para predecir la demanda futura de productos o servicios en función de datos históricos, ayudando así la planificación de la producción y la gestión de inventarios.Control de procesos: Las redes neuronales pueden utilizarse para controlar y optimizar procesos industriales. Pueden aprender de los datos recopilados en tiempo real y ajustar los parámetros del sistema para maximizar la eficiencia y minimizar los errores.Control de procesos: Las redes neuronales pueden utilizarse para controlar y optimizar procesos industriales. Pueden aprender de los datos recopilados en tiempo real y ajustar los parámetros del sistema para maximizar la eficiencia y minimizar los errores.Mantenimiento predictivo: Las redes neuronales pueden utilizarse para predecir fallos y realizar mantenimiento predictivo en maquinaria y equipos industriales. Al analizar datos de sensores y patrones de fallas anteriores, las redes neuronales pueden identificar señales de advertencia temprana y ayudar evitar costosas interrupciones en la producción.Mantenimiento predictivo: Las redes neuronales pueden utilizarse para predecir fallos y realizar mantenimiento predictivo en maquinaria y equipos industriales. Al analizar datos de sensores y patrones de fallas anteriores, las redes neuronales pueden identificar señales de advertencia temprana y ayudar evitar costosas interrupciones en la producción.Control de calidad: Las redes neuronales pueden utilizarse para realizar inspección y control de calidad en productos manufacturados. Pueden identificar defectos o anomalías en imágenes, señales o datos sensoriales, ayudando mejorar la calidad del producto final.Control de calidad: Las redes neuronales pueden utilizarse para realizar inspección y control de calidad en productos manufacturados. Pueden identificar defectos o anomalías en imágenes, señales o datos sensoriales, ayudando mejorar la calidad del producto final.Optimización de la cadena de suministro: Las redes neuronales pueden utilizarse para optimizar la gestión de la cadena de suministro, analizando grandes volúmenes de datos y tomando decisiones óptimas en tiempo real sobre inventario, distribución y logística.Optimización de la cadena de suministro: Las redes neuronales pueden utilizarse para optimizar la gestión de la cadena de suministro, analizando grandes volúmenes de datos y tomando decisiones óptimas en tiempo real sobre inventario, distribución y logística.La ventaja de las redes neuronales artificiales en ingeniería industrial radica en su capacidad para modelar relaciones complejas y lineales entre variables, así como en su capacidad de aprendizaje partir de datos. Sin embargo, es importante tener en cuenta que el uso de redes neuronales artificiales requiere una adecuada preparación y procesamiento de los datos, así como una validación y ajuste adecuados de los modelos para garantizar resultados confiables y precisos.25Si la ANN (Artificial Neural Net) tiene sólo una neurona de salida actua como clasificador dicotómico de la entrada.Si la ANN (Artificial Neural Net) tiene sólo una neurona de salida actua como clasificador dicotómico de la entrada.Si tiene varios niveles o categorías del tipo binario puede actuar como clasificador categóricoSi tiene varios niveles o categorías del tipo binario puede actuar como clasificador categóricoSi tiene una salida de doble precisión puede actuar como predictor (clustering)Si tiene una salida de doble precisión puede actuar como predictor (clustering)","code":""},{"path":"redes-neuronales-artificiales.html","id":"bibliografía-recomendada-4","chapter":"Capítulo 7 Redes Neuronales artificiales","heading":"7.1 Bibliografía recomendada","text":"","code":""},{"path":"redes-neuronales-artificiales.html","id":"redes-neuronales-en-r-cran","chapter":"Capítulo 7 Redes Neuronales artificiales","heading":"7.2 Redes Neuronales en R-Cran","text":"En R-Cran, puedes implementar redes neuronales artificiales utilizando la biblioteca “neuralnet”. Esta biblioteca proporciona funciones para construir y entrenar redes neuronales de una o varias capas en R.Aquí hay un ejemplo básico de cómo construir y entrenar una red neuronal artificial utilizando la biblioteca “neuralnet” en R-Cran:Instalación del paquete “neuralnet”:\ninstall.packages(“neuralnet”)Instalación del paquete “neuralnet”:install.packages(“neuralnet”)Carga del paquete “neuralnet”:\nlibrary(neuralnet)Carga del paquete “neuralnet”:library(neuralnet)Preparación de los datos de entrenamiento y prueba:Preparación de los datos de entrenamiento y prueba:Supongamos que tienes un conjunto de datos con variables de entrada (predictoras) llamadas “x1” y “x2” y una variable de salida (objetivo) llamada “y”. Debes dividir tus datos en conjuntos de entrenamiento y prueba. Aquí hay un ejemplo de cómo puedes hacerlo:Construcción de la red neuronal:Aquí puedes definir la arquitectura de tu red neuronal especificando el número de nodos en cada capa oculta. Por ejemplo, una red con una capa oculta de 5 nodos se puede definir de la siguiente manera:Crear la red neuronalEn este ejemplo, “formula” especifica la relación entre las variables de entrada y salida, y “hidden” indica el número de nodos en la capa oculta.Entrenamiento de la red neuronal:Utiliza la función “train” para entrenar la red neuronal con los datos de entrenamiento:Entrenar la red neuronalPredicción con la red neuronal entrenada:Utiliza la función “compute” para realizar predicciones con la red neuronal entrenada:Realizar predicciones con la red neuronalEn este ejemplo, “test_data[, c(”x1”, ”x2”)]” son las variables de entrada para las cuales deseas realizar predicciones.Este es solo un ejemplo básico para construir y entrenar una red neuronal artificial en R-Cran utilizando la biblioteca “neuralnet”. Puedes consultar la documentación de la biblioteca para obtener más información sobre las funciones y opciones disponibles.26","code":"set.seed(123)  # Establecer una semilla aleatoria para reproducibilidad\n\n  # Dividir datos en conjuntos de entrenamiento y prueba\n\ntrain_indices <- sample(1:nrow(tus_datos), nrow(tus_datos)*0.8)  # 80% de los datos para entrenamiento\ntrain_data <- tus_datos[train_indices, ]\ntest_data <- tus_datos[-train_indices, ]# Definir la fórmula de la red neuronal\nformula <- y ~ x1 + x2nn <- neuralnet(formula, data = train_data, hidden = c(5))trained_nn <- train(nn)predictions <- compute(trained_nn, test_data[, c(\"x1\", \"x2\")])"},{"path":"redes-neuronales-artificiales.html","id":"caso-de-estudio-cargar-de-bibliotecas","chapter":"Capítulo 7 Redes Neuronales artificiales","heading":"7.3 Caso de estudio: Cargar de bibliotecas","text":"","code":"\nlibrary(neuralnet)  # regression\n\nlibrary(nnet) # classification \n\nlibrary(NeuralNetTools)\n\nlibrary(plyr)"},{"path":"redes-neuronales-artificiales.html","id":"carga-de-datos","chapter":"Capítulo 7 Redes Neuronales artificiales","heading":"7.4 Carga de Datos","text":"La columna supervivencia de la tabla Categoric tiene etiquetas que identifican la situación en la que terminaron los emprendimientos, saber:SpinOf La empresa se separó de una principal y logró su independencia económicaSpinOf La empresa se separó de una principal y logró su independencia económicaRevEq La empresa alcanzó su punto de equilibrio canceló su deuda con el banco y fue comprada por la competencia.RevEq La empresa alcanzó su punto de equilibrio canceló su deuda con el banco y fue comprada por la competencia.BankR La empresa prosperó y el banco se quedó con los activos que vendidos compensaron el preśtamo inicial (Banca Rota).BankR La empresa prosperó y el banco se quedó con los activos que vendidos compensaron el preśtamo inicial (Banca Rota).","code":"\nlibrary(readr)\nStartups <- read_csv(\"/home/rpalma/AAA_Datos/2020/Posgrado/Di3/Datasets/50 Start Ups/50_Startups_LAC.csv\")\n#> Rows: 50 Columns: 6\n#> ── Column specification ────────────────────────────────────\n#> Delimiter: \",\"\n#> chr (1): Pais\n#> dbl (5): R_D_Spend, POM, Logist_Market, Profit, Superviv...\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nCategoric <- read_csv(\"/home/rpalma/AAA_Datos/2020/Posgrado/Di3/Datasets/50 Start Ups/50_Startups_Categoric_LAC.csv\")\n#> Rows: 50 Columns: 6\n#> ── Column specification ────────────────────────────────────\n#> Delimiter: \",\"\n#> chr (2): Pais, Supervivencia\n#> dbl (4): R_D_Spend, POM, Logist_Market, Profit\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."},{"path":"redes-neuronales-artificiales.html","id":"tratamiento-de-variables-categóricas","chapter":"Capítulo 7 Redes Neuronales artificiales","heading":"7.5 Tratamiento de variables categóricas","text":"","code":"\ntabla1 <- table(Categoric$Pais)\ntabla2 <- table(Categoric$Supervivencia)\ntabla3 <- table(Categoric$Pais,Categoric$Supervivencia)\nplot(tabla1, col=c(\"red\",\"green\",\"blue\"))\nplot(tabla2, col=c(\"red\",\"green\",\"blue\"))\nplot(tabla3, col=c(\"red\",\"green\",\"blue\"))"},{"path":"redes-neuronales-artificiales.html","id":"histogramas-superpuestos","chapter":"Capítulo 7 Redes Neuronales artificiales","heading":"7.6 Histogramas superpuestos","text":"","code":"\nind_1 <- which(Categoric$Pais==\"Colombia\")\np1 <- as.matrix(Categoric[ind_1,5])\n\nind_2 <- which(Categoric$Pais==\"Ecuador\")\np2 <- as.matrix(Categoric[ind_2,5])\n\nind_3 <- which(Categoric$Pais==\"Chile\")\np3 <- as.matrix(Categoric[ind_3,5])\nhp1 <- hist(p1)\nhp2 <- hist(p2)\nhp3 <- hist(p3)\n\npar(mfrow=c(3,1))\nplot( hp1, col=rgb(0,0,1,1/4), xlim=c(30000,200000),ylim=c(0,5),main=\"Ecuador\")\nplot( hp2, col=rgb(1,0,0,1/4),xlim=c(30000,200000),ylim=c(0,10),main=\"Colombia\") \nplot( hp3, col=rgb(1,0,0,1/4),xlim=c(30000,200000),ylim=c(0,10),main=\"Chile\") \npar(mfrow=c(1,3))\nplot( hp1, col=rgb(0,0,1,1/4), xlim=c(30000,200000),ylim=c(0,5),main=\"Ecuador\")\nplot( hp2, col=rgb(1,0,0,1/4),xlim=c(30000,200000),ylim=c(0,10),main=\"Colombia\") \nplot( hp3, col=rgb(1,0,0,1/4),xlim=c(30000,200000),ylim=c(0,10),main=\"Chile\") \npairs(Categoric[ ,1:3])\nboxplot(Categoric[ ,1:3])\nCategoric$Pais <- as.numeric(revalue(Categoric$Pais,\n                          c(\"Colombia\"=\"0\", \"Ecuador\"=\"1\",\n                            \"Chile\"=\"2\")))"},{"path":"redes-neuronales-artificiales.html","id":"cuadro-de-campos-categóricos","chapter":"Capítulo 7 Redes Neuronales artificiales","heading":"7.7 Cuadro de campos categóricos","text":"","code":"\nCategoric$Supervivencia <- as.numeric(revalue(Categoric$Supervivencia,\n                          c(\"BankR\"=\"0\", \"RevEq\"=\"1\",\n                            \"SpinOff\"=\"2\")))"},{"path":"redes-neuronales-artificiales.html","id":"profit-versus-país","chapter":"Capítulo 7 Redes Neuronales artificiales","heading":"7.8 Profit versus País","text":"","code":"\nplot(Categoric$Pais, Categoric$Profit)"},{"path":"redes-neuronales-artificiales.html","id":"visualización-de-tablas","chapter":"Capítulo 7 Redes Neuronales artificiales","heading":"7.9 Visualización de Tablas","text":"Tabla TextualTabla Simple","code":"\nlibrary(kableExtra)\nkable(head(Categoric), \"pipe\")\nkable(head(Categoric), \"simple\")"},{"path":"redes-neuronales-artificiales.html","id":"normailización","chapter":"Capítulo 7 Redes Neuronales artificiales","heading":"7.10 Normailización","text":"Datos Originales y Datos normalizadosMuestreo para entrenamento","code":"\nnormalize<-function(x){\n  return ( (x-min(x))/(max(x)-min(x)))\n}\n\nStartups_norm<-as.data.frame(lapply(Categoric,FUN=normalize))\nsummary(Startups_norm$Profit)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>  0.0000  0.4249  0.5254  0.5481  0.7044  1.0000\nhead(Categoric$Profit)\n#> [1] 192261.8 191792.1 191050.4 182902.0 166187.9 156991.1\nhead(Startups_norm)\n#>   R_D_Spend       POM Logist_Market Pais    Profit\n#> 1 1.0000000 0.6517439     1.0000000  0.0 1.0000000\n#> 2 0.9833595 0.7619717     0.9408934  0.5 0.9973546\n#> 3 0.9279846 0.3795790     0.8646636  1.0 0.9931781\n#> 4 0.8731364 0.5129984     0.8122351  0.0 0.9472924\n#> 5 0.8594377 0.3053280     0.7761356  1.0 0.8531714\n#> 6 0.7975660 0.3694479     0.7691259  0.0 0.8013818\n#>   Supervivencia\n#> 1             1\n#> 2             1\n#> 3             1\n#> 4             1\n#> 5             1\n#> 6             1\nindice <- sample(2, nrow(Startups_norm), replace = TRUE, prob = c(0.7,0.3))\nstartups_train <- Startups_norm[indice==1,]\nstartups_test  <- Startups_norm[indice==2,]"},{"path":"redes-neuronales-artificiales.html","id":"modelo-de-neural-net","chapter":"Capítulo 7 Redes Neuronales artificiales","heading":"7.11 Modelo de Neural Net","text":"","code":"\nlibrary(neuralnet)\nattach(Categoric)\n\nstartups_model <- neuralnet(Profit ~ R_D_Spend+ POM + Logist_Market + Pais , data = startups_train)\n\nstr(startups_model)\n#> List of 14\n#>  $ call               : language neuralnet(formula = Profit ~ R_D_Spend + POM + Logist_Market +      Pais, data = startups_train)\n#>  $ response           : num [1:38, 1] 1 0.997 0.993 0.853 0.801 ...\n#>   ..- attr(*, \"dimnames\")=List of 2\n#>   .. ..$ : chr [1:38] \"1\" \"2\" \"3\" \"5\" ...\n#>   .. ..$ : chr \"Profit\"\n#>  $ covariate          : num [1:38, 1:4] 1 0.983 0.928 0.859 0.798 ...\n#>   ..- attr(*, \"dimnames\")=List of 2\n#>   .. ..$ : chr [1:38] \"1\" \"2\" \"3\" \"5\" ...\n#>   .. ..$ : chr [1:4] \"R_D_Spend\" \"POM\" \"Logist_Market\" \"Pais\"\n#>  $ model.list         :List of 2\n#>   ..$ response : chr \"Profit\"\n#>   ..$ variables: chr [1:4] \"R_D_Spend\" \"POM\" \"Logist_Market\" \"Pais\"\n#>  $ err.fct            :function (x, y)  \n#>   ..- attr(*, \"type\")= chr \"sse\"\n#>  $ act.fct            :function (x)  \n#>   ..- attr(*, \"type\")= chr \"logistic\"\n#>  $ linear.output      : logi TRUE\n#>  $ data               :'data.frame': 38 obs. of  6 variables:\n#>   ..$ R_D_Spend    : num [1:38] 1 0.983 0.928 0.859 0.798 ...\n#>   ..$ POM          : num [1:38] 0.652 0.762 0.38 0.305 0.369 ...\n#>   ..$ Logist_Market: num [1:38] 1 0.941 0.865 0.776 0.769 ...\n#>   ..$ Pais         : num [1:38] 0 0.5 1 1 0 1 0 0.5 1 0.5 ...\n#>   ..$ Profit       : num [1:38] 1 0.997 0.993 0.853 0.801 ...\n#>   ..$ Supervivencia: num [1:38] 1 1 1 1 1 1 1 0.5 0.5 0.5 ...\n#>  $ exclude            : NULL\n#>  $ net.result         :List of 1\n#>   ..$ : num [1:38, 1] 0.945 0.935 0.925 0.893 0.849 ...\n#>   .. ..- attr(*, \"dimnames\")=List of 2\n#>   .. .. ..$ : chr [1:38] \"1\" \"2\" \"3\" \"5\" ...\n#>   .. .. ..$ : NULL\n#>  $ weights            :List of 1\n#>   ..$ :List of 2\n#>   .. ..$ : num [1:5, 1] -1.6037 3.4756 -0.3046 0.2851 0.0369\n#>   .. ..$ : num [1:2, 1] 0.057 1.014\n#>  $ generalized.weights:List of 1\n#>   ..$ : num [1:38, 1:4] 7.37 6.71 6.26 5.32 4.69 ...\n#>   .. ..- attr(*, \"dimnames\")=List of 2\n#>   .. .. ..$ : chr [1:38] \"1\" \"2\" \"3\" \"5\" ...\n#>   .. .. ..$ : NULL\n#>  $ startweights       :List of 1\n#>   ..$ :List of 2\n#>   .. ..$ : num [1:5, 1] 0.4884 0.9995 0.8446 0.0553 1.1865\n#>   .. ..$ : num [1:2, 1] -0.367 -0.649\n#>  $ result.matrix      : num [1:10, 1] 0.06629 0.00832 406 -1.60375 3.47563 ...\n#>   ..- attr(*, \"dimnames\")=List of 2\n#>   .. ..$ : chr [1:10] \"error\" \"reached.threshold\" \"steps\" \"Intercept.to.1layhid1\" ...\n#>   .. ..$ : NULL\n#>  - attr(*, \"class\")= chr \"nn\""},{"path":"redes-neuronales-artificiales.html","id":"ploteo-de-la-red-neuronal","chapter":"Capítulo 7 Redes Neuronales artificiales","heading":"7.12 Ploteo de la red Neuronal","text":"","code":"\nplot(startups_model, rep = \"best\")"},{"path":"redes-neuronales-artificiales.html","id":"ploteo-de-la-red-proporcional","chapter":"Capítulo 7 Redes Neuronales artificiales","heading":"7.13 Ploteo de la red proporcional","text":"Esto indica cuales son los KPI","code":"\npar(mar = numeric(4), family = 'serif')\nplotnet(startups_model, alpha = 0.6)"},{"path":"redes-neuronales-artificiales.html","id":"evaluación-de-la-performance-del-modelo","chapter":"Capítulo 7 Redes Neuronales artificiales","heading":"7.13.1 Evaluación de la performance del modelo","text":"","code":"\nmodel_results <- compute(startups_model,startups_test[1:4])\npredicted_profit <- model_results$net.result"},{"path":"redes-neuronales-artificiales.html","id":"predicted-profit-vs-actual-profit-of-test-data.","chapter":"Capítulo 7 Redes Neuronales artificiales","heading":"7.14 Predicted profit Vs Actual profit of test data.","text":"","code":"\ncor(predicted_profit,startups_test$Profit)\n#>           [,1]\n#> [1,] 0.9716114"},{"path":"redes-neuronales-artificiales.html","id":"desnormalización-de-los-resultados","chapter":"Capítulo 7 Redes Neuronales artificiales","heading":"7.15 Desnormalización de los resultados","text":"Dado que hicimos la predicciones con los datos normalizados, ahora deberemos des-normalizarlos27","code":"\n\nstr_max <- max(Startups$Profit)\nstr_min <- min(Startups$Profit)\n\nunnormalize <- function(x, min, max) { \n  return( (max - min)*x + min )\n}\n\nActualProfit_pred <- unnormalize(predicted_profit,str_min,str_max)\nhead(ActualProfit_pred)\n#>         [,1]\n#> 4  172113.85\n#> 7  159799.94\n#> 21 117169.88\n#> 23 115060.71\n#> 26  99314.84\n#> 28 112440.55"},{"path":"redes-neuronales-artificiales.html","id":"mejoramiento-de-la-performance-del-modelo","chapter":"Capítulo 7 Redes Neuronales artificiales","heading":"7.16 Mejoramiento de la performance del modelo","text":"Es posible mejorar la performance con el agregado de más capas ocultas.","code":"\nStartups_model2 <- neuralnet(Profit~R_D_Spend+ POM + Logist_Market + Pais , data = startups_train, hidden = c(2,4))\n\nplot(Startups_model2 ,rep = \"best\")"},{"path":"redes-neuronales-artificiales.html","id":"performance-del-modelo-mejorado","chapter":"Capítulo 7 Redes Neuronales artificiales","heading":"7.17 Performance del modelo mejorado","text":"","code":"\nmodel_results2<-compute(Startups_model2,startups_test[1:4])\npredicted_Profit2<-model_results2$net.result\ncor(predicted_Profit2,startups_test$Profit)\n#>           [,1]\n#> [1,] 0.9743737"},{"path":"redes-neuronales-artificiales.html","id":"modelo-mejorado-kpi","chapter":"Capítulo 7 Redes Neuronales artificiales","heading":"7.18 Modelo Mejorado KPI","text":"","code":"\npar(mar = numeric(4), family = 'serif')\nplotnet(Startups_model2, alpha = 0.6)"},{"path":"redes-neuronales-artificiales.html","id":"neural-net-clasificación","chapter":"Capítulo 7 Redes Neuronales artificiales","heading":"7.19 Neural Net Clasificación","text":"Armamos el dataset de datos clasificar","code":"\nlibrary(nnet)\n\nsupervivencia <- as.factor(Categoric$Supervivencia)\nR_D_Spend <- as.matrix(Categoric$R_D_Spend)\nPOM <- as.matrix(Categoric$POM)\nLogist_Market <- as.matrix(Categoric$Logist_Market)\nClasificar <- data.frame (supervivencia,R_D_Spend,POM,Logist_Market)"},{"path":"redes-neuronales-artificiales.html","id":"muestreo","chapter":"Capítulo 7 Redes Neuronales artificiales","heading":"7.20 Muestreo","text":"Entrenamiento de nnet como clasificador","code":"\nindice <- sample(2, nrow(Clasificar), replace = TRUE, prob = c(0.7,0.3))\n\nclasificar_train <- Startups_norm[indice==1,]\nclasificar_test  <- Startups_norm[indice==2,]\n\nsupervivientes_clasificados <- factor(clasificar_train$Supervivencia)\nsupervivientes_train<-nnet(supervivientes_clasificados~clasificar_train$R_D_Spend + clasificar_train$POM+ clasificar_train$Logist_Market ,data=clasificar_train,size=5, decay=5e-4, maxit=2000)\n#> # weights:  38\n#> initial  value 59.107806 \n#> iter  10 value 14.748014\n#> iter  20 value 3.603393\n#> iter  30 value 2.820916\n#> iter  40 value 1.612401\n#> iter  50 value 1.455408\n#> iter  60 value 1.296047\n#> iter  70 value 1.236838\n#> iter  80 value 1.193746\n#> iter  90 value 1.166896\n#> iter 100 value 1.146649\n#> iter 110 value 1.111960\n#> iter 120 value 1.097064\n#> iter 130 value 1.089686\n#> iter 140 value 1.077179\n#> iter 150 value 1.069923\n#> iter 160 value 1.066188\n#> iter 170 value 1.062448\n#> iter 180 value 1.054359\n#> iter 190 value 1.046659\n#> iter 200 value 1.039676\n#> iter 210 value 1.033308\n#> iter 220 value 1.031089\n#> iter 230 value 1.030091\n#> iter 240 value 1.029339\n#> iter 250 value 1.028950\n#> iter 260 value 1.028491\n#> iter 270 value 1.027658\n#> iter 280 value 1.026666\n#> iter 290 value 1.025955\n#> iter 300 value 1.025262\n#> iter 310 value 1.024848\n#> iter 320 value 1.024725\n#> iter 330 value 1.024631\n#> iter 340 value 1.024528\n#> iter 350 value 1.024417\n#> iter 360 value 1.024343\n#> iter 370 value 1.024279\n#> iter 380 value 1.024248\n#> iter 390 value 1.024241\n#> iter 400 value 1.024234\n#> iter 410 value 1.024228\n#> iter 420 value 1.024221\n#> iter 430 value 1.024219\n#> iter 440 value 1.024218\n#> iter 450 value 1.024217\n#> iter 450 value 1.024217\n#> iter 450 value 1.024217\n#> final  value 1.024217 \n#> converged"},{"path":"redes-neuronales-artificiales.html","id":"visualización-del-clasificador","chapter":"Capítulo 7 Redes Neuronales artificiales","heading":"7.21 Visualización del clasificador","text":"","code":"\nplotnet(supervivientes_train, alpha = 0.6)"},{"path":"redes-neuronales-artificiales.html","id":"aramdos-de-set-de-entrenamiento-y-de-predicción","chapter":"Capítulo 7 Redes Neuronales artificiales","heading":"7.22 Aramdos de set de entrenamiento y de predicción","text":"https://stackoverrun.com/es/q/3338607crs\\(nnet <- nnet(.factor(Target) ~ .,  data=crs\\)dataset[crs\\(sample,c(crs\\)input, crs$target)],\nsize=10, skip=TRUE, MaxNWts=10000,\ntrace=FALSE, maxit=100)","code":""},{"path":"árboles-de-decisión.html","id":"árboles-de-decisión","chapter":"Capítulo 8 Árboles de decisión","heading":"Capítulo 8 Árboles de decisión","text":"Los árboles de decisión son modelos de aprendizaje automático que se utilizan para resolver problemas de clasificación y regresión. Se basan en una estructura similar un árbol, donde cada nodo representa una pregunta o una condición sobre una característica de los datos. medida que se desciende por el árbol, se toman decisiones basadas en las respuestas las preguntas, hasta llegar una hoja que representa la predicción final.En un árbol de decisión, los datos se dividen en diferentes ramas en función de las características y los valores de las mismas. Estas divisiones se realizan de manera que se maximice la pureza o la homogeneidad de los subconjuntos resultantes. En el caso de problemas de clasificación, los subconjuntos se dividen de forma que se agrupen las instancias con la misma clase, mientras que en problemas de regresión, se busca reducir la varianza de los valores objetivo.Se trata de un proceso de aprendizaje partir de datos y se dice que es supervisado. Necesita datos con etiquetas.El proceso de construcción de un árbol de decisión implica varios pasos, que incluyen la selección de características, la definición de criterios de división y la poda del árbol para evitar el sobreajuste. Algunos algoritmos populares para la construcción de árboles de decisión son el algoritmo ID3, C4.5, CART y Random Forest.Los árboles de decisión tienen varias ventajas, como la interpretabilidad y la capacidad de manejar tanto datos numéricos como categóricos. Además, pueden capturar relaciones lineales entre las características y la variable objetivo. Sin embargo, también tienen limitaciones, como la tendencia al sobreajuste y la sensibilidad pequeñas variaciones en los datos de entrenamiento.En resumen, los árboles de decisión son modelos versátiles y ampliamente utilizados en machine learning debido su capacidad para resolver problemas de clasificación y regresión. Su estructura jerárquica basada en preguntas y decisiones los hace fáciles de interpretar y explicar, lo que los convierte en una herramienta valiosa para tomar decisiones basadas en datos en diferentes dominios.En esta parte veremos como trabajar con el entrenamiento de árboles para luego comprender mejor el algoritmo ramdom-forest.","code":""},{"path":"árboles-de-decisión.html","id":"bibliografía-recomendada-5","chapter":"Capítulo 8 Árboles de decisión","heading":"8.1 Bibliografía recomendada","text":"","code":""},{"path":"árboles-de-decisión.html","id":"carga-de-datos-desde-la-web","chapter":"Capítulo 8 Árboles de decisión","heading":"8.2 Carga de datos desde la web","text":"","code":"\nlibrary(car)\n#> Loading required package: carData\nlibrary(readr)\npartners <- read_csv(\"https://themys.sid.uncu.edu.ar/rpalma/R-cran/UCB/BSC_proveedores.csv\", \n    col_types = cols(Empresa = col_factor(levels = c(\"CNB-Cerveza\", \n        \"FARMACORP\", \"SOFIA-Avicola\"))))\n#> New names:\n#> • `` -> `...1`\n# partners <- read.table(\"BSC_proveedores.csv\",header=TRUE,sep=\",\")\nsummary(partners)\n#>       ...1          Tecnologia        Normas     \n#>  Min.   :  1.00   Min.   :4.300   Min.   :2.000  \n#>  1st Qu.: 38.25   1st Qu.:5.100   1st Qu.:2.800  \n#>  Median : 75.50   Median :5.800   Median :3.000  \n#>  Mean   : 75.50   Mean   :5.843   Mean   :3.057  \n#>  3rd Qu.:112.75   3rd Qu.:6.400   3rd Qu.:3.300  \n#>  Max.   :150.00   Max.   :7.900   Max.   :4.400  \n#>     Capital          Equipo               Empresa  \n#>  Min.   :1.000   Min.   :0.100   CNB-Cerveza  :50  \n#>  1st Qu.:1.600   1st Qu.:0.300   FARMACORP    :50  \n#>  Median :4.350   Median :1.300   SOFIA-Avicola:50  \n#>  Mean   :3.758   Mean   :1.199                     \n#>  3rd Qu.:5.100   3rd Qu.:1.800                     \n#>  Max.   :6.900   Max.   :2.500"},{"path":"árboles-de-decisión.html","id":"matriz-de-covarianza","chapter":"Capítulo 8 Árboles de decisión","heading":"8.3 Matriz de Covarianza","text":"Como vemos la columna 1, PK, (primary key) es parte de los datos. Se trata de un número secuencial que está relacionado con la muestra.","code":"\nlibrary(scatterPlotMatrix)\nscatterPlotMatrix(partners)"},{"path":"árboles-de-decisión.html","id":"entrenamiento-de-árbol-de-decisión","chapter":"Capítulo 8 Árboles de decisión","heading":"8.4 Entrenamiento de árbol de decisión","text":"Esta técnica utiliza un set de datos representativos de una situaci'y utilizando recursivamente el teoréma de Bayes puede armar un pronosticador o clasificador de datos. Es una t'ecnica parecida la de clustering, pero m'refinada, pues se basa en reglas sino en parendizaje del set de datos usado como entrenamento. En el paquete party existen dos funciones ctree que se utiliza para entrenar y predict que se usa para pronosticar o generar la regla de decici'que debemos usar.Impresión del Árbol de Decisión","code":"\nlibrary(party)\n#> Loading required package: grid\n#> Loading required package: mvtnorm\n#> Loading required package: modeltools\n#> Loading required package: stats4\n#> \n#> Attaching package: 'modeltools'\n#> The following object is masked from 'package:car':\n#> \n#>     Predict\n#> Loading required package: strucchange\n#> Loading required package: zoo\n#> \n#> Attaching package: 'zoo'\n#> The following objects are masked from 'package:base':\n#> \n#>     as.Date, as.Date.numeric\n#> Loading required package: sandwich\nattach(partners)\nstr(partners)    \n#> spc_tbl_ [150 × 6] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n#>  $ ...1      : num [1:150] 1 2 3 4 5 6 7 8 9 10 ...\n#>  $ Tecnologia: num [1:150] 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n#>  $ Normas    : num [1:150] 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n#>  $ Capital   : num [1:150] 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n#>  $ Equipo    : num [1:150] 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n#>  $ Empresa   : Factor w/ 3 levels \"CNB-Cerveza\",..: 1 1 1 1 1 1 1 1 1 1 ...\n#>  - attr(*, \"spec\")=\n#>   .. cols(\n#>   ..   ...1 = col_double(),\n#>   ..   Tecnologia = col_double(),\n#>   ..   Normas = col_double(),\n#>   ..   Capital = col_double(),\n#>   ..   Equipo = col_double(),\n#>   ..   Empresa = col_factor(levels = c(\"CNB-Cerveza\", \"FARMACORP\", \"SOFIA-Avicola\"), ordered = FALSE, include_na = FALSE)\n#>   .. )\n#>  - attr(*, \"problems\")=<externalptr>\n# describe al objeto transit y muestras las columna que tiene\n\nind <- sample(2, nrow(partners), replace=TRUE, prob=c(0.7, 0.3))  \n# toma una muestra  \nind  \n#>   [1] 1 2 2 1 1 1 2 1 2 1 1 2 2 2 1 1 1 1 2 2 1 1 2 1 2 2 1\n#>  [28] 1 1 2 1 2 1 2 2 1 1 2 2 1 1 1 1 1 2 1 1 1 1 2 1 1 1 1\n#>  [55] 1 1 1 1 1 1 1 1 2 2 1 1 1 2 1 1 1 2 1 1 1 1 1 2 1 2 2\n#>  [82] 1 1 1 1 1 2 2 2 1 1 1 1 1 1 1 2 1 1 1 2 2 1 2 1 1 2 1\n#> [109] 1 1 1 1 1 1 1 2 1 1 2 2 1 1 1 1 1 2 1 1 1 2 1 2 2 1 2\n#> [136] 1 1 1 2 1 1 1 1 2 1 1 2 1 1 1\n# nos imprime la muestra tomada.\ntrainData <- partners [ind==1,]    \n# genero un set de entrenamiento \ntestData <- partners [ind==2,]    \n# genero un set de datos de prueba\nmyFormula <- Empresa ~ Normas + Tecnologia + Capital + Equipo \ntransit_ctree <- ctree(myFormula, data=trainData)    \n# creo el motor de entrenamiento\n# Verificar las prediciones \ntable(predict(transit_ctree), trainData$Empresa) \n#>                \n#>                 CNB-Cerveza FARMACORP SOFIA-Avicola\n#>   CNB-Cerveza            30         0             0\n#>   FARMACORP               0        38             1\n#>   SOFIA-Avicola           0         1            34\nprint(transit_ctree) \n#> \n#>   Conditional inference tree with 3 terminal nodes\n#> \n#> Response:  Empresa \n#> Inputs:  Normas, Tecnologia, Capital, Equipo \n#> Number of observations:  104 \n#> \n#> 1) Capital <= 1.7; criterion = 1, statistic = 96.529\n#>   2)*  weights = 30 \n#> 1) Capital > 1.7\n#>   3) Equipo <= 1.6; criterion = 1, statistic = 54.04\n#>     4)*  weights = 39 \n#>   3) Equipo > 1.6\n#>     5)*  weights = 35\nlibrary(party)\nattach(partners)\n#> The following objects are masked from partners (pos = 3):\n#> \n#>     ...1, Capital, Empresa, Equipo, Normas,\n#>     Tecnologia\nstr(partners)    \n#> spc_tbl_ [150 × 6] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n#>  $ ...1      : num [1:150] 1 2 3 4 5 6 7 8 9 10 ...\n#>  $ Tecnologia: num [1:150] 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n#>  $ Normas    : num [1:150] 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n#>  $ Capital   : num [1:150] 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n#>  $ Equipo    : num [1:150] 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n#>  $ Empresa   : Factor w/ 3 levels \"CNB-Cerveza\",..: 1 1 1 1 1 1 1 1 1 1 ...\n#>  - attr(*, \"spec\")=\n#>   .. cols(\n#>   ..   ...1 = col_double(),\n#>   ..   Tecnologia = col_double(),\n#>   ..   Normas = col_double(),\n#>   ..   Capital = col_double(),\n#>   ..   Equipo = col_double(),\n#>   ..   Empresa = col_factor(levels = c(\"CNB-Cerveza\", \"FARMACORP\", \"SOFIA-Avicola\"), ordered = FALSE, include_na = FALSE)\n#>   .. )\n#>  - attr(*, \"problems\")=<externalptr>\n# describe al objeto transit\nind <- sample(2, nrow(partners), replace=TRUE, prob=c(0.7, 0.3))  \n# toma una muestra \nind  \n#>   [1] 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 2 1 1 2 1 1 1 2 1 2 2 2\n#>  [28] 2 1 1 1 1 1 1 1 1 1 2 2 1 2 1 1 1 2 1 1 1 2 1 1 1 1 2\n#>  [55] 1 2 2 1 2 2 2 2 1 1 1 2 1 1 2 1 1 2 1 2 1 1 1 1 2 1 1\n#>  [82] 1 1 1 2 1 2 1 2 1 2 1 2 1 1 1 1 2 1 1 2 1 1 1 1 1 1 2\n#> [109] 2 1 2 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 2 1 2 1 1 1 2 2\n#> [136] 1 2 1 1 1 2 1 1 2 1 1 1 1 2 1\n# nos imprime la muestra tomada.\ntable(predict(transit_ctree), trainData$Empresa) \n#>                \n#>                 CNB-Cerveza FARMACORP SOFIA-Avicola\n#>   CNB-Cerveza            30         0             0\n#>   FARMACORP               0        38             1\n#>   SOFIA-Avicola           0         1            34\nplot(transit_ctree, las=2)\nsummary(trainData$Empresa)\n#>   CNB-Cerveza     FARMACORP SOFIA-Avicola \n#>            30            39            35"},{"path":"random-forest.html","id":"random-forest","chapter":"Capítulo 9 Random Forest","heading":"Capítulo 9 Random Forest","text":"Es una técnica que utiliza un conjunto de árboles predictores.\nCada árbol depende de un vector randómicamiente generado y con árboles que tienen entre sí como factor común una misma estructura (Breiman 2001.1). Su uso implica construir múltiples árboles de decisión que conforman un bosque o colección de árboles. Cada bosque su vez utiliza un técnica de votación para hacer la clasificación y la solución más botada es la que se toma por cierta o verdadera.Utilizaremos la función ramdonForest() de la biblioteca que lleva el mismo nombre para construir el modelo del bosque. Especificaremos que en el bosque hay 500 árboles que serán informados en el parámetro ntree que le pasaremos la función.Nota: La salida de la función nos proporcionará una muestra “--bag” más conocida como OOB como resultado, que es una muestra reforzada aproximada del conjunto de datos de entrenamiento.28","code":""},{"path":"random-forest.html","id":"bibliografía-recomendada-6","chapter":"Capítulo 9 Random Forest","heading":"9.1 Bibliografía recomendada","text":"Creación del conjunto de datosSeparación de índicesPodemos observar una representación multidimensional escalada de la proximidad de los conjuntos clasificados según la muestra o MDS.Podemos también ver las variables que más peso tienen en la clasificación del randmoforest construyendo un gráfica de “promedio de exactitud decreciente”\no Average Decrease Accuracy (ADC) y otro gráfico semejante denominado “promedio decreciente de nodo de impuridad” o Mean Decrease Node Impurity  esta técnica coincide con el análisis del índice Gini. Ambos gráficos deben producir","code":"\nlibrary(randomForest)\n#> randomForest 4.7-1.1\n#> Type rfNews() to see new features/changes/bug fixes.\nlibrary(caret) # Tienen la función de partición \n#> Loading required package: ggplot2\n#> \n#> Attaching package: 'ggplot2'\n#> The following object is masked from 'package:randomForest':\n#> \n#>     margin\n#> Loading required package: lattice\nin.Train.C <- createDataPartition (iris$Species, p=0.7 , list=FALSE)\ntraining <- iris[in.Train.C, ]\ntest <- iris[-in.Train.C, ]\nset.seed(831)\nrf.mod.C <- randomForest(Species~., data= training, importance=TRUE, proximity=TRUE, ntree=500 )\nrf.mod.C\n#> \n#> Call:\n#>  randomForest(formula = Species ~ ., data = training, importance = TRUE,      proximity = TRUE, ntree = 500) \n#>                Type of random forest: classification\n#>                      Number of trees: 500\n#> No. of variables tried at each split: 2\n#> \n#>         OOB estimate of  error rate: 5.71%\n#> Confusion matrix:\n#>            setosa versicolor virginica class.error\n#> setosa         35          0         0  0.00000000\n#> versicolor      0         33         2  0.05714286\n#> virginica       0          4        31  0.11428571\nMDSplot(rf.mod.C, training$Species, xlab=\"CP1\", ylab=\"CP2\", main=\"MDS basada en randmo forest\") \nlegend(\"bottomleft\", title = \"Especies\", c(\"setosa\",\"versicolor\",\"virginica\"), fill=c(\"red\",\"green\",\"blue\"), horiz=TRUE, cex=0.45)\nvarImpPlot(rf.mod.C, main=\"Grafico de Importancia Relativa\")"},{"path":"random-forest.html","id":"medición-de-la-preformance","chapter":"Capítulo 9 Random Forest","heading":"9.2 Medición de la preformance","text":"Cada vez que implementamos un método lo indicado es proceder evaluar que tan bueno son los resultados que podemos obtener y esto podemos hacerlo con el ser de entrenamiento o con el de verificación.29","code":"\nrf.train <- predict(rf.mod.C, training[ ,1:4], type = \"class\")\nrf.train.confmat <- confusionMatrix(rf.train, training$Species, mode=\"prec_recall\")\nrf.train.confmat\n#> Confusion Matrix and Statistics\n#> \n#>             Reference\n#> Prediction   setosa versicolor virginica\n#>   setosa         35          0         0\n#>   versicolor      0         35         0\n#>   virginica       0          0        35\n#> \n#> Overall Statistics\n#>                                      \n#>                Accuracy : 1          \n#>                  95% CI : (0.9655, 1)\n#>     No Information Rate : 0.3333     \n#>     P-Value [Acc > NIR] : < 2.2e-16  \n#>                                      \n#>                   Kappa : 1          \n#>                                      \n#>  Mcnemar's Test P-Value : NA         \n#> \n#> Statistics by Class:\n#> \n#>                      Class: setosa Class: versicolor\n#> Precision                   1.0000            1.0000\n#> Recall                      1.0000            1.0000\n#> F1                          1.0000            1.0000\n#> Prevalence                  0.3333            0.3333\n#> Detection Rate              0.3333            0.3333\n#> Detection Prevalence        0.3333            0.3333\n#> Balanced Accuracy           1.0000            1.0000\n#>                      Class: virginica\n#> Precision                      1.0000\n#> Recall                         1.0000\n#> F1                             1.0000\n#> Prevalence                     0.3333\n#> Detection Rate                 0.3333\n#> Detection Prevalence           0.3333\n#> Balanced Accuracy              1.0000"},{"path":"random-forest.html","id":"prueba-de-performance-utilizando-dataset-de-prueba","chapter":"Capítulo 9 Random Forest","heading":"9.3 Prueba de performance utilizando dataset de prueba","text":"Al utilizar el dataset de prueba y tener las columna previamente etiquetadas podemos medir la eficacia del algoritmo utilizado","code":"\nrf.test <- predict(rf.mod.C, test[ ,1:4], type = \"class\")\nrf.test.confmat <- confusionMatrix(rf.test, test$Species, mode=\"prec_recall\")\nrf.test.confmat\n#> Confusion Matrix and Statistics\n#> \n#>             Reference\n#> Prediction   setosa versicolor virginica\n#>   setosa         15          0         0\n#>   versicolor      0         14         2\n#>   virginica       0          1        13\n#> \n#> Overall Statistics\n#>                                          \n#>                Accuracy : 0.9333         \n#>                  95% CI : (0.8173, 0.986)\n#>     No Information Rate : 0.3333         \n#>     P-Value [Acc > NIR] : < 2.2e-16      \n#>                                          \n#>                   Kappa : 0.9            \n#>                                          \n#>  Mcnemar's Test P-Value : NA             \n#> \n#> Statistics by Class:\n#> \n#>                      Class: setosa Class: versicolor\n#> Precision                   1.0000            0.8750\n#> Recall                      1.0000            0.9333\n#> F1                          1.0000            0.9032\n#> Prevalence                  0.3333            0.3333\n#> Detection Rate              0.3333            0.3111\n#> Detection Prevalence        0.3333            0.3556\n#> Balanced Accuracy           1.0000            0.9333\n#>                      Class: virginica\n#> Precision                      0.9286\n#> Recall                         0.8667\n#> F1                             0.8966\n#> Prevalence                     0.3333\n#> Detection Rate                 0.2889\n#> Detection Prevalence           0.3111\n#> Balanced Accuracy              0.9167"},{"path":"random-forest.html","id":"ajuste-del-modelo-2","chapter":"Capítulo 9 Random Forest","heading":"9.4 Ajuste del modelo","text":"Comparando los resultados anteriores podemos estimar la exactitud del modelo (Accuracy).Como vemos la exactitud con el set de entrenamiento es del 100% y con el de prueba es bastante aceptable 97%.30","code":"\nrbind(rf.train.confmat$overall[1], rf.test.confmat$overall[1])\n#>       Accuracy\n#> [1,] 1.0000000\n#> [2,] 0.9333333"},{"path":"random-forest.html","id":"buenas-prácticas-en-randomforest","chapter":"Capítulo 9 Random Forest","heading":"9.5 Buenas prácticas en RandomForest","text":"","code":""},{"path":"random-forest.html","id":"ajuste-del-número-de-árboles-en-el-bosque","chapter":"Capítulo 9 Random Forest","heading":"9.5.1 Ajuste del número de árboles en el bosque","text":"Deberían existir suficiente árboles como para garantizar la estabilidad de la soluciones, pero tanto como para que el costo computacional crezca más allá de la capacidad de la computadora que utilizas. Te darás cuenta si estás utilizando muchos árboles porque llegarás al resultado y tendrás la pantalla azul que aparece cuando Windows se cuelga.\nPuede ayudarte el gráfico de abajo que muestre la nivel de clase de error, que se estabiliza para este ejemplo con 100 árboles. Luego cualquier modelo semejante que tenga resultados mejores debería utilizar más árboles que estos.31\\[ mtry = \\sqrt {n_{features}} \\]Por defecto **mtry* tomará el valor entero más próximo la raíz cuadrada de la cantidad de columnas que tenga el dataset.","code":"    * number of trees **(ntree)**\nplot(rf.mod.C, main = \"Class-Leve Error vs Número de Árboles\")  * Ajuste de variables aleatoriamente muestreadas \nset.seed(831)\ntuneR <- tuneRF(x=training[ ,1:4], y=training$Species, ntreeTry = 500, stepFactor = 1.5)\n#> mtry = 2  OOB error = 5.71% \n#> Searching left ...\n#> Searching right ...\n#> mtry = 3     OOB error = 5.71% \n#> 0 0.05"},{"path":"bibliografía.html","id":"bibliografía","chapter":"Bibliografía","heading":"Bibliografía","text":"nocite: ‘32’","code":""}]
